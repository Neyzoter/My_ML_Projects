{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='0'>目录</a>\n",
    "\n",
    "- <a href='#1'>介绍</a>  \n",
    "- <a href='#2'>前期准备</a>  \n",
    "- <a href='#3'>数据初探</a>  \n",
    "- <a href='#4'>特征工程</a>  \n",
    "- <a href='#5'>模型训练和预测</a>  \n",
    "- <a href='#5'>生成预测结果</a>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='1'>介绍</a>  \n",
    "基于XGBOOST的LANL地震预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='2'>准备</a>\n",
    "## 加载库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# garbage collection\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from xgboost import XGBRegressor\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from scipy.signal import hann\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import hilbert\n",
    "from scipy.signal import convolve\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载数据\n",
    "获取数据的路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'F:/Softcodes/Python/LANL_Earthquake/datasets/trainsets/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8b62f5b94b5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mPATH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"F:/Softcodes/Python/LANL_Earthquake/datasets/trainsets/\"\u001b[0m \u001b[0;31m# windows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#     PATH = \"/media/songchaochao/Projects/Softcodes/Python/LANL_Earthquake/datasets/trainsets/\" # ubuntu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 输出该目录下的文件\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'F:/Softcodes/Python/LANL_Earthquake/datasets/trainsets/'"
     ]
    }
   ],
   "source": [
    "IS_LOCAL = False\n",
    "if(IS_LOCAL):\n",
    "    PATH=\"../input/LANL/\"\n",
    "else:\n",
    "    PATH=\"F:/Softcodes/Python/LANL_Earthquake/datasets/trainsets/\" # windows\n",
    "#     PATH = \"/media/songchaochao/Projects/Softcodes/Python/LANL_Earthquake/datasets/trainsets/\" # ubuntu\n",
    "os.listdir(PATH) # 输出该目录下的文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train_df = pd.read_csv(PATH+'train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64},nrows = 150000*1000)\n",
    "train_df = pd.read_csv(PATH+'train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出数据的行数和列数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: rows:629145480 cols:2\n"
     ]
    }
   ],
   "source": [
    "print(\"Train: rows:{} cols:{}\".format(train_df.shape[0], train_df.shape[1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acoustic_data</th>\n",
       "      <th>time_to_failure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>1.4690999832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>1.4690999821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>1.4690999810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1.4690999799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>1.4690999788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>1.4690999777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>1.4690999766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1.4690999755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-5</td>\n",
       "      <td>1.4690999744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>1.4690999733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acoustic_data  time_to_failure\n",
       "0             12     1.4690999832\n",
       "1              6     1.4690999821\n",
       "2              8     1.4690999810\n",
       "3              5     1.4690999799\n",
       "4              8     1.4690999788\n",
       "5              8     1.4690999777\n",
       "6              9     1.4690999766\n",
       "7              7     1.4690999755\n",
       "8             -5     1.4690999744\n",
       "9              3     1.4690999733"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.precision = 15  # Set number's precison\n",
    "pd.options.display.max_rows = 50 \n",
    "train_df.head(10)  # Output head 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='3'>数据初探</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAv0AAAHfCAYAAADDSme0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ad_sample_df = train_df['acoustic_data'].values[::100]  # 相隔100个取点\n",
    "train_ttf_sample_df = train_df['time_to_failure'].values[::100]  # 相隔100个取点\n",
    "\n",
    "def plot_acc_ttf_data(train_ad_sample_df, train_ttf_sample_df, title=\"Acoustic data and time to failure: 1% sampled data\"):\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "    plt.title(title)\n",
    "    plt.plot(train_ad_sample_df, color='r')\n",
    "    ax1.set_ylabel('acoustic data', color='r')\n",
    "    plt.legend(['acoustic data'], loc=(0.01, 0.95)) # 设置标签的位置\n",
    "    ax2 = ax1.twinx() # 复制X轴坐标\n",
    "    plt.plot(train_ttf_sample_df, color='b')\n",
    "    ax2.set_ylabel('time to failure', color='b')\n",
    "    plt.legend(['time to failure'], loc=(0.01, 0.9))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_acc_ttf_data(train_ad_sample_df, train_ttf_sample_df)\n",
    "del train_ad_sample_df\n",
    "del train_ttf_sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAv0AAAHfCAYAAADDSme0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ad_sample_df = train_df['acoustic_data'].values[:6291455]\n",
    "train_ttf_sample_df = train_df['time_to_failure'].values[:6291455]\n",
    "plot_acc_ttf_data(train_ad_sample_df, train_ttf_sample_df, title=\"Acoustic data and time to failure: 1% of data\")\n",
    "del train_ad_sample_df\n",
    "del train_ttf_sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='4'>特征工程</a>\n",
    "从测试集看到，每个cvs文件包含15000行数据，所以特征提取也以15000行为单位。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of segments:  4194\n"
     ]
    }
   ],
   "source": [
    "rows = 150000  # 每组数据集合的数据个数\n",
    "segments = int(np.floor(train_df.shape[0] / rows)) # 所有数据行数 / 每一个segment的个数\n",
    "print(\"Number of segments: \", segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义两个用于提取特征的函数，分别用于挖掘趋势（线性回归）和STA/LTA数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR para:[-3.2682998170341496e-06, 5.1292341854609864]\n"
     ]
    }
   ],
   "source": [
    "def add_trend_feature(arr, abs_values=False):\n",
    "    '''\n",
    "    得到线性模型的估计参数，包括横截和斜率\n",
    "    '''\n",
    "    idx = np.array(range(len(arr)))\n",
    "    if abs_values:\n",
    "        arr = np.abs(arr)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(idx.reshape(-1, 1), arr)  # -1表示，行是由array的另外一个长度来推测出来的\n",
    "    return [lr.coef_[0],lr.intercept_] # 返回线性回归器的估计参数\n",
    "\n",
    "def classic_sta_lta(x, length_sta, length_lta):\n",
    "#     sta = np.cumsum(x ** 2) # x^2\n",
    "    sta = np.cumsum(np.abs(x)) # abs(x)\n",
    "    # 转化为float\n",
    "    sta = np.require(sta, dtype=np.float)\n",
    "    # Copy for LTA\n",
    "    lta = sta.copy()\n",
    "    # Compute the STA and the LTA\n",
    "    sta[length_sta:] = sta[length_sta:] - sta[:-length_sta] # sta的第length_sta个元素后所有元素\n",
    "    sta /= length_sta\n",
    "    lta[length_lta:] = lta[length_lta:] - lta[:-length_lta]\n",
    "    lta /= length_lta\n",
    "    # Pad zeros\n",
    "    sta[:length_lta - 1] = 0\n",
    "    # Avoid division by zero by setting zero values to tiny float\n",
    "    dtiny = np.finfo(0.0).tiny\n",
    "    idx = lta < dtiny\n",
    "    lta[idx] = dtiny\n",
    "    return sta / lta\n",
    "\n",
    "# 使用例子\n",
    "data = train_df.iloc[0:rows]\n",
    "xc = pd.Series(data['acoustic_data'].values)\n",
    "print('LR para:{}'.format(add_trend_feature(xc)))\n",
    "del data\n",
    "del xc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练集特征提取过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 4194/4194 [42:17<00:00,  1.73it/s]\n"
     ]
    }
   ],
   "source": [
    "train_X = pd.DataFrame(index=range(segments), dtype=np.float64)\n",
    "train_y = pd.DataFrame(index=range(segments), dtype=np.float64, columns=['time_to_failure'])\n",
    "\n",
    "# # 所有数据，基本没有意义\n",
    "# total_mean = train_df['acoustic_data'].mean()\n",
    "# total_std = train_df['acoustic_data'].std()\n",
    "# total_max = train_df['acoustic_data'].max()\n",
    "# total_min = train_df['acoustic_data'].min()\n",
    "# total_sum = train_df['acoustic_data'].sum()\n",
    "# total_abs_sum = np.abs(train_df['acoustic_data']).sum()\n",
    "\n",
    "# 用于防止分母为0\n",
    "epsilon = 0.01\n",
    "# iterate over all segments\n",
    "for segment in tqdm(range(segments)):\n",
    "    # 提取每个segment的数据，共150000个\n",
    "    seg = train_df.iloc[segment*rows:segment*rows+rows]\n",
    "    xc = pd.Series(seg['acoustic_data'].values)\n",
    "    xc.replace(0,epsilon,inplace = True)  # 用一个epsilon代替0，防止分母出现0\n",
    "    xc_abs = np.abs(xc)# 后面需要多次使用，缓存下来\n",
    "    yc = seg['time_to_failure'].values[-1]  # 最后一个数据的距离地震的时间作为训练样本的时间\n",
    "    \n",
    "    # 1.给train_y和train_X赋值\n",
    "    ## 1.2距离地震的时间\n",
    "    train_y.loc[segment, 'time_to_failure'] = yc\n",
    "    ## 1.3声波信号的均值\n",
    "    train_X.loc[segment, 'mean'] = xc.mean()\n",
    "    ## 1.4声波信号的标准差\n",
    "    train_X.loc[segment, 'std'] = xc.std()\n",
    "    ## 1.5声波信号的最大值\n",
    "    train_X.loc[segment, 'max'] = xc.max()\n",
    "    ## 1.6声波信号的最小值\n",
    "    train_X.loc[segment, 'min'] = xc.min()\n",
    "    ## 1.7声波信号的一阶差分  xc(k)-xc(k-1)的均值\n",
    "    train_X.loc[segment, 'mean_change_abs'] = np.mean(np.diff(xc))\n",
    "    ## 1.8一阶差分/xc(除去最后一个)    np.nonzero返回非零的元素下标\n",
    "#     train_X.loc[segment, 'mean_change_rate'] = np.mean(np.nonzero((np.diff(xc) / xc[:-1]))[0]) #原来是去掉了零元素的下标\n",
    "    \n",
    "    train_X.loc[segment, 'mean_change_rate'] = np.mean(np.diff(xc) / xc[:-1])\n",
    "    ## 1.9xc的绝对值最大值\n",
    "    train_X.loc[segment, 'abs_max'] = xc_abs.max()\n",
    "    ## 1.10xc的绝对值最小值\n",
    "    train_X.loc[segment, 'abs_min'] = xc_abs.min()\n",
    "    ## 1.11前面50000个的数据\n",
    "    ### 1.11.1前面50000个的标准差\n",
    "    train_X.loc[segment, 'std_first_50000'] = xc[:50000].std()\n",
    "    ### 1.11.2后面50000个的标准差\n",
    "    train_X.loc[segment, 'std_last_50000'] = xc[-50000:].std()\n",
    "    ### 1.11.3前面10000个的标准差\n",
    "    train_X.loc[segment, 'std_first_10000'] = xc[:10000].std()\n",
    "    ### 1.11.4后面10000个的标准差\n",
    "    train_X.loc[segment, 'std_last_10000'] = xc[-10000:].std()\n",
    "    ### 1.11.5前面50000个的平均值\n",
    "    train_X.loc[segment, 'avg_first_50000'] = xc[:50000].mean()\n",
    "    ### 1.11.6后面50000个的平均值\n",
    "    train_X.loc[segment, 'avg_last_50000'] = xc[-50000:].mean()\n",
    "    ### 1.11.7前面10000个的平均值\n",
    "    train_X.loc[segment, 'avg_first_10000'] = xc[:10000].mean()\n",
    "    ### 1.11.7后面10000个的平均值\n",
    "    train_X.loc[segment, 'avg_last_10000'] = xc[-10000:].mean()\n",
    "    ### 1.11.8前面50000个的最小值\n",
    "    train_X.loc[segment, 'min_first_50000'] = xc[:50000].min()\n",
    "    ### 1.11.9后面50000个的最小值\n",
    "    train_X.loc[segment, 'min_last_50000'] = xc[-50000:].min()\n",
    "    ### 1.11.10前面10000个的最小值\n",
    "    train_X.loc[segment, 'min_first_10000'] = xc[:10000].min()\n",
    "    ### 1.11.11后面10000个的最小值\n",
    "    train_X.loc[segment, 'min_last_10000'] = xc[-10000:].min()\n",
    "    ### 1.11.12最大值\n",
    "    train_X.loc[segment, 'max_first_50000'] = xc[:50000].max()\n",
    "    ### 1.11.13\n",
    "    train_X.loc[segment, 'max_last_50000'] = xc[-50000:].max()\n",
    "    ### 1.11.14\n",
    "    train_X.loc[segment, 'max_first_10000'] = xc[:10000].max()\n",
    "    ### 1.11.15\n",
    "    train_X.loc[segment, 'max_last_10000'] = xc[-10000:].max()\n",
    "    ## 1.12 max/min的绝对值\n",
    "    train_X.loc[segment, 'max_to_min'] = np.abs(xc.max() / xc.min())  \n",
    "    ## 1.13 峰峰值（原来是xc.max() - np.abs(xc.min())，去绝对值后的峰峰值）\n",
    "    train_X.loc[segment, 'max_to_min_diff'] = xc.max() - xc.min()\n",
    "    ## 1.14 比较大的数个数\n",
    "    train_X.loc[segment, 'count_big'] = len(xc[xc_abs > 500])\n",
    "    ## 1.15 xc数据的和\n",
    "    train_X.loc[segment, 'sum'] = xc.sum()\n",
    "    ## 1.16 对前后50000和10000运用一阶差分\n",
    "    ### 1.16.1 去除掉0\n",
    "#     train_X.loc[segment, 'mean_change_rate_first_50000'] = np.mean(np.nonzero((np.diff(xc[:50000]) / xc[:50000][:-1]))[0])\n",
    "#     train_X.loc[segment, 'mean_change_rate_last_50000'] = np.mean(np.nonzero((np.diff(xc[-50000:]) / xc[-50000:][:-1]))[0])\n",
    "#     train_X.loc[segment, 'mean_change_rate_first_10000'] = np.mean(np.nonzero((np.diff(xc[:10000]) / xc[:10000][:-1]))[0])\n",
    "#     train_X.loc[segment, 'mean_change_rate_last_10000'] = np.mean(np.nonzero((np.diff(xc[-10000:]) / xc[-10000:][:-1]))[0])\n",
    "    ### 1.16.2 不去除0\n",
    "    train_X.loc[segment, 'mean_change_rate_first_50000'] = np.mean((np.diff(xc[:50000]) / xc[:50000][:-1]))\n",
    "    train_X.loc[segment, 'mean_change_rate_last_50000'] = np.mean((np.diff(xc[-50000:]) / xc[-50000:][:-1]))\n",
    "    train_X.loc[segment, 'mean_change_rate_first_10000'] = np.mean((np.diff(xc[:10000]) / xc[:10000][:-1]))\n",
    "    train_X.loc[segment, 'mean_change_rate_last_10000'] = np.mean((np.diff(xc[-10000:]) / xc[-10000:][:-1]))\n",
    "    \n",
    "    ## 1.17 q-quantile：q位数，至少有0.95的数据小于等于返回的q位数，(1-0.95)的数大于等于\n",
    "    train_X.loc[segment, 'q95'] = np.percentile(xc, 95)\n",
    "    train_X.loc[segment, 'q99'] = np.percentile(xc, 99)\n",
    "    train_X.loc[segment, 'q05'] = np.percentile(xc, 5)\n",
    "    train_X.loc[segment, 'q01'] = np.percentile(xc, 1)\n",
    "    \n",
    "    train_X.loc[segment, 'abs_q95'] = np.percentile(np.abs(xc), 95)\n",
    "    train_X.loc[segment, 'abs_q99'] = np.percentile(np.abs(xc), 99)\n",
    "    train_X.loc[segment, 'abs_q05'] = np.percentile(np.abs(xc), 5)\n",
    "    train_X.loc[segment, 'abs_q01'] = np.percentile(np.abs(xc), 1)\n",
    "    \n",
    "    ## 1.18 用线性模型拟合后的趋势是一个  coef * x + intercept\n",
    "    train_X.loc[segment, 'trend_coef'] = (add_trend_feature(xc))[0]  # 线性模型的coef\n",
    "    train_X.loc[segment, 'trend_intercept'] = (add_trend_feature(xc))[1] # 线性模型的intercept\n",
    "    train_X.loc[segment, 'abs_trend_coef'] = (add_trend_feature(xc, abs_values=True))[0]\n",
    "    train_X.loc[segment, 'abs_trend_intercept'] = (add_trend_feature(xc, abs_values=True))[1]\n",
    "    \n",
    "    ## 1.19 绝对均值、绝对标准差\n",
    "    train_X.loc[segment, 'abs_mean'] = xc_abs.mean()\n",
    "    train_X.loc[segment, 'abs_std'] = xc_abs.std()\n",
    "    \n",
    "    ## 1.20 平均绝对偏差\n",
    "    train_X.loc[segment, 'mad'] = xc.mad()\n",
    "    ## 1.21 峰度kurtosis\n",
    "    train_X.loc[segment, 'kurt'] = xc.kurtosis()\n",
    "    ## 1.22 偏度Skewness\n",
    "    train_X.loc[segment, 'skew'] = xc.skew()\n",
    "    ## 1.23 中位数median\n",
    "    train_X.loc[segment, 'med'] = xc.median()\n",
    "    ## 1.24 希尔伯特变换，转化为二位复平面\n",
    "    ## 对复平面数据a+bj进行abs，则得到一维数据sqrt(a*a+b*b)\n",
    "    train_X.loc[segment, 'Hilbert_mean'] = np.abs(hilbert(xc)).mean()\n",
    "    ## 1.25 数据和hann窗口进行same卷积运算\n",
    "    train_X.loc[segment, 'Hann_window_mean'] = (convolve(xc, hann(150), mode='same') / sum(hann(150))).mean()\n",
    "    ## 1.26 STA和LTA数据\n",
    "    train_X.loc[segment, 'classic_sta_lta1_mean'] = classic_sta_lta(xc, 500, 10000).mean() # STA长度500和LTA长度10000\n",
    "    train_X.loc[segment, 'classic_sta_lta2_mean'] = classic_sta_lta(xc, 5000, 100000).mean()\n",
    "    train_X.loc[segment, 'classic_sta_lta3_mean'] = classic_sta_lta(xc, 3333, 6666).mean()\n",
    "    train_X.loc[segment, 'classic_sta_lta4_mean'] = classic_sta_lta(xc, 10000, 25000).mean()\n",
    "    ## 1.27 根据滚动窗口计算,排除掉NA/null的情况\n",
    "    train_X.loc[segment, 'Moving_average_400_mean'] = xc.rolling(window=400).mean().mean(skipna=True)\n",
    "    train_X.loc[segment, 'Moving_average_700_mean'] = xc.rolling(window=700).mean().mean(skipna=True)\n",
    "    train_X.loc[segment, 'Moving_average_1500_mean'] = xc.rolling(window=1500).mean().mean(skipna=True)\n",
    "    train_X.loc[segment, 'Moving_average_3000_mean'] = xc.rolling(window=3000).mean().mean(skipna=True)\n",
    "    train_X.loc[segment, 'Moving_average_6000_mean'] = xc.rolling(window=6000).mean().mean(skipna=True)\n",
    "    ## 1.28 根据跨度span指定衰减alpha = 2/(span+1),span >= 1\n",
    "    ewma = pd.Series.ewm  # 指数加权函数\n",
    "    train_X.loc[segment, 'exp_Moving_average_300_mean'] = (ewma(xc, span=300).mean()).mean(skipna=True) \n",
    "    train_X.loc[segment, 'exp_Moving_average_3000_mean'] = ewma(xc, span=3000).mean().mean(skipna=True)\n",
    "    train_X.loc[segment, 'exp_Moving_average_30000_mean'] = ewma(xc, span=30000).mean().mean(skipna=True)\n",
    "    \n",
    "    ## 1.29 滚动窗口的均值和标准差结合，置信区间\n",
    "    no_of_std = 2  # 置信区间取2倍的方差\n",
    "    train_X.loc[segment, 'MA_400MA_std_mean'] = xc.rolling(window=400).std().mean()\n",
    "    train_X.loc[segment,'MA_400MA_BB_high_mean'] = (train_X.loc[segment, 'Moving_average_400_mean'] + no_of_std * train_X.loc[segment, 'MA_400MA_std_mean']).mean()\n",
    "    train_X.loc[segment,'MA_400MA_BB_low_mean'] = (train_X.loc[segment, 'Moving_average_400_mean'] - no_of_std * train_X.loc[segment, 'MA_400MA_std_mean']).mean()\n",
    "    train_X.loc[segment, 'MA_700MA_std_mean'] = xc.rolling(window=700).std().mean()\n",
    "    train_X.loc[segment,'MA_700MA_BB_high_mean'] = (train_X.loc[segment, 'Moving_average_700_mean'] + no_of_std * train_X.loc[segment, 'MA_700MA_std_mean']).mean()\n",
    "    train_X.loc[segment,'MA_700MA_BB_low_mean'] = (train_X.loc[segment, 'Moving_average_700_mean'] - no_of_std * train_X.loc[segment, 'MA_700MA_std_mean']).mean()\n",
    "    train_X.loc[segment, 'MA_1500MA_std_mean'] = xc.rolling(window=1500).std().mean()\n",
    "    train_X.loc[segment,'MA_1500MA_BB_high_mean'] = (train_X.loc[segment, 'Moving_average_1500_mean'] + no_of_std * train_X.loc[segment, 'MA_1500MA_std_mean']).mean()\n",
    "    train_X.loc[segment,'MA_1500MA_BB_low_mean'] = (train_X.loc[segment, 'Moving_average_1500_mean'] - no_of_std * train_X.loc[segment, 'MA_1500MA_std_mean']).mean()\n",
    "    train_X.loc[segment, 'MA_3000MA_std_mean'] = xc.rolling(window=3000).std().mean()\n",
    "    train_X.loc[segment,'MA_3000MA_BB_high_mean'] = (train_X.loc[segment, 'Moving_average_3000_mean'] + no_of_std * train_X.loc[segment, 'MA_3000MA_std_mean']).mean()\n",
    "    train_X.loc[segment,'MA_3000MA_BB_low_mean'] = (train_X.loc[segment, 'Moving_average_3000_mean'] - no_of_std * train_X.loc[segment, 'MA_3000MA_std_mean']).mean()    \n",
    "    train_X.loc[segment, 'MA_6000MA_std_mean'] = xc.rolling(window=6000).std().mean()\n",
    "    train_X.loc[segment,'MA_6000MA_BB_high_mean'] = (train_X.loc[segment, 'Moving_average_6000_mean'] + no_of_std * train_X.loc[segment, 'MA_6000MA_std_mean']).mean()\n",
    "    train_X.loc[segment,'MA_6000MA_BB_low_mean'] = (train_X.loc[segment, 'Moving_average_6000_mean'] - no_of_std * train_X.loc[segment, 'MA_6000MA_std_mean']).mean()    \n",
    "    ## 1.30 位数相减\n",
    "    train_X.loc[segment, 'iqr'] = np.subtract(*np.percentile(xc, [75, 25]))\n",
    "    ## 1.31 大/小位数\n",
    "    train_X.loc[segment, 'q999'] = np.percentile(xc,99.9)\n",
    "    train_X.loc[segment, 'q001'] = np.percentile(xc,0.1)\n",
    "    ## 1.32 分布去掉头尾后的均值\n",
    "    train_X.loc[segment, 'ave10'] = stats.trim_mean(xc, 0.1)\n",
    "    ## 1.33 滚动窗口为10,100,1000的一些数据\n",
    "    for windows in [10, 100, 1000]:\n",
    "        x_roll_std = xc.rolling(windows).std().dropna().values\n",
    "        x_roll_mean = xc.rolling(windows).mean().dropna().values\n",
    "        x_roll_std  = pd.Series(x_roll_std)\n",
    "        x_roll_std.replace(0,epsilon,True)\n",
    "        x_roll_mean  = pd.Series(x_roll_mean)\n",
    "        x_roll_mean.replace(0,epsilon,True)\n",
    "        train_X.loc[segment, 'ave_roll_std_' + str(windows)] = x_roll_std.mean()\n",
    "        train_X.loc[segment, 'std_roll_std_' + str(windows)] = x_roll_std.std()\n",
    "        train_X.loc[segment, 'max_roll_std_' + str(windows)] = x_roll_std.max()\n",
    "        train_X.loc[segment, 'min_roll_std_' + str(windows)] = x_roll_std.min()\n",
    "        train_X.loc[segment, 'q01_roll_std_' + str(windows)] = np.percentile(x_roll_std, 1)\n",
    "        train_X.loc[segment, 'q05_roll_std_' + str(windows)] = np.percentile(x_roll_std, 5)\n",
    "        train_X.loc[segment, 'q95_roll_std_' + str(windows)] = np.percentile(x_roll_std, 95)\n",
    "        train_X.loc[segment, 'q99_roll_std_' + str(windows)] = np.percentile(x_roll_std, 99)\n",
    "        train_X.loc[segment, 'av_change_abs_roll_std_' + str(windows)] = np.mean(np.diff(x_roll_std))\n",
    "        train_X.loc[segment, 'av_change_rate_roll_std_' + str(windows)] = np.mean(np.diff(x_roll_std) / x_roll_std[:-1])\n",
    "        train_X.loc[segment, 'abs_max_roll_std_' + str(windows)] = np.abs(x_roll_std).max()\n",
    "        train_X.loc[segment, 'abs_min_roll_std_' + str(windows)] = np.abs(x_roll_std).min()  ## 加入的\n",
    "        \n",
    "        train_X.loc[segment, 'ave_roll_mean_' + str(windows)] = x_roll_mean.mean()\n",
    "        train_X.loc[segment, 'std_roll_mean_' + str(windows)] = x_roll_mean.std()\n",
    "        train_X.loc[segment, 'max_roll_mean_' + str(windows)] = x_roll_mean.max()\n",
    "        train_X.loc[segment, 'min_roll_mean_' + str(windows)] = x_roll_mean.min()\n",
    "        train_X.loc[segment, 'q01_roll_mean_' + str(windows)] = np.percentile(x_roll_mean, 1)\n",
    "        train_X.loc[segment, 'q05_roll_mean_' + str(windows)] = np.percentile(x_roll_mean, 5)\n",
    "        train_X.loc[segment, 'q95_roll_mean_' + str(windows)] = np.percentile(x_roll_mean, 95)\n",
    "        train_X.loc[segment, 'q99_roll_mean_' + str(windows)] = np.percentile(x_roll_mean, 99)\n",
    "        train_X.loc[segment, 'av_change_abs_roll_mean_' + str(windows)] = np.mean(np.diff(x_roll_mean))\n",
    "        train_X.loc[segment, 'av_change_rate_roll_mean_' + str(windows)] = np.mean(np.diff(x_roll_mean) / x_roll_mean[:-1])\n",
    "        train_X.loc[segment, 'abs_max_roll_mean_' + str(windows)] = np.abs(x_roll_mean).max()\n",
    "        train_X.loc[segment, 'abs_min_roll_mean_' + str(windows)] = np.abs(x_roll_mean).min()## 加入的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看前面10条数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>mean_change_abs</th>\n",
       "      <th>mean_change_rate</th>\n",
       "      <th>abs_max</th>\n",
       "      <th>abs_min</th>\n",
       "      <th>std_first_50000</th>\n",
       "      <th>std_last_50000</th>\n",
       "      <th>...</th>\n",
       "      <th>max_roll_mean_1000</th>\n",
       "      <th>min_roll_mean_1000</th>\n",
       "      <th>q01_roll_mean_1000</th>\n",
       "      <th>q05_roll_mean_1000</th>\n",
       "      <th>q95_roll_mean_1000</th>\n",
       "      <th>q99_roll_mean_1000</th>\n",
       "      <th>av_change_abs_roll_mean_1000</th>\n",
       "      <th>av_change_rate_roll_mean_1000</th>\n",
       "      <th>abs_max_roll_mean_1000</th>\n",
       "      <th>abs_min_roll_mean_1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.884496</td>\n",
       "      <td>5.100740</td>\n",
       "      <td>104.0</td>\n",
       "      <td>-98.0</td>\n",
       "      <td>-7.993387e-05</td>\n",
       "      <td>7.854260</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6.488255</td>\n",
       "      <td>3.664141</td>\n",
       "      <td>...</td>\n",
       "      <td>5.62926</td>\n",
       "      <td>3.89649</td>\n",
       "      <td>4.07250</td>\n",
       "      <td>4.37943</td>\n",
       "      <td>5.33823</td>\n",
       "      <td>5.48425</td>\n",
       "      <td>-1.704362e-06</td>\n",
       "      <td>7.214393e-07</td>\n",
       "      <td>5.62926</td>\n",
       "      <td>3.89649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.726157</td>\n",
       "      <td>6.588544</td>\n",
       "      <td>181.0</td>\n",
       "      <td>-154.0</td>\n",
       "      <td>2.368492e-20</td>\n",
       "      <td>7.712235</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>7.304973</td>\n",
       "      <td>5.492756</td>\n",
       "      <td>...</td>\n",
       "      <td>5.66735</td>\n",
       "      <td>3.41214</td>\n",
       "      <td>4.23338</td>\n",
       "      <td>4.34556</td>\n",
       "      <td>5.06637</td>\n",
       "      <td>5.22331</td>\n",
       "      <td>-2.449732e-06</td>\n",
       "      <td>1.416888e-06</td>\n",
       "      <td>5.66735</td>\n",
       "      <td>3.41214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.906768</td>\n",
       "      <td>6.967133</td>\n",
       "      <td>140.0</td>\n",
       "      <td>-106.0</td>\n",
       "      <td>-1.333342e-05</td>\n",
       "      <td>7.178467</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6.104523</td>\n",
       "      <td>8.603489</td>\n",
       "      <td>...</td>\n",
       "      <td>5.95734</td>\n",
       "      <td>4.05549</td>\n",
       "      <td>4.23938</td>\n",
       "      <td>4.44634</td>\n",
       "      <td>5.34426</td>\n",
       "      <td>5.48630</td>\n",
       "      <td>1.141544e-06</td>\n",
       "      <td>2.231434e-06</td>\n",
       "      <td>5.95734</td>\n",
       "      <td>4.05549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.902610</td>\n",
       "      <td>6.922044</td>\n",
       "      <td>197.0</td>\n",
       "      <td>-199.0</td>\n",
       "      <td>8.408145e-19</td>\n",
       "      <td>7.344139</td>\n",
       "      <td>199.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6.237806</td>\n",
       "      <td>5.652138</td>\n",
       "      <td>...</td>\n",
       "      <td>5.85826</td>\n",
       "      <td>3.72215</td>\n",
       "      <td>4.30446</td>\n",
       "      <td>4.43357</td>\n",
       "      <td>5.31726</td>\n",
       "      <td>5.45317</td>\n",
       "      <td>-2.551074e-06</td>\n",
       "      <td>1.554261e-06</td>\n",
       "      <td>5.85826</td>\n",
       "      <td>3.72215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.909083</td>\n",
       "      <td>7.300866</td>\n",
       "      <td>145.0</td>\n",
       "      <td>-126.0</td>\n",
       "      <td>-6.666711e-06</td>\n",
       "      <td>7.395516</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.323484</td>\n",
       "      <td>7.694276</td>\n",
       "      <td>...</td>\n",
       "      <td>6.07805</td>\n",
       "      <td>3.91841</td>\n",
       "      <td>4.43439</td>\n",
       "      <td>4.54343</td>\n",
       "      <td>5.30646</td>\n",
       "      <td>5.49132</td>\n",
       "      <td>1.551074e-06</td>\n",
       "      <td>2.453099e-06</td>\n",
       "      <td>6.07805</td>\n",
       "      <td>3.91841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.913899</td>\n",
       "      <td>5.433763</td>\n",
       "      <td>142.0</td>\n",
       "      <td>-144.0</td>\n",
       "      <td>-7.333382e-05</td>\n",
       "      <td>7.492517</td>\n",
       "      <td>144.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.441722</td>\n",
       "      <td>5.144874</td>\n",
       "      <td>...</td>\n",
       "      <td>5.83833</td>\n",
       "      <td>4.04272</td>\n",
       "      <td>4.27052</td>\n",
       "      <td>4.42241</td>\n",
       "      <td>5.40218</td>\n",
       "      <td>5.62030</td>\n",
       "      <td>-3.698121e-06</td>\n",
       "      <td>4.434326e-07</td>\n",
       "      <td>5.83833</td>\n",
       "      <td>4.04272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.856033</td>\n",
       "      <td>5.687505</td>\n",
       "      <td>120.0</td>\n",
       "      <td>-78.0</td>\n",
       "      <td>3.326689e-05</td>\n",
       "      <td>7.348631</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.944053</td>\n",
       "      <td>5.386527</td>\n",
       "      <td>...</td>\n",
       "      <td>5.54633</td>\n",
       "      <td>3.95848</td>\n",
       "      <td>4.28545</td>\n",
       "      <td>4.50431</td>\n",
       "      <td>5.21436</td>\n",
       "      <td>5.37021</td>\n",
       "      <td>-1.146980e-07</td>\n",
       "      <td>1.342316e-06</td>\n",
       "      <td>5.54633</td>\n",
       "      <td>3.95848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.505860</td>\n",
       "      <td>5.854179</td>\n",
       "      <td>139.0</td>\n",
       "      <td>-134.0</td>\n",
       "      <td>1.006609e-19</td>\n",
       "      <td>8.747825</td>\n",
       "      <td>139.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6.069433</td>\n",
       "      <td>7.077895</td>\n",
       "      <td>...</td>\n",
       "      <td>5.18036</td>\n",
       "      <td>3.48923</td>\n",
       "      <td>4.04062</td>\n",
       "      <td>4.14855</td>\n",
       "      <td>4.86335</td>\n",
       "      <td>4.95338</td>\n",
       "      <td>-1.395906e-06</td>\n",
       "      <td>1.463884e-06</td>\n",
       "      <td>5.18036</td>\n",
       "      <td>3.48923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.718248</td>\n",
       "      <td>7.789392</td>\n",
       "      <td>168.0</td>\n",
       "      <td>-156.0</td>\n",
       "      <td>1.333342e-05</td>\n",
       "      <td>7.234857</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.287829</td>\n",
       "      <td>5.992777</td>\n",
       "      <td>...</td>\n",
       "      <td>5.71750</td>\n",
       "      <td>3.93347</td>\n",
       "      <td>4.15867</td>\n",
       "      <td>4.31555</td>\n",
       "      <td>5.10436</td>\n",
       "      <td>5.33731</td>\n",
       "      <td>2.221208e-06</td>\n",
       "      <td>3.084838e-06</td>\n",
       "      <td>5.71750</td>\n",
       "      <td>3.93347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.731362</td>\n",
       "      <td>6.890184</td>\n",
       "      <td>152.0</td>\n",
       "      <td>-126.0</td>\n",
       "      <td>-6.666711e-06</td>\n",
       "      <td>7.557252</td>\n",
       "      <td>152.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.012481</td>\n",
       "      <td>4.943657</td>\n",
       "      <td>...</td>\n",
       "      <td>5.42322</td>\n",
       "      <td>3.83219</td>\n",
       "      <td>4.19046</td>\n",
       "      <td>4.30147</td>\n",
       "      <td>5.12835</td>\n",
       "      <td>5.26126</td>\n",
       "      <td>5.126376e-06</td>\n",
       "      <td>3.324529e-06</td>\n",
       "      <td>5.42322</td>\n",
       "      <td>3.83219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 155 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean       std    max    min  mean_change_abs  mean_change_rate  \\\n",
       "0  4.884496  5.100740  104.0  -98.0    -7.993387e-05          7.854260   \n",
       "1  4.726157  6.588544  181.0 -154.0     2.368492e-20          7.712235   \n",
       "2  4.906768  6.967133  140.0 -106.0    -1.333342e-05          7.178467   \n",
       "3  4.902610  6.922044  197.0 -199.0     8.408145e-19          7.344139   \n",
       "4  4.909083  7.300866  145.0 -126.0    -6.666711e-06          7.395516   \n",
       "5  4.913899  5.433763  142.0 -144.0    -7.333382e-05          7.492517   \n",
       "6  4.856033  5.687505  120.0  -78.0     3.326689e-05          7.348631   \n",
       "7  4.505860  5.854179  139.0 -134.0     1.006609e-19          8.747825   \n",
       "8  4.718248  7.789392  168.0 -156.0     1.333342e-05          7.234857   \n",
       "9  4.731362  6.890184  152.0 -126.0    -6.666711e-06          7.557252   \n",
       "\n",
       "   abs_max  abs_min  std_first_50000  std_last_50000  ...  max_roll_mean_1000  \\\n",
       "0    104.0     0.01         6.488255        3.664141  ...             5.62926   \n",
       "1    181.0     0.01         7.304973        5.492756  ...             5.66735   \n",
       "2    140.0     0.01         6.104523        8.603489  ...             5.95734   \n",
       "3    199.0     0.01         6.237806        5.652138  ...             5.85826   \n",
       "4    145.0     0.01         5.323484        7.694276  ...             6.07805   \n",
       "5    144.0     0.01         4.441722        5.144874  ...             5.83833   \n",
       "6    120.0     0.01         5.944053        5.386527  ...             5.54633   \n",
       "7    139.0     0.01         6.069433        7.077895  ...             5.18036   \n",
       "8    168.0     0.01         9.287829        5.992777  ...             5.71750   \n",
       "9    152.0     0.01         9.012481        4.943657  ...             5.42322   \n",
       "\n",
       "   min_roll_mean_1000  q01_roll_mean_1000  q05_roll_mean_1000  \\\n",
       "0             3.89649             4.07250             4.37943   \n",
       "1             3.41214             4.23338             4.34556   \n",
       "2             4.05549             4.23938             4.44634   \n",
       "3             3.72215             4.30446             4.43357   \n",
       "4             3.91841             4.43439             4.54343   \n",
       "5             4.04272             4.27052             4.42241   \n",
       "6             3.95848             4.28545             4.50431   \n",
       "7             3.48923             4.04062             4.14855   \n",
       "8             3.93347             4.15867             4.31555   \n",
       "9             3.83219             4.19046             4.30147   \n",
       "\n",
       "   q95_roll_mean_1000  q99_roll_mean_1000  av_change_abs_roll_mean_1000  \\\n",
       "0             5.33823             5.48425                 -1.704362e-06   \n",
       "1             5.06637             5.22331                 -2.449732e-06   \n",
       "2             5.34426             5.48630                  1.141544e-06   \n",
       "3             5.31726             5.45317                 -2.551074e-06   \n",
       "4             5.30646             5.49132                  1.551074e-06   \n",
       "5             5.40218             5.62030                 -3.698121e-06   \n",
       "6             5.21436             5.37021                 -1.146980e-07   \n",
       "7             4.86335             4.95338                 -1.395906e-06   \n",
       "8             5.10436             5.33731                  2.221208e-06   \n",
       "9             5.12835             5.26126                  5.126376e-06   \n",
       "\n",
       "   av_change_rate_roll_mean_1000  abs_max_roll_mean_1000  \\\n",
       "0                   7.214393e-07                 5.62926   \n",
       "1                   1.416888e-06                 5.66735   \n",
       "2                   2.231434e-06                 5.95734   \n",
       "3                   1.554261e-06                 5.85826   \n",
       "4                   2.453099e-06                 6.07805   \n",
       "5                   4.434326e-07                 5.83833   \n",
       "6                   1.342316e-06                 5.54633   \n",
       "7                   1.463884e-06                 5.18036   \n",
       "8                   3.084838e-06                 5.71750   \n",
       "9                   3.324529e-06                 5.42322   \n",
       "\n",
       "   abs_min_roll_mean_1000  \n",
       "0                 3.89649  \n",
       "1                 3.41214  \n",
       "2                 4.05549  \n",
       "3                 3.72215  \n",
       "4                 3.91841  \n",
       "5                 4.04272  \n",
       "6                 3.95848  \n",
       "7                 3.48923  \n",
       "8                 3.93347  \n",
       "9                 3.83219  \n",
       "\n",
       "[10 rows x 155 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_X = pd.read_csv('./train_X_V0.0.csv')\n",
    "\n",
    "scaler = StandardScaler()  # 标准化\n",
    "\n",
    "## 处理INF、NaN等数值问题\n",
    "# # 得到NaN的数目\n",
    "# print('NaN num  {}'.format((np.isnan(train_X)).sum()))\n",
    "# print('NaN num  {}'.format((np.isinf(train_X)).sum()))\n",
    "# ## 用很大的数来代替inf\n",
    "# train_X.replace([-np.inf],-1000000,inplace=True)\n",
    "# train_X.replace([np.inf],1000000,inplace=True)\n",
    "# train_X.replace(-np.nan,1000000,inplace=True)\n",
    "\n",
    "scaler.fit(train_X)\n",
    "# print('scaler_mean:{}'.format(scaler.mean_))\n",
    "scaled_train_X = pd.DataFrame(scaler.transform(train_X), columns=train_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成csv文件保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scaled_train_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-51734e300265>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscaled_train_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'scaled_train_X.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# index=False不要unamed索引\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_X.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# index=False不要unamed索引\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_y.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# index=False不要unamed索引\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scaled_train_X' is not defined"
     ]
    }
   ],
   "source": [
    "scaled_train_X.to_csv('scaled_train_X.csv',index=False) # index=False不要unamed索引\n",
    "train_X.to_csv('train_X.csv',index=False) # index=False不要unamed索引\n",
    "train_y.to_csv('train_y.csv',index=False) # index=False不要unamed索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_to_failure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.430797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.391499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.353196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.313798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.274400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_to_failure\n",
       "0         1.430797\n",
       "1         1.391499\n",
       "2         1.353196\n",
       "3         1.313798\n",
       "4         1.274400"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_train_X = pd.read_csv('./scaled_train_X_V0.0.csv')\n",
    "train_y = pd.read_csv('./train_y_V0.0.csv')\n",
    "# # 用于补救，去除unamed列\n",
    "# train_X.drop(columns = ['Unnamed: 0'],inplace = True)\n",
    "# train_y.drop(columns = ['Unnamed: 0'],inplace = True)\n",
    "\n",
    "scaled_train_X.head(10)\n",
    "train_y.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试集数据csv文件名获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# submission = pd.read_csv('F:/Softcodes/Python/LANL_Earthquake/sample_submission.csv', index_col='seg_id') #读取test文件的名称\n",
    "submission = pd.read_csv('/media/songchaochao/Projects/Softcodes/Python/LANL_Earthquake/sample_submission.csv',index_col='seg_id')\n",
    "test_X = pd.DataFrame(columns=scaled_train_X.columns, dtype=np.float64, index=submission.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试集特征提取过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2624/2624 [27:27<00:00,  1.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# iterate over all seg_ids\n",
    "for i, seg_id in enumerate(tqdm((test_X.index))):\n",
    "    seg = pd.read_csv('F:/Softcodes/Python/LANL_Earthquake/datasets/testsets/' + seg_id + '.csv')\n",
    "    xc = pd.Series(seg['acoustic_data'].values)\n",
    "    xc.replace(0,epsilon,inplace = True)  # 用一个epsilon代替0，防止分母出现0\n",
    "    xc_abs = np.abs(xc)# 后面需要多次使用，缓存下来\n",
    "    \n",
    "    # 1.给train_y和test_X赋值\n",
    "#     ## 1.2距离地震的时间\n",
    "#     train_y.loc[seg_id, 'time_to_failure'] = yc\n",
    "    ## 1.3声波信号的均值\n",
    "    test_X.loc[seg_id, 'mean'] = xc.mean()\n",
    "    ## 1.4声波信号的标准差\n",
    "    test_X.loc[seg_id, 'std'] = xc.std()\n",
    "    ## 1.5声波信号的最大值\n",
    "    test_X.loc[seg_id, 'max'] = xc.max()\n",
    "    ## 1.6声波信号的最小值\n",
    "    test_X.loc[seg_id, 'min'] = xc.min()\n",
    "    ## 1.7声波信号的一阶差分  xc(k)-xc(k-1)的均值\n",
    "    test_X.loc[seg_id, 'mean_change_abs'] = np.mean(np.diff(xc))\n",
    "    ## 1.8一阶差分/xc(除去最后一个)    np.nonzero返回非零的元素下标\n",
    "#     test_X.loc[seg_id, 'mean_change_rate'] = np.mean(np.nonzero((np.diff(xc) / xc[:-1]))[0]) #原来是去掉了零元素的下标\n",
    "    \n",
    "    test_X.loc[seg_id, 'mean_change_rate'] = np.mean(np.diff(xc) / xc[:-1])\n",
    "    ## 1.9xc的绝对值最大值\n",
    "    test_X.loc[seg_id, 'abs_max'] = xc_abs.max()\n",
    "    ## 1.10xc的绝对值最小值\n",
    "    test_X.loc[seg_id, 'abs_min'] = xc_abs.min()\n",
    "    ## 1.11前面50000个的数据\n",
    "    ### 1.11.1前面50000个的标准差\n",
    "    test_X.loc[seg_id, 'std_first_50000'] = xc[:50000].std()\n",
    "    ### 1.11.2后面50000个的标准差\n",
    "    test_X.loc[seg_id, 'std_last_50000'] = xc[-50000:].std()\n",
    "    ### 1.11.3前面10000个的标准差\n",
    "    test_X.loc[seg_id, 'std_first_10000'] = xc[:10000].std()\n",
    "    ### 1.11.4后面10000个的标准差\n",
    "    test_X.loc[seg_id, 'std_last_10000'] = xc[-10000:].std()\n",
    "    ### 1.11.5前面50000个的平均值\n",
    "    test_X.loc[seg_id, 'avg_first_50000'] = xc[:50000].mean()\n",
    "    ### 1.11.6后面50000个的平均值\n",
    "    test_X.loc[seg_id, 'avg_last_50000'] = xc[-50000:].mean()\n",
    "    ### 1.11.7前面10000个的平均值\n",
    "    test_X.loc[seg_id, 'avg_first_10000'] = xc[:10000].mean()\n",
    "    ### 1.11.7后面10000个的平均值\n",
    "    test_X.loc[seg_id, 'avg_last_10000'] = xc[-10000:].mean()\n",
    "    ### 1.11.8前面50000个的最小值\n",
    "    test_X.loc[seg_id, 'min_first_50000'] = xc[:50000].min()\n",
    "    ### 1.11.9后面50000个的最小值\n",
    "    test_X.loc[seg_id, 'min_last_50000'] = xc[-50000:].min()\n",
    "    ### 1.11.10前面10000个的最小值\n",
    "    test_X.loc[seg_id, 'min_first_10000'] = xc[:10000].min()\n",
    "    ### 1.11.11后面10000个的最小值\n",
    "    test_X.loc[seg_id, 'min_last_10000'] = xc[-10000:].min()\n",
    "    ### 1.11.12最大值\n",
    "    test_X.loc[seg_id, 'max_first_50000'] = xc[:50000].max()\n",
    "    ### 1.11.13\n",
    "    test_X.loc[seg_id, 'max_last_50000'] = xc[-50000:].max()\n",
    "    ### 1.11.14\n",
    "    test_X.loc[seg_id, 'max_first_10000'] = xc[:10000].max()\n",
    "    ### 1.11.15\n",
    "    test_X.loc[seg_id, 'max_last_10000'] = xc[-10000:].max()\n",
    "    ## 1.12 max/min的绝对值\n",
    "    test_X.loc[seg_id, 'max_to_min'] = np.abs(xc.max() / xc.min())  \n",
    "    ## 1.13 峰峰值（原来是xc.max() - np.abs(xc.min())，去绝对值后的峰峰值）\n",
    "    test_X.loc[seg_id, 'max_to_min_diff'] = xc.max() - xc.min()\n",
    "    ## 1.14 比较大的数个数\n",
    "    test_X.loc[seg_id, 'count_big'] = len(xc[xc_abs > 500])\n",
    "    ## 1.15 xc数据的和\n",
    "    test_X.loc[seg_id, 'sum'] = xc.sum()\n",
    "    ## 1.16 对前后50000和10000运用一阶差分\n",
    "    ### 1.16.1 去除掉0\n",
    "#     test_X.loc[seg_id, 'mean_change_rate_first_50000'] = np.mean(np.nonzero((np.diff(xc[:50000]) / xc[:50000][:-1]))[0])\n",
    "#     test_X.loc[seg_id, 'mean_change_rate_last_50000'] = np.mean(np.nonzero((np.diff(xc[-50000:]) / xc[-50000:][:-1]))[0])\n",
    "#     test_X.loc[seg_id, 'mean_change_rate_first_10000'] = np.mean(np.nonzero((np.diff(xc[:10000]) / xc[:10000][:-1]))[0])\n",
    "#     test_X.loc[seg_id, 'mean_change_rate_last_10000'] = np.mean(np.nonzero((np.diff(xc[-10000:]) / xc[-10000:][:-1]))[0])\n",
    "    ### 1.16.2 不去除0\n",
    "    test_X.loc[seg_id, 'mean_change_rate_first_50000'] = np.mean((np.diff(xc[:50000]) / xc[:50000][:-1]))\n",
    "    test_X.loc[seg_id, 'mean_change_rate_last_50000'] = np.mean((np.diff(xc[-50000:]) / xc[-50000:][:-1]))\n",
    "    test_X.loc[seg_id, 'mean_change_rate_first_10000'] = np.mean((np.diff(xc[:10000]) / xc[:10000][:-1]))\n",
    "    test_X.loc[seg_id, 'mean_change_rate_last_10000'] = np.mean((np.diff(xc[-10000:]) / xc[-10000:][:-1]))\n",
    "    \n",
    "    ## 1.17 q-quantile：q位数，至少有0.95的数据小于等于返回的q位数，(1-0.95)的数大于等于\n",
    "    test_X.loc[seg_id, 'q95'] = np.percentile(xc, 95)\n",
    "    test_X.loc[seg_id, 'q99'] = np.percentile(xc, 99)\n",
    "    test_X.loc[seg_id, 'q05'] = np.percentile(xc, 5)\n",
    "    test_X.loc[seg_id, 'q01'] = np.percentile(xc, 1)\n",
    "    \n",
    "    test_X.loc[seg_id, 'abs_q95'] = np.percentile(np.abs(xc), 95)\n",
    "    test_X.loc[seg_id, 'abs_q99'] = np.percentile(np.abs(xc), 99)\n",
    "    test_X.loc[seg_id, 'abs_q05'] = np.percentile(np.abs(xc), 5)\n",
    "    test_X.loc[seg_id, 'abs_q01'] = np.percentile(np.abs(xc), 1)\n",
    "    \n",
    "    ## 1.18 用线性模型拟合后的趋势是一个  coef * x + intercept\n",
    "    test_X.loc[seg_id, 'trend_coef'] = (add_trend_feature(xc))[0]  # 线性模型的coef\n",
    "    test_X.loc[seg_id, 'trend_intercept'] = (add_trend_feature(xc))[1] # 线性模型的intercept\n",
    "    test_X.loc[seg_id, 'abs_trend_coef'] = (add_trend_feature(xc, abs_values=True))[0]\n",
    "    test_X.loc[seg_id, 'abs_trend_intercept'] = (add_trend_feature(xc, abs_values=True))[1]\n",
    "    \n",
    "    ## 1.19 绝对均值、绝对标准差\n",
    "    test_X.loc[seg_id, 'abs_mean'] = xc_abs.mean()\n",
    "    test_X.loc[seg_id, 'abs_std'] = xc_abs.std()\n",
    "    \n",
    "    ## 1.20 平均绝对偏差\n",
    "    test_X.loc[seg_id, 'mad'] = xc.mad()\n",
    "    ## 1.21 峰度kurtosis\n",
    "    test_X.loc[seg_id, 'kurt'] = xc.kurtosis()\n",
    "    ## 1.22 偏度Skewness\n",
    "    test_X.loc[seg_id, 'skew'] = xc.skew()\n",
    "    ## 1.23 中位数median\n",
    "    test_X.loc[seg_id, 'med'] = xc.median()\n",
    "    ## 1.24 希尔伯特变换，转化为二位复平面\n",
    "    ## 对复平面数据a+bj进行abs，则得到一维数据sqrt(a*a+b*b)\n",
    "    test_X.loc[seg_id, 'Hilbert_mean'] = np.abs(hilbert(xc)).mean()\n",
    "    ## 1.25 数据和hann窗口进行same卷积运算\n",
    "    test_X.loc[seg_id, 'Hann_window_mean'] = (convolve(xc, hann(150), mode='same') / sum(hann(150))).mean()\n",
    "    ## 1.26 STA和LTA数据\n",
    "    test_X.loc[seg_id, 'classic_sta_lta1_mean'] = classic_sta_lta(xc, 500, 10000).mean() # STA长度500和LTA长度10000\n",
    "    test_X.loc[seg_id, 'classic_sta_lta2_mean'] = classic_sta_lta(xc, 5000, 100000).mean()\n",
    "    test_X.loc[seg_id, 'classic_sta_lta3_mean'] = classic_sta_lta(xc, 3333, 6666).mean()\n",
    "    test_X.loc[seg_id, 'classic_sta_lta4_mean'] = classic_sta_lta(xc, 10000, 25000).mean()\n",
    "    ## 1.27 根据滚动窗口计算,排除掉NA/null的情况\n",
    "    test_X.loc[seg_id, 'Moving_average_400_mean'] = xc.rolling(window=400).mean().mean(skipna=True)\n",
    "    test_X.loc[seg_id, 'Moving_average_700_mean'] = xc.rolling(window=700).mean().mean(skipna=True)\n",
    "    test_X.loc[seg_id, 'Moving_average_1500_mean'] = xc.rolling(window=1500).mean().mean(skipna=True)\n",
    "    test_X.loc[seg_id, 'Moving_average_3000_mean'] = xc.rolling(window=3000).mean().mean(skipna=True)\n",
    "    test_X.loc[seg_id, 'Moving_average_6000_mean'] = xc.rolling(window=6000).mean().mean(skipna=True)\n",
    "    ## 1.28 根据跨度span指定衰减alpha = 2/(span+1),span >= 1\n",
    "    ewma = pd.Series.ewm  # 指数加权函数\n",
    "    test_X.loc[seg_id, 'exp_Moving_average_300_mean'] = (ewma(xc, span=300).mean()).mean(skipna=True) \n",
    "    test_X.loc[seg_id, 'exp_Moving_average_3000_mean'] = ewma(xc, span=3000).mean().mean(skipna=True)\n",
    "    test_X.loc[seg_id, 'exp_Moving_average_30000_mean'] = ewma(xc, span=30000).mean().mean(skipna=True)\n",
    "    \n",
    "    ## 1.29 滚动窗口的均值和标准差结合，置信区间\n",
    "    no_of_std = 2  # 置信区间取2倍的方差\n",
    "    test_X.loc[seg_id, 'MA_400MA_std_mean'] = xc.rolling(window=400).std().mean()\n",
    "    test_X.loc[seg_id,'MA_400MA_BB_high_mean'] = (test_X.loc[seg_id, 'Moving_average_400_mean'] + no_of_std * test_X.loc[seg_id, 'MA_400MA_std_mean']).mean()\n",
    "    test_X.loc[seg_id,'MA_400MA_BB_low_mean'] = (test_X.loc[seg_id, 'Moving_average_400_mean'] - no_of_std * test_X.loc[seg_id, 'MA_400MA_std_mean']).mean()\n",
    "    test_X.loc[seg_id, 'MA_700MA_std_mean'] = xc.rolling(window=700).std().mean()\n",
    "    test_X.loc[seg_id,'MA_700MA_BB_high_mean'] = (test_X.loc[seg_id, 'Moving_average_700_mean'] + no_of_std * test_X.loc[seg_id, 'MA_700MA_std_mean']).mean()\n",
    "    test_X.loc[seg_id,'MA_700MA_BB_low_mean'] = (test_X.loc[seg_id, 'Moving_average_700_mean'] - no_of_std * test_X.loc[seg_id, 'MA_700MA_std_mean']).mean()\n",
    "    test_X.loc[seg_id, 'MA_1500MA_std_mean'] = xc.rolling(window=1500).std().mean()\n",
    "    test_X.loc[seg_id,'MA_1500MA_BB_high_mean'] = (test_X.loc[seg_id, 'Moving_average_1500_mean'] + no_of_std * test_X.loc[seg_id, 'MA_1500MA_std_mean']).mean()\n",
    "    test_X.loc[seg_id,'MA_1500MA_BB_low_mean'] = (test_X.loc[seg_id, 'Moving_average_1500_mean'] - no_of_std * test_X.loc[seg_id, 'MA_1500MA_std_mean']).mean()\n",
    "    test_X.loc[seg_id, 'MA_3000MA_std_mean'] = xc.rolling(window=3000).std().mean()\n",
    "    test_X.loc[seg_id,'MA_3000MA_BB_high_mean'] = (test_X.loc[seg_id, 'Moving_average_3000_mean'] + no_of_std * test_X.loc[seg_id, 'MA_3000MA_std_mean']).mean()\n",
    "    test_X.loc[seg_id,'MA_3000MA_BB_low_mean'] = (test_X.loc[seg_id, 'Moving_average_3000_mean'] - no_of_std * test_X.loc[seg_id, 'MA_3000MA_std_mean']).mean()    \n",
    "    test_X.loc[seg_id, 'MA_6000MA_std_mean'] = xc.rolling(window=6000).std().mean()\n",
    "    test_X.loc[seg_id,'MA_6000MA_BB_high_mean'] = (test_X.loc[seg_id, 'Moving_average_6000_mean'] + no_of_std * test_X.loc[seg_id, 'MA_6000MA_std_mean']).mean()\n",
    "    test_X.loc[seg_id,'MA_6000MA_BB_low_mean'] = (test_X.loc[seg_id, 'Moving_average_6000_mean'] - no_of_std * test_X.loc[seg_id, 'MA_6000MA_std_mean']).mean()    \n",
    "    ## 1.30 位数相减\n",
    "    test_X.loc[seg_id, 'iqr'] = np.subtract(*np.percentile(xc, [75, 25]))\n",
    "    ## 1.31 大/小位数\n",
    "    test_X.loc[seg_id, 'q999'] = np.percentile(xc,99.9)\n",
    "    test_X.loc[seg_id, 'q001'] = np.percentile(xc,0.1)\n",
    "    ## 1.32 分布去掉头尾后的均值\n",
    "    test_X.loc[seg_id, 'ave10'] = stats.trim_mean(xc, 0.1)\n",
    "    ## 1.33 滚动窗口为10,100,1000的一些数据\n",
    "    for windows in [10, 100, 1000]:\n",
    "        x_roll_std = xc.rolling(windows).std().dropna().values\n",
    "        x_roll_mean = xc.rolling(windows).mean().dropna().values\n",
    "        x_roll_std  = pd.Series(x_roll_std)\n",
    "        x_roll_std.replace(0,epsilon,True)\n",
    "        x_roll_mean  = pd.Series(x_roll_mean)\n",
    "        x_roll_mean.replace(0,epsilon,True)\n",
    "        test_X.loc[seg_id, 'ave_roll_std_' + str(windows)] = x_roll_std.mean()\n",
    "        test_X.loc[seg_id, 'std_roll_std_' + str(windows)] = x_roll_std.std()\n",
    "        test_X.loc[seg_id, 'max_roll_std_' + str(windows)] = x_roll_std.max()\n",
    "        test_X.loc[seg_id, 'min_roll_std_' + str(windows)] = x_roll_std.min()\n",
    "        test_X.loc[seg_id, 'q01_roll_std_' + str(windows)] = np.percentile(x_roll_std, 1)\n",
    "        test_X.loc[seg_id, 'q05_roll_std_' + str(windows)] = np.percentile(x_roll_std, 5)\n",
    "        test_X.loc[seg_id, 'q95_roll_std_' + str(windows)] = np.percentile(x_roll_std, 95)\n",
    "        test_X.loc[seg_id, 'q99_roll_std_' + str(windows)] = np.percentile(x_roll_std, 99)\n",
    "        test_X.loc[seg_id, 'av_change_abs_roll_std_' + str(windows)] = np.mean(np.diff(x_roll_std))\n",
    "        test_X.loc[seg_id, 'av_change_rate_roll_std_' + str(windows)] = np.mean(np.diff(x_roll_std) / x_roll_std[:-1])\n",
    "        test_X.loc[seg_id, 'abs_max_roll_std_' + str(windows)] = np.abs(x_roll_std).max()\n",
    "        test_X.loc[seg_id, 'abs_min_roll_std_' + str(windows)] = np.abs(x_roll_std).min()  ## 加入的\n",
    "        \n",
    "        test_X.loc[seg_id, 'ave_roll_mean_' + str(windows)] = x_roll_mean.mean()\n",
    "        test_X.loc[seg_id, 'std_roll_mean_' + str(windows)] = x_roll_mean.std()\n",
    "        test_X.loc[seg_id, 'max_roll_mean_' + str(windows)] = x_roll_mean.max()\n",
    "        test_X.loc[seg_id, 'min_roll_mean_' + str(windows)] = x_roll_mean.min()\n",
    "        test_X.loc[seg_id, 'q01_roll_mean_' + str(windows)] = np.percentile(x_roll_mean, 1)\n",
    "        test_X.loc[seg_id, 'q05_roll_mean_' + str(windows)] = np.percentile(x_roll_mean, 5)\n",
    "        test_X.loc[seg_id, 'q95_roll_mean_' + str(windows)] = np.percentile(x_roll_mean, 95)\n",
    "        test_X.loc[seg_id, 'q99_roll_mean_' + str(windows)] = np.percentile(x_roll_mean, 99)\n",
    "        test_X.loc[seg_id, 'av_change_abs_roll_mean_' + str(windows)] = np.mean(np.diff(x_roll_mean))\n",
    "        test_X.loc[seg_id, 'av_change_rate_roll_mean_' + str(windows)] = np.mean(np.diff(x_roll_mean) / x_roll_mean[:-1])\n",
    "        test_X.loc[seg_id, 'abs_max_roll_mean_' + str(windows)] = np.abs(x_roll_mean).max()\n",
    "        test_X.loc[seg_id, 'abs_min_roll_mean_' + str(windows)] = np.abs(x_roll_mean).min()## 加入的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成csv文件保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_X = pd.read_csv('./test_X_V0.0.csv')\n",
    "# print('scaler_mean:{}'.format(scaler.mean_))\n",
    "scaled_test_X = pd.DataFrame(scaler.transform(test_X), columns=test_X.columns)\n",
    "\n",
    "scaled_test_X.to_csv('scaled_test_X.csv',index=False)\n",
    "# test_X.to_csv('test_X.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>mean_change_abs</th>\n",
       "      <th>mean_change_rate</th>\n",
       "      <th>abs_max</th>\n",
       "      <th>abs_min</th>\n",
       "      <th>std_first_50000</th>\n",
       "      <th>std_last_50000</th>\n",
       "      <th>...</th>\n",
       "      <th>max_roll_mean_1000</th>\n",
       "      <th>min_roll_mean_1000</th>\n",
       "      <th>q01_roll_mean_1000</th>\n",
       "      <th>q05_roll_mean_1000</th>\n",
       "      <th>q95_roll_mean_1000</th>\n",
       "      <th>q99_roll_mean_1000</th>\n",
       "      <th>av_change_abs_roll_mean_1000</th>\n",
       "      <th>av_change_rate_roll_mean_1000</th>\n",
       "      <th>abs_max_roll_mean_1000</th>\n",
       "      <th>abs_min_roll_mean_1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.108000</td>\n",
       "      <td>-0.194612</td>\n",
       "      <td>-0.177889</td>\n",
       "      <td>0.279984</td>\n",
       "      <td>0.445826</td>\n",
       "      <td>-0.024536</td>\n",
       "      <td>-0.185515</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.089787</td>\n",
       "      <td>-0.156760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047319</td>\n",
       "      <td>0.127043</td>\n",
       "      <td>-0.093304</td>\n",
       "      <td>-0.044583</td>\n",
       "      <td>-0.199409</td>\n",
       "      <td>-0.210435</td>\n",
       "      <td>0.681416</td>\n",
       "      <td>-0.019859</td>\n",
       "      <td>-0.047319</td>\n",
       "      <td>0.294366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.360286</td>\n",
       "      <td>-0.073582</td>\n",
       "      <td>-0.042321</td>\n",
       "      <td>0.034777</td>\n",
       "      <td>-0.218786</td>\n",
       "      <td>0.707466</td>\n",
       "      <td>-0.060886</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022277</td>\n",
       "      <td>-0.230500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.201906</td>\n",
       "      <td>-0.054943</td>\n",
       "      <td>-0.604034</td>\n",
       "      <td>-1.160103</td>\n",
       "      <td>-1.381692</td>\n",
       "      <td>-0.801642</td>\n",
       "      <td>-0.062096</td>\n",
       "      <td>-0.019801</td>\n",
       "      <td>-0.201906</td>\n",
       "      <td>-0.502110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.354594</td>\n",
       "      <td>0.046862</td>\n",
       "      <td>0.309425</td>\n",
       "      <td>-0.165162</td>\n",
       "      <td>-0.329554</td>\n",
       "      <td>-0.283632</td>\n",
       "      <td>0.262476</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.464013</td>\n",
       "      <td>-0.107463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187627</td>\n",
       "      <td>0.030014</td>\n",
       "      <td>0.164506</td>\n",
       "      <td>0.372875</td>\n",
       "      <td>0.159202</td>\n",
       "      <td>-0.025133</td>\n",
       "      <td>0.041344</td>\n",
       "      <td>-0.019763</td>\n",
       "      <td>0.187627</td>\n",
       "      <td>-0.130290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.046787</td>\n",
       "      <td>-0.286283</td>\n",
       "      <td>-0.287810</td>\n",
       "      <td>0.212080</td>\n",
       "      <td>0.778131</td>\n",
       "      <td>0.928723</td>\n",
       "      <td>-0.259619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.299989</td>\n",
       "      <td>-0.306565</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062922</td>\n",
       "      <td>0.175396</td>\n",
       "      <td>0.204151</td>\n",
       "      <td>0.261926</td>\n",
       "      <td>-0.040131</td>\n",
       "      <td>-0.095414</td>\n",
       "      <td>1.480860</td>\n",
       "      <td>-0.019844</td>\n",
       "      <td>-0.062922</td>\n",
       "      <td>0.505984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.527552</td>\n",
       "      <td>-0.088361</td>\n",
       "      <td>0.049280</td>\n",
       "      <td>0.008370</td>\n",
       "      <td>-0.108017</td>\n",
       "      <td>0.901030</td>\n",
       "      <td>0.023323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.093441</td>\n",
       "      <td>0.150402</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196167</td>\n",
       "      <td>-0.048451</td>\n",
       "      <td>-0.831876</td>\n",
       "      <td>-1.621292</td>\n",
       "      <td>-1.277309</td>\n",
       "      <td>-0.505054</td>\n",
       "      <td>-1.096552</td>\n",
       "      <td>-0.019907</td>\n",
       "      <td>-0.196167</td>\n",
       "      <td>-0.473694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 155 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean       std       max       min  mean_change_abs  mean_change_rate  \\\n",
       "0 -0.108000 -0.194612 -0.177889  0.279984         0.445826         -0.024536   \n",
       "1 -1.360286 -0.073582 -0.042321  0.034777        -0.218786          0.707466   \n",
       "2  0.354594  0.046862  0.309425 -0.165162        -0.329554         -0.283632   \n",
       "3  0.046787 -0.286283 -0.287810  0.212080         0.778131          0.928723   \n",
       "4 -1.527552 -0.088361  0.049280  0.008370        -0.108017          0.901030   \n",
       "\n",
       "    abs_max  abs_min  std_first_50000  std_last_50000           ...            \\\n",
       "0 -0.185515      0.0        -0.089787       -0.156760           ...             \n",
       "1 -0.060886      0.0         0.022277       -0.230500           ...             \n",
       "2  0.262476      0.0         0.464013       -0.107463           ...             \n",
       "3 -0.259619      0.0        -0.299989       -0.306565           ...             \n",
       "4  0.023323      0.0        -0.093441        0.150402           ...             \n",
       "\n",
       "   max_roll_mean_1000  min_roll_mean_1000  q01_roll_mean_1000  \\\n",
       "0           -0.047319            0.127043           -0.093304   \n",
       "1           -0.201906           -0.054943           -0.604034   \n",
       "2            0.187627            0.030014            0.164506   \n",
       "3           -0.062922            0.175396            0.204151   \n",
       "4           -0.196167           -0.048451           -0.831876   \n",
       "\n",
       "   q05_roll_mean_1000  q95_roll_mean_1000  q99_roll_mean_1000  \\\n",
       "0           -0.044583           -0.199409           -0.210435   \n",
       "1           -1.160103           -1.381692           -0.801642   \n",
       "2            0.372875            0.159202           -0.025133   \n",
       "3            0.261926           -0.040131           -0.095414   \n",
       "4           -1.621292           -1.277309           -0.505054   \n",
       "\n",
       "   av_change_abs_roll_mean_1000  av_change_rate_roll_mean_1000  \\\n",
       "0                      0.681416                      -0.019859   \n",
       "1                     -0.062096                      -0.019801   \n",
       "2                      0.041344                      -0.019763   \n",
       "3                      1.480860                      -0.019844   \n",
       "4                     -1.096552                      -0.019907   \n",
       "\n",
       "   abs_max_roll_mean_1000  abs_min_roll_mean_1000  \n",
       "0               -0.047319                0.294366  \n",
       "1               -0.201906               -0.502110  \n",
       "2                0.187627               -0.130290  \n",
       "3               -0.062922                0.505984  \n",
       "4               -0.196167               -0.473694  \n",
       "\n",
       "[5 rows x 155 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_test_X = pd.read_csv('./scaled_test_X_V0.0.csv')\n",
    "# # # 删掉seg_id\n",
    "# scaled_test_X.drop(columns = ['seg_id'],inplace = True)\n",
    "scaled_test_X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='5'>模型训练和预测</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分割数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X_train, train_X_verify, train_y_train, train_y_verify = train_test_split(scaled_train_X, \n",
    "                                                                                train_y,\n",
    "                                                                                train_size=0.8, \n",
    "                                                                                test_size=0.2, \n",
    "                                                                                random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>mean_change_abs</th>\n",
       "      <th>mean_change_rate</th>\n",
       "      <th>abs_max</th>\n",
       "      <th>abs_min</th>\n",
       "      <th>std_first_50000</th>\n",
       "      <th>std_last_50000</th>\n",
       "      <th>...</th>\n",
       "      <th>max_roll_mean_1000</th>\n",
       "      <th>min_roll_mean_1000</th>\n",
       "      <th>q01_roll_mean_1000</th>\n",
       "      <th>q05_roll_mean_1000</th>\n",
       "      <th>q95_roll_mean_1000</th>\n",
       "      <th>q99_roll_mean_1000</th>\n",
       "      <th>av_change_abs_roll_mean_1000</th>\n",
       "      <th>av_change_rate_roll_mean_1000</th>\n",
       "      <th>abs_max_roll_mean_1000</th>\n",
       "      <th>abs_min_roll_mean_1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.108000449718552</td>\n",
       "      <td>-0.194612108541045</td>\n",
       "      <td>-0.177889292023322</td>\n",
       "      <td>0.279983819099870</td>\n",
       "      <td>0.445825791337080</td>\n",
       "      <td>-0.024535783720093</td>\n",
       "      <td>-0.185514914486404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.089787032675796</td>\n",
       "      <td>-0.156760384923464</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047318661555123</td>\n",
       "      <td>0.127043372143013</td>\n",
       "      <td>-0.093304459454173</td>\n",
       "      <td>-0.044583132352107</td>\n",
       "      <td>-0.199408606643107</td>\n",
       "      <td>-0.210435468752101</td>\n",
       "      <td>0.681415903593577</td>\n",
       "      <td>-0.019859450661234</td>\n",
       "      <td>-0.047318661555123</td>\n",
       "      <td>0.294365544530510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.360285783412028</td>\n",
       "      <td>-0.073582050613659</td>\n",
       "      <td>-0.042320563226568</td>\n",
       "      <td>0.034776592887460</td>\n",
       "      <td>-0.218785617617470</td>\n",
       "      <td>0.707466343286253</td>\n",
       "      <td>-0.060885784833906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022277232849547</td>\n",
       "      <td>-0.230500470040194</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.201906442198004</td>\n",
       "      <td>-0.054943359738177</td>\n",
       "      <td>-0.604034235925073</td>\n",
       "      <td>-1.160102810563681</td>\n",
       "      <td>-1.381691966507248</td>\n",
       "      <td>-0.801642391651589</td>\n",
       "      <td>-0.062096035423744</td>\n",
       "      <td>-0.019801250601101</td>\n",
       "      <td>-0.201906442198004</td>\n",
       "      <td>-0.502109710115231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.354594029525078</td>\n",
       "      <td>0.046861671468020</td>\n",
       "      <td>0.309425327705551</td>\n",
       "      <td>-0.165161606947274</td>\n",
       "      <td>-0.329554185776557</td>\n",
       "      <td>-0.283631542575929</td>\n",
       "      <td>0.262476281291494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.464012754459941</td>\n",
       "      <td>-0.107462539793626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187626782455569</td>\n",
       "      <td>0.030013798038445</td>\n",
       "      <td>0.164505754918557</td>\n",
       "      <td>0.372875310475684</td>\n",
       "      <td>0.159202107433392</td>\n",
       "      <td>-0.025132820174525</td>\n",
       "      <td>0.041344484417079</td>\n",
       "      <td>-0.019763154292099</td>\n",
       "      <td>0.187626782455569</td>\n",
       "      <td>-0.130289890205013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.046786855620226</td>\n",
       "      <td>-0.286282836810129</td>\n",
       "      <td>-0.287809882939609</td>\n",
       "      <td>0.212080279533356</td>\n",
       "      <td>0.778131495814374</td>\n",
       "      <td>0.928722594299049</td>\n",
       "      <td>-0.259618721306808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.299988745590724</td>\n",
       "      <td>-0.306564644957183</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062922122207485</td>\n",
       "      <td>0.175396155699781</td>\n",
       "      <td>0.204151255272133</td>\n",
       "      <td>0.261925897191747</td>\n",
       "      <td>-0.040130881552465</td>\n",
       "      <td>-0.095413785617437</td>\n",
       "      <td>1.480860097286391</td>\n",
       "      <td>-0.019844153975919</td>\n",
       "      <td>-0.062922122207485</td>\n",
       "      <td>0.505984244411061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.527551655908804</td>\n",
       "      <td>-0.088360646300706</td>\n",
       "      <td>0.049279929203671</td>\n",
       "      <td>0.008369660833816</td>\n",
       "      <td>-0.108017049458369</td>\n",
       "      <td>0.901030207556418</td>\n",
       "      <td>0.023323086552917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.093441075312032</td>\n",
       "      <td>0.150401680174381</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196166836231523</td>\n",
       "      <td>-0.048450596268665</td>\n",
       "      <td>-0.831876198062004</td>\n",
       "      <td>-1.621291558724861</td>\n",
       "      <td>-1.277308819562885</td>\n",
       "      <td>-0.505054000434841</td>\n",
       "      <td>-1.096551828363636</td>\n",
       "      <td>-0.019906985497754</td>\n",
       "      <td>-0.196166836231523</td>\n",
       "      <td>-0.473693762511371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 155 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                mean                std                max                min  \\\n",
       "0 -0.108000449718552 -0.194612108541045 -0.177889292023322  0.279983819099870   \n",
       "1 -1.360285783412028 -0.073582050613659 -0.042320563226568  0.034776592887460   \n",
       "2  0.354594029525078  0.046861671468020  0.309425327705551 -0.165161606947274   \n",
       "3  0.046786855620226 -0.286282836810129 -0.287809882939609  0.212080279533356   \n",
       "4 -1.527551655908804 -0.088360646300706  0.049279929203671  0.008369660833816   \n",
       "\n",
       "     mean_change_abs   mean_change_rate            abs_max  abs_min  \\\n",
       "0  0.445825791337080 -0.024535783720093 -0.185514914486404      0.0   \n",
       "1 -0.218785617617470  0.707466343286253 -0.060885784833906      0.0   \n",
       "2 -0.329554185776557 -0.283631542575929  0.262476281291494      0.0   \n",
       "3  0.778131495814374  0.928722594299049 -0.259618721306808      0.0   \n",
       "4 -0.108017049458369  0.901030207556418  0.023323086552917      0.0   \n",
       "\n",
       "     std_first_50000     std_last_50000  ...  max_roll_mean_1000  \\\n",
       "0 -0.089787032675796 -0.156760384923464  ...  -0.047318661555123   \n",
       "1  0.022277232849547 -0.230500470040194  ...  -0.201906442198004   \n",
       "2  0.464012754459941 -0.107462539793626  ...   0.187626782455569   \n",
       "3 -0.299988745590724 -0.306564644957183  ...  -0.062922122207485   \n",
       "4 -0.093441075312032  0.150401680174381  ...  -0.196166836231523   \n",
       "\n",
       "   min_roll_mean_1000  q01_roll_mean_1000  q05_roll_mean_1000  \\\n",
       "0   0.127043372143013  -0.093304459454173  -0.044583132352107   \n",
       "1  -0.054943359738177  -0.604034235925073  -1.160102810563681   \n",
       "2   0.030013798038445   0.164505754918557   0.372875310475684   \n",
       "3   0.175396155699781   0.204151255272133   0.261925897191747   \n",
       "4  -0.048450596268665  -0.831876198062004  -1.621291558724861   \n",
       "\n",
       "   q95_roll_mean_1000  q99_roll_mean_1000  av_change_abs_roll_mean_1000  \\\n",
       "0  -0.199408606643107  -0.210435468752101             0.681415903593577   \n",
       "1  -1.381691966507248  -0.801642391651589            -0.062096035423744   \n",
       "2   0.159202107433392  -0.025132820174525             0.041344484417079   \n",
       "3  -0.040130881552465  -0.095413785617437             1.480860097286391   \n",
       "4  -1.277308819562885  -0.505054000434841            -1.096551828363636   \n",
       "\n",
       "   av_change_rate_roll_mean_1000  abs_max_roll_mean_1000  \\\n",
       "0             -0.019859450661234      -0.047318661555123   \n",
       "1             -0.019801250601101      -0.201906442198004   \n",
       "2             -0.019763154292099       0.187626782455569   \n",
       "3             -0.019844153975919      -0.062922122207485   \n",
       "4             -0.019906985497754      -0.196166836231523   \n",
       "\n",
       "   abs_min_roll_mean_1000  \n",
       "0       0.294365544530510  \n",
       "1      -0.502109710115231  \n",
       "2      -0.130289890205013  \n",
       "3       0.505984244411061  \n",
       "4      -0.473693762511371  \n",
       "\n",
       "[5 rows x 155 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_test_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(max_depth=3,learning_rate=0.1,Missing=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型参数调试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -2.160261 using {'learning_rate': 0.025, 'max_depth': 3}\n",
      "-2.230104 (0.392254) with: {'learning_rate': 0.015, 'max_depth': 3}\n",
      "-2.238260 (0.395214) with: {'learning_rate': 0.015, 'max_depth': 4}\n",
      "-2.242320 (0.389942) with: {'learning_rate': 0.015, 'max_depth': 5}\n",
      "-2.248506 (0.393117) with: {'learning_rate': 0.015, 'max_depth': 6}\n",
      "-2.163136 (0.431498) with: {'learning_rate': 0.02, 'max_depth': 3}\n",
      "-2.167421 (0.433403) with: {'learning_rate': 0.02, 'max_depth': 4}\n",
      "-2.172815 (0.429366) with: {'learning_rate': 0.02, 'max_depth': 5}\n",
      "-2.171993 (0.430295) with: {'learning_rate': 0.02, 'max_depth': 6}\n",
      "-2.160261 (0.456790) with: {'learning_rate': 0.025, 'max_depth': 3}\n",
      "-2.164751 (0.457953) with: {'learning_rate': 0.025, 'max_depth': 4}\n",
      "-2.163604 (0.447589) with: {'learning_rate': 0.025, 'max_depth': 5}\n",
      "-2.165737 (0.448974) with: {'learning_rate': 0.025, 'max_depth': 6}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "learning_rate = [0.015,0.02,0.025]\n",
    "max_depth = [3, 4, 5, 6]\n",
    "param_grid = dict(max_depth=max_depth,learning_rate=learning_rate)\n",
    "# kfold：将数据分割的份数\n",
    "## 分类器\n",
    "# kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=7)\n",
    "## 回归模型\n",
    "kfold = 5\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, scoring=\"neg_mean_absolute_error\", n_jobs=-1, cv=kfold,refit=True)\n",
    "grid_result = grid_search.fit(scaled_train_X, train_y)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "# 打印模型的分数\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看特征重要性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 720x2880 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoEAAAFlCAYAAACUbjKxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xlcjen/x/HXaU8lUZqQGWVkm5Bt\nRpbJXtaxRBHZGcuQpUiWoayFyWQ3yKAoxjpjxDAyY5+xjH1LQqW9tJ7790e/zldTJ+cYFK7n4+Hx\ncK5z3dd93Z88Hq7u5X3LJEmSEARBEARBED4oGqU9AUEQBEEQBOHtE4tAQRAEQRCED5BYBAqCIAiC\nIHyAxCJQEARBEAThAyQWgYIgCIIgCB8gsQgUBEEQBEH4AIlFoCAIQhllY2NDt27d6NGjh+KPt7f3\nK4936dIlZs2a9RpnWFhERATz589/Y+Mr8/DhQ8aPH//W9ysI7zqt0p6AIAiCoNzmzZupWLHiaxnr\n9u3bPH369LWMVZx27drRrl27Nza+MjExMdy7d++t71cQ3nUyERYtCIJQNtnY2PDHH38Uuwi8c+cO\nvr6+JCUlkZeXh5ubG3369EEul+Pn58fff/9Neno6kiQxf/58qlSpgouLC6mpqXTs2JGePXsyb948\n9u/fD8Dp06cVnwMDA/nrr7+IjY3FxsaGpUuXsmrVKg4fPoxcLqdq1arMnj0bc3PzQnMKDw/nl19+\nYc2aNbi5uVGvXj3++usvEhIScHZ2Jj4+njNnzvD8+XOWL1+OjY0Nbm5u1K1bl/Pnz5OYmEiPHj2Y\nMGECAEeOHGHlypXI5XIMDAyYPn06tra2heb36aefcvnyZZ4+fUrTpk3ZsGEDq1evJiIigszMTJ4/\nf46npycdOnQgMDCQR48eERcXx6NHjzA3N2fJkiVUrlyZe/fuMWvWLBISEtDQ0GDMmDE4OTnx9OlT\nvv32Wx4/fkxOTg5dunRh9OjRb/6HLwhvgyQIgiCUSbVq1ZK6du0qde/eXfEnPj5eysnJkZycnKQr\nV65IkiRJKSkpkqOjo3Tx4kXpwoUL0vjx46W8vDxJkiRpzZo10qhRoyRJkqSwsDBp5MiRkiRJ0p9/\n/il16dJFsa8XP3/33XdSp06dpJycHEmSJGn37t3SxIkTFZ937NghDR8+vMh8Xxx/4MCB0rhx4yRJ\nkqS//vpLqlWrlhQRESFJkiT5+vpKM2fOVPQbMWKElJ2dLSUnJ0udOnWSjh49Kt2+fVtq0aKFFBUV\nJUmSJJ06dUqyt7eXUlNTi8zvxblHR0dLbm5u0vPnzyVJkqT9+/dLXbt2VRxXu3btpNTUVEmSJGnU\nqFHSihUrJEmSpJ49e0pbt26VJEmSYmJiFP3c3NwU887MzJTc3NykAwcOqPujFIQySVwOFgRBKMOK\nuxx8+/ZtoqKimDFjhqItMzOTf/75B1dXV4yNjdmxYwcPHz7k9OnTGBgYqL3fhg0boqWV/1/EsWPH\nuHz5Mr179wZALpfz/Pnzl47RoUMHACwtLQFo1aoVANWrV+fMmTOKfv369UNbWxttbW06d+7MyZMn\nsbKy4vPPP1ds+8UXX1CxYkWuXLlSZH4vqlq1KosXL2bfvn08ePBAcUa0QLNmzTA0NASgbt26JCcn\nk5SUxPXr1+nbty8AFhYWHDlyhIyMDM6ePUtycjIrVqwAICMjg+vXr+Pk5KRqKQWhzBKLQEEQhHdM\nXl4eRkZG/PTTT4q2+Ph4jIyM+O233/D19WXIkCG0a9cOKysr9u7dW2QMmUyG9MLdQDk5OYW+L1eu\nnOLvcrmc4cOH4+rqCkB2djbJyckvnaeOjk6hz9ra2sX2e3ExJ0kSGhoayOVyZDJZoX6SJJGbm1tk\nfi+6evUqX3/9Ne7u7tjb29O0aVPmzp2r+F5PT0/x94IaFOz/xf3dvXsXMzMzJElix44d6OvrA5CQ\nkICuru5Lj10Q3gXi6WBBEIR3TI0aNdDT01MsAh8/fkzXrl25cuUKkZGRODg44OrqSv369Tly5Ah5\neXkAaGpqKhZRFStWJCYmhmfPniFJEgcOHFC6v5YtW7Jr1y7S0tIAWLFiBdOmTXttx7N3717kcjnJ\nyckcOnSItm3b8sUXX3Dy5EkePnwIwB9//MHjx49p0KBBke01NTUVi9izZ89Sv359hgwZQrNmzYiI\niFAcvzKGhobUq1ePPXv2APn1dHFxITMzk4YNG/LDDz8AkJKSgouLCxEREa/t2AWhNIkzgYIgCO8Y\nHR0dgoKC8PX1Zf369eTm5vLNN9/QuHFjKlSowOTJk+nWrRu5ubnY29srHuho2LAh33//PePGjWPl\nypX079+f3r17Y2Zmxpdffsnly5eL3V/fvn15+vQpzs7OyGQyLCwsWLhw4Ws7nszMTPr06UN6ejqu\nrq588cUXAMyePZtx48aRl5eHnp4eq1evxsjIqMj2NWvWRFdXlz59+rB69WoOHz6Mo6MjcrkcBwcH\nkpOTFQtYZfz9/Zk7dy7BwcHIZDJ8fX0xMzNj6dKlzJs3j27dupGdnU3Xrl3p3r37azt2QShN4ulg\nQRAEodS4ubkxYMAAOnfuXNpTEYQPjrgcLAiCIAiC8AESZwIFQRAEQRA+QOJMoCAIgiAIwgdILAIF\nQRAEQRA+QGIRKAiCIAiC8AESETHCByU3N4/ExIzSnsY7wcSknKiVGkS9VCdqpTpRK9WJWilnZlY0\nWgnEmUDhA6OlpVnaU3hniFqpR9RLdaJWqhO1Up2olfrEIlAQBEEQBOEDJC4HC4IgCIIgvGaSJOHr\nOwcrq5q4urqRl5fHypXLOX36FHl5ebi4DKRnzz4A3Lt3l8WLfXn+/DkyGYwePZ7mzb9443MUZwLf\nQQ8ePMDFxQVXV1dmz56NXC4HYPTo0fTv3x83NzeGDx/+n/YRHR2Ns7MzAG3btiUrK0ut7c+ePcv1\n69eLtE+aNInTp0+/dHs/Pz+2b9+u+BwaGkqvXr1wdnbm2LFjQP6L3IcOHYqrqysTJ07k+fPnas1R\nEARBEN6E+/fv8c03Y/jtt/+9Z/qnn8J5+PABW7aEsG7dFkJDt/PPP1cA8PdfSJcu3dm0aRvTp89i\n1iwvxXu+3ySxCHwHLViwgIkTJ7Jt2zYkSVK8zDwqKort27cTHBzM+vXrS3WOYWFhxMbGqr1dQkIC\nw4cP5+jRo4q2uLg4goOD2bFjBxs2bCAgIIDs7GyCgoLo2rUr27Zto27duoSEhLzOQxAEQRCEVxIe\nHkrXrj1xcGivaDtx4hhOTt3R0tKifPnytGvXkcOHDwEgl8tJTU0FICMjAx0d3bcyT3E5uIxLT09n\n8uTJpKSkULNmTS5evEhSUhLNmjUDoHXr1kRGRtKoUSNSUlIYPXo0KSkpjBw5EgcHh2LHjI6OZsyY\nMVSoUIHWrVtjb2/PvHnz0NTURFdXl3nz5qk1Ry8vL6KiosjKymLYsGFUr16d33//natXr1KzZk2O\nHTvGzp07MTMz49mzZy893vHjx3PixAlF26VLl2jUqBE6Ojro6OhQvXp1rl+/zvnz5xk1apSiDgEB\nAbi7u5c4frfJP6l1bIIgCIJQko1ebYu0eXh4AnD27J+KttjYp1SubK74XLmyOXfu3Fb0/+ab0YSG\nbiMxMYG5c/3Q0nrzSzSxCCzjtm3bho2NDZMmTeLChQucPHkSSZKQyWQAGBgYkJqaSk5ODkOHDmXQ\noEEkJyfj4uKCra0tlSpVKnbcuLg4wsLC0NHRoVevXvj6+lKnTh2OHDnCwoULmTZtmkrzS0tL4/Tp\n04SFhQEQGRlJ/fr1adWqFU5OThgZGbFlyxb27duHTCajV69eJY5naWmJpaVloUVgWloaRkb/e7zd\nwMCAtLS0Qu0FdRAEQRCEt0lZ/AqAnp42hoa6mJkZoaEho2JFA0V/Q0Nd9PV1KF9eh2+/9WbRokU4\nODjw119/MXr0aOztm2FhYfFG5y4WgWVcdHQ0rVq1AsDOzg4dHZ1C9wmkp6dTvnx5TE1N6d+/P1pa\nWlSqVIk6depw7949pYvAatWqoaOjA0BsbCx16tQBoGnTpvj7+6s8P0NDQ3x8fPDx8SEtLY3u3bsX\n+v7u3bvUrFlTsS9bW1vVD/6FfaSnpys+p6enY2RkpGjX09NT1OFl9vn3IC5OLBZVYWZmJGqlBlEv\n1YlaqU7USnWlVauS9pmZmUNaWhZxcalUqlSZW7ceUKWKFQD37j3E2LgiZ878RXp6BvXrNyEuLpWq\nVa35+OMa/P77n4UuJ/8XIifwHWVjY8OFCxcAuHHjBtnZ2dStW1fxcMWJEydo0qQJp06dYuLEiUD+\nIunWrVtYWVkpHVdD438/+sqVKyse4jh79iyffPKJyvOLjY3l6tWrfP/996xdu5YlS5aQm5uLTCZD\nkiQsLS25ffs2mZmZ5OXlce3aNXVLgK2tLefPnycrK4vU1FTu3LlDrVq1sLOz4/jx44o6NG7cWO2x\nBUEQBOFtaNWqNQcO7CU3N5fU1FQiIg7TqtWXVK1qSXp6Gpcv/w3Ao0fR3L9/j1q1ar/xOYkzgWVc\n37598fb2ZsCAAVSpUgUAT09PfHx8CAgIwMrKik6dOqGpqcnJkydxdnZGQ0MDDw8PKlasqNI+5s+f\nz7x585AkCU1NTfz8/FSen5mZGXFxcfTs2ZNy5coxdOhQtLS0aNCgAUuXLmX58uV888039O/fn4oV\nK6Kvr692DczMzHBzc8PV1RVJkpg0aRK6urqMGTMGT09PQkNDMTExUesMpiAIwrtgz549rFu3QfE5\nPT2N2Nin7N59kOXLl3Lr1g309fVxcupGnz79S3Gmwsv07NmHR48e4e7uSm5uDt2796JRo/yTF35+\nS1mxwp/s7Cw0NTWZNs2bqlWrvfE5ySRJkt74XoRX9uDBA7y8vJDJZFhZWREZGcmxY8cYPXo0SUlJ\naGtro6ur+5+eBo6OjsbDw4PQ0FDatm3LoUOH0NVV/cmks2fPYmRkRO3ahX9rmTRpEv3796d58+Yl\nbu/n50eNGjVwcXEB8uNgduzYgZaWFmPGjMHBwYGEhASmTJlCZmYmlStXZsGCBejr6xfb92XEpRXV\niMtQ6hH1Up2oleperFVubi5jx47A0bErV65cUiwW5HI506dP5quv+mJv36qUZ1x6xL8r5ZRdDhZn\nAsu4gjiY5s2b4+3tTWZmJpAfB3PgwAHFAyLFCQkJYf/+/UXaPTw8aNSokVrziIiIYNOmTUXaBw0a\nREREBE5OTkUWgcrExMTg6elJTk4Od+/eJTMzk48++oi4uDhcXFwIDg4mLCyMrKwsXF1dsbe3V8TB\n9OrVi7Vr1xISEkKXLl2K7Vtw/6EgCML7ZOvWTZiYmNCzZ2/CwkKYNGkampqaaGpq8sUXLfntt4gP\nehEoqE8sAsuQl8XBODg4oKOjQ3x8vEpxMP369cPe3v61xMG0a9eOdu3aFYmDsbCwUDsOpkqVKgQH\nB/Pw4UMSEhI4ceIEpqamuLi4EBERoXIcjKWlZbF9X+XhE0EQhLIsKSmJHTt+ZMOGYADq1q3PL78c\nxNa2IdnZ2Rw/fvStRIoI7xfxL6YMEXEw6sXBKOtbEpETKAhCWbbPv0eRNjMzI8LCfqRDh/Y0bJif\n5DBnjg+LFi1ixAg3TE1N+fLL1ly8eLHEuJIPwYd+/OoSi8AyRMTBqBcHo6xvSUREjOrE/TXqEfVS\nnaiVcv+uS0Gt9u7dz8SJUxTfP3nylKFDx1C+vDEAW7ZsxMzsow+6ruLflXIiIuYdIOJg1IuDUdZX\nEAThfZKSksKjRw/57LMGiraffgpj/frVACQkPGPfvp/o0KFzaU1ReEeJM4FliIiDUS8Oply5csX2\nFQRBKG2HDu0nJGSb4vOL0S4VK+ZftZkxYyqmpqaKV4wp8+jRQypVMi10z5+bmzvz5s3Czc0ZSYLh\nw0dRp069N3MwwntLRMSUUVlZWTg6OnL06NHSnsp7R1wuUI24tKIeUS/VfWi1ejHapWfP3gD8+ONm\ntm8Ppm3bDiUuAj+0Wv0XolbKiYiY91xISAi7du3i7t27AOjr6/PJJ58wefJk1qxZo1amYElxMHXq\n1FErU7AgDqZASkoKWlpatGnThgkTJijaX2emoCAIQlnyYrQLwIUL5zh9+g969OhNampKKc9O+JCJ\nRWAZpaurq9ZZwH79+nHs2DGmTJlC8+bNmTVrFq1ataJRo0YqZQq+qCAOpjjR0dEqzwn+FwdTwMvL\nCycnJ1q3bq3WOAkJCUybNo379+8zbNgwIP+pZ5ETKAhCWfbvaJf4+DhWrPDH3z+Qn34KK+XZCR86\nsQh8R70sU7B169ZERkbSqFEjlTIFIX+B9zoyBQv8O1OwevXqamcKvni848ePLxQnc+nSJbVzAkVE\njCAIr9tGr7ZKv9u7N5xWrdpQtWo1cnNzmTPHmwkTPDA1NX2LMxSE4olF4DtKZAoqzxQsSXEZXIIg\nCG/K8eMRzJw5EzMzIy5evMiTJzGsWrUCgPj4ePLy8tDQkPD19VU6hsi+U52olXrEIvAdJTIFlWcK\nvoy4cVg14iZr9Yh6qe5DqVVKSgoPHjzA0vJT4uJSqVatJrt2/e9Vnhs2rCE5OYmJEz2V1uNDqdXr\nIGqlnMgJfM+ITEHlmYJC2fXLLwcZPNgFd3dXRo8eyvXr/xT6fsUKf6ZNm1hKsxOE16u4aBdBKEvE\nv8x3lMgUVJ4pKJRNUVH3CQpawYYNP2Jqasoff5xkxoyphIcfACAi4ld+/fUQdevWL+WZCsLrUadO\nPUJC9ij9ftiwUW9xNoJQlFgElnEPHjzAy8sLmUzGp59+yuzZs9HQ0EBbW5uxY8cyduxYNm7ciKOj\nIzVq1GDr1q1FxvD29n7pfuzt7YmMjERXV5c7d+5gbW1N3bp1+fHHH4v0DQ0NBVA8vXzjxg1SUlJo\n2rSpoo9MJqN8+fIMGjSo0P1+/fv3p3///gBYW1vj5OTEpk2biI+PV8TDHD16lO+//x4tLS169+6N\ns7MzmZmZTJ06lWfPnmFgYECnTp2oWLGi4uZqbW1tkpOTVS2rUAq0tXXw9PRR/Mxq165LQsIzcnJy\nePQomm3btuDuPpwzZ/4s5ZkKgiB8GMQisIxbsGABEydOVMS+RERE0KFDB/bs2cOWLVtITExUe8yQ\nkBD2799fqC0pKYmLFy+qPVZERATz589HW1sbc3NzRfugQYNeum1mZiYzZ87k0qVLdOzYkZiYGKZO\nncrly5epV68eGhoaLFiwgLt372Jubk6tWrUYP348Bw4cICgoCE9PTxYsWMCuXbvQ19fHxcUFBwcH\nzMzM1D4O4c2zsKiChUX+WWtJkggMXEbLlq3Jyclh3rxZeHvP5vp19W8LEARBEF6NWASWIarGvnTo\n0AFjY2O2bt1Khw4dXpop6ODggJWVFVZWVgwePJiDBw8il8uRyWTMnDmT2rVrY29vT6NGjV46x2XL\nlvHnn38il8vp0qULjo6OyGQyZDIZU6dO5fHjx6xatYrt27eTk5NT4v2HWVlZ9OzZkxYtWnD37l2q\nVKmCj48PS5YsYcOGDUB+OHSDBg04cOAAw4cPV9QhKCiIO3fuUL16dYyN81+g3rhxY86dO4ejo6PS\nfYqImLejpMiM58+f4+s7h9jYp/j7B7Jw4Tz69OmHlVVNsQgUBEF4i8QisAxRNfYFUOvNGI8fPyY8\nPBwTExMmTJiAm5sb7du359q1a8yYMYPw8HCVx9qzZw9bt27F3Nyc8PBwzM3N+eqrrzA1NcXW1hYP\nDw927txJhQoVGDlyZIljGRsb07Jly0L7Vxb78mJ7QR1eJSJGeDuUPYkWExPDuHGjsba2Zvv2H0lK\nSuLy5b+IiXlIWNgOkpOTSU1NZcYMD9atW/eWZ/3fiXgK1YlaqU7USnWiVuoRi8AyRNXYF3WZmJhg\nYmICwJ07dxT37tWpU4cnT56oNVZAQAABAQHEx8cr5logPj4eQ0NDxb5UObP4b8piX15sL6jDq0TE\n7PPvISIEVPRf4haK2y4jI53Bgwfi6NiFoUNHkpqag6amAbt3H1L0OXhwH7/9FoGfX8A793MS8RSq\nE7VSnaiV6kStlBMRMe8AVWNf1PVi7Iu1tTXnzp0D4Nq1a2ql1mdnZ/Pzzz8TEBDA5s2b2b17N48e\nPUImkyGXy6lQoQKpqakkJCQAcPnyZbXnam1tzYMHD0hKSiI7O5tz587RqFEj7OzsOH78OJBfh8aN\nGyvtK5RNYWGhPH36mBMnfsPd3VXxJzk5qbSnJgiC8EESZwLLEFVjX/6LadOm4ePjw8aNG8nNzS0x\npf7fdHR0MDY2pkePHhgbG2Nvb0+VKlWoX78+ixcvxtramgULFjBs2DCMjY1fKRtLW1sbLy8vhg0b\nhiRJ9O7dG3Nzc1xcXPD09MTFxQVtbW38/f2V9hXKJje3IVSubM62bcHIZDL09PSYOHEKxsYVFH1u\n3bpZijMUBEH4sMgkSZJKexJCUVlZWTg6Opb4wMfr5OXlhZOTE61bt34r+/svQkJC6NWrF9ra2q+0\nvbhcoJrXfWklKuo+48ePKpQTuGTJgkI5gcuWLaJu3fosXrz8te33bRGXolQnaqU6USvViVopp+xy\nsDgT+J6IiIhg06ZNRdoHDRpEhw4d1Brr0qVLLFmypEi7o6Mjrq6uao2VnZ3NsGHDirTXqFGDb7/9\nVq2xCqxZs4aePXu+0rZC6RE5gYIgCGWLWASWovDwcI4dO0ZmZiZxcXEMGjSIiIgIbt26xbRp05g6\ndSr9+vVDQ0ODxo0bM2XKFJ48ecKcOXPIysoiKSmJsWPH0r59e5YvX06zZs24ceMGMpmMoKAgpQ9J\n3L9/n5kzZ5KTk4Oenh7Lli0D8s+wrV+/nrS0NObMmYOtrS3+/v5cuXKF9PR0xT1+gYGBREdH8+zZ\nM2JiYpg+fTqtWrXi2LFjfPfddxgaGmJsbIyNjQ3jx4+nYcOGnD17FkmScHd3VxrhEh0dzZgxY6hQ\noQKtW7emQYMGrFy5EsjPFFy0aBHnzp0jLi6OSZMmERQUhL+/v0pjC6VP5AQKgiCULWIRWMrS09PZ\nuHEjBw4cYNOmTYSGhnL69Gk2bdpEVFQUYWFh6OvrM3XqVCIjI5HJZAwZMoTmzZtz4cIFAgMDad++\nPenp6XTp0gUfHx8mT57MiRMn6NKlS7H7XLRoESNHjqR169YcPHiQf/7Jf39rvXr1+PrrrwkPDyc8\nPBwrKyvKly/PDz/8oMgFfPr0KZB/f+D69euJjIxk48aNtGjRgvnz5xMSEoKpqSmTJ08G4Pjx40RH\nR7Njxw6ysrJwdnbG3t5e6VPOcXFxhIWFoaOjw48//siSJUswNzdn9erV/Pzzz4wZM4ZVq1axbNky\ntccGkRP4tuzz76H0u4yMDLy8vHj69Anr169n1qxZDBkymObNG/Ho0T10dLTe2ZiHd3XepUHUSnWi\nVqoTtVKPWASWsjp16gBgZGSEtbU1MpkMY2NjMjIySEhIUGTtpaen8/DhQxo3bsyqVavYtWsXMpms\nUIRM3bp1AbCwsCArK0vpPu/du6d4itbJyQmA/fv3U69ePQBMTU3JzMxEV1eXhIQEPDw8KFeuHBkZ\nGeTk5BSa90cffUR2djYJCQkYGhoqLvU1adKE+Ph4bt68ydWrV3FzcwMgNzeXmJgYpQu1atWqoaOj\nA4C5uTm+vr6UK1eOp0+fYmdnV6ivumODiIhRx+uOiAF48uQJnp6T+OSTTwgI+J6HD2M5c+Yst27d\nYf36jaSkJJOensbgwUNYuvS7/zL9t07cj6Q6USvViVqpTtRKOXFPYBlVEARdXLuFhQUbN25EW1ub\n8PBw6tSpw4oVK+jbty9t2rQhLCyM3bt3v3Ssf7O2tuby5cu0aNGCvXv3Kt65++/tT5w4wePHj1m+\nfDkJCQn8+uuvFDxH9O++lSpVIj09nYSEBCpWrMjff/9N1apVsbKyonnz5sybNw+5XE5QUBDVqlVT\nOrcX42xmzpzJkSNHMDQ0xNPTs9C+5XK52mMLpSsjI53x40cpcgIBKlfW46efflb0KcgJfBcfDBEE\nQXjXiEVgGaWlpYW7uztubm7k5eVRtWpVHB0d6dy5M76+vqxZswYLC4tXenfwtGnTmDVrFqtWrUJP\nT48lS5Zw9erVIv1sbW0JCgrC2dkZHR0dLC0tiY2NLXZMDQ0NfHx8GDFiBEZGRsjlcj7++GPatm3L\nmTNncHV1JSMjg/bt22NoaKjSPHv06IGzszPly5fH1NRUse8mTZowcuRItmzZ8spjC29fWFgoT57E\nsHXrJrZu3YyGhgwzs8qsXr1RERPz668/c/u2iIkRBEF4G0REjPDarFmzhiFDhqCjo8OUKVNo2bJl\nmXyKV1wuUI2IiFGPuBSlOlEr1YlaqU7USjlxOfg/io6OxsPDg9DQ0NKeykvFxMRw5coVgoODi3z3\nX6JZ4H95gvHx8dy9e5cpU6YovjMwMMDZ2Rk9PT2qVq2quN+wQFZWFnv37kUul7N//35F+/Pnz7l/\n/z7r1q0r8Y0fDx48YOzYsYptExISmDJlCpmZmVSuXJkFCxagr6//yscmvFkiIkYQBKFsEYvA99Cf\nf/7J3bt3i10EvkkDBw5k4MCBSr+Pi4tj586dhIaG0q9fP0X7nTt3mDNnTokLwD179rBly5ZCl7+D\ngoLo2rUrvXr1Yu3atYSEhODu7v5ajkV4/UREjCAIQtny3i4C09LS8Pb2JjU1lcTERNq3b8/+/fs5\nePAgMpmMuXPn0qJFC6VBykFBQRw5coS8vDxcXFxo2bIlCQkJfP3118TFxWFjY8P8+fO5efMmCxcu\nRC6Xk5KSwsyZM7Gzs6Njx47Y2dlx7949KlWqRGBgIDk5OUybNo3Y2FgsLCw4e/YsJ0+e5MaNG8yf\nPx+AChUq4OfnpzTjz8vLi6SkJJKSkli1ahVLly7lyZMnJCYm0rp1a8aPH8/atWvJzMykUaNGVKtW\n7ZXGXrNmDatWreL8+fMAdO3alcGDB6tc//Pnz7No0SK0tLQoX748S5cuZfXq1dy+fZuVK1fi7OzM\nlClTkCQJMzOzl45nbGzM1q1bC/28zp8/z6hRowBo3bo1AQEBL10EioiYt2OjV1ul3z1//hxf3znE\nxj7F3z+QhQvn0adPP6ysaorfXDtyAAAgAElEQVRFoCAIwtskvaeuXLki/fLLL5IkSdKTJ0+kDh06\nSN9884105swZKSsrS3JycpJycnKK3fbq1atSv379pNzcXCkjI0OaN2+eFBUVJTVv3lxKSkqS8vLy\npLZt20rx8fHSgQMHpOvXr0uSJEl79+6VvL29JUmSpNq1a0sxMTGSJElSv379pIsXL0qbNm2SFi1a\nJEmSJN2+fVuqXbu2JEmS1LdvX+nWrVuSJElSaGioFBAQoPS4PD09pR9++EGSJEl6+PChFBoaKkmS\nJGVmZkrNmjWTJEmSwsLCpCVLlvynsY8ePSqNHTtWksvlUnZ2ttSnTx/p+vXrkqenp3T8+PFC+yjO\nwoULpbVr10p5eXnSr7/+Kj169Eh6+PCh1LdvX8X3ISEhkiRJ0oEDB6SBAwcqHetFLVq0UPy9ffv2\n0vPnzyVJkqSoqCipf//+Ko0hlJ5Hjx5J3bp1kyZOnCg9f/5cevz4sWRvby91795d6t69u9SmTRvJ\nzs5OGj58eGlPVRAE4b333p4JNDU1ZfPmzRw+fBhDQ0Nyc3NxdnZm9+7dxMXF0bZtW7S0ij/8e/fu\nYWtri6amJvr6+sycOZPo6GgsLS0xNjYG8iNRnj9/TuXKlQkKCkJPT4/09HTF06kmJiZYWFgA/8vt\nu3PnjuLdvNbW1lSsWBHIvxw6d+5cAHJycqhRo0aJx1bwfYUKFbh8+TJ//vknhoaGZGdnF+n7qmPf\nuXOHJk2aIJPJ0NbWpkGDBty5c6fEbV80evRoVq9ezeDBgzE3N8fW1rbQ/G7dukWPHvmBwnZ2dmzf\nvl3lsQsYGhqSnp6uqH1J+YAvEjcOq+Z132SdkZHO4MEDFRExqak5aGoasHv3IUWfgogYP7+Ad+7n\nJG5KV52olepErVQnaqWcsgdDNIptfQ9s3LiRhg0bsnTpUjp37owkSXzxxRdcu3aNsLAw+vTpo3Rb\nKysr/vnnH+RyOTk5OQwZMoTs7Oxic/h8fX2ZMGECixYtolatWkpz9ABq1arFxYsXAYiKilLc31aj\nRg0WLVpEcHAwU6dOpU2bNiUeW8HY4eHhGBkZ4e/vz9ChQ8nMzESSJDQ0NJDL5f9pbGtra8Wl4Jyc\nHC5evMjHH39c4rYv2rdvH1999RXBwcF8+umnhIaGFpqXlZWVohYFr6NTl52dHcePHwfyMw0bN278\nSuMIr88vvxxk8GAX3N1dGT16KNev//P/GY4rcHbuyePHjwgJ2YabmzPu7q64u7uSnJxU2tMWBEH4\nIL23ZwIdHByYM2cO+/bto0KFCmhqapKTk0OnTp04depUiQuaOnXq0KpVK1xcXJDL5bi4uCjeYvFv\n3bt35+uvv6ZSpUp89NFHJeb29enTBy8vLwYMGECVKlXQ1dUFYM6cOXh6epKXlwfkLyxV8cUXX+Dh\n4cH58+fR19fn448/JjY2llq1arFq1Srq1av3ymM7ODhw5swZ+vXrR05ODp07d1a8UUQVn332GV5e\nXpQrVw5tbW2+/fZbKlWqRE5ODkuWLOGbb75h0qRJHDx48JUDnseMGYOnpyehoaGYmJjg7+//SuMI\nr0dU1H2CglYUioCZMWMqQ4aM4MaN64SHH0BHR4egoBU8e/YMH5+iT6k7OXXDyalbKcxeEAThw/NO\n5QR6eXlx5MgRTp06pViUXb16lV69erFlyxaaN2+u8li+vr4MGTKEKlWqvKnpFnHhwgUyMjJo2bIl\n9+/fZ/jw4Rw5cuSt7f/27dv4+PggSRK1a9fGx8cHTU1NQkND2bFjB1paWowZMwYHBwel8Svq9C2r\nxOUC1ah7aeXx4xju3btLixYtAUhMTOCrr5xYtux7ypUzwMamNpCfB7h7905Wrlz7RuZdWsSlKNWJ\nWqlO1Ep1olbKvTc5gWZmZpw4cYL27dsD+ZcdLS0t1R7H29ubkJCQQnl1BTw8PEqMK3lVlpaWeHh4\nsHLlSnJzc5k1a1ax/bKzsxk2bFiR9v+a8RcQEMCECRMICgri119/5ezZsxgaGnLjxg26dOmCl5cX\nrq6u2NvbFxu/0qVLF4KDgwkLCyMrKwtXV1d27tzJlStXKFeuHGZmZly+fJm+ffsWW9eXWblyJadP\nny7S7ufn90o/Y+HtUhYB06jR/y7Tp6SksGnTOnr27F1a0xQEQRD+3xtbBObk5DB79mwePHiAXC5n\n+PDh+Pv7s2zZMjQ1NZk0aRLbt2/H2dmZJk2acOvWLYyNjQkICKBcuXJKx+3SpQv79++nffv2yOVy\nrl69ymeffabY54wZM3j48CF5eXkMGTKEzz//nAEDBhSJhtmyZQtz5swhNjaW6Ohonj17RkxMDNOn\nT6dRo0YcO3aM7777DkNDQ4yNjbGxsWH8+PHFzqm4mJjExESOHDnCggULAOjZsycbNmzA1dWVTZs2\noa2tzZkzZ2jdujWBgYFcvHiRjIwMfH192bNnD1paWqSnp2Ntbc2CBQsUZ9sGDhxIjRo1+PPPP/n1\n1185c+aMoqaWlpZ8++23aGtrFzvPwMBANDU1ady4MWPGjGHs2LEkJiZy/PhxxeKyevXqXL9+vdj4\nFUtLSxo1aoSOjg46OjpUr16dUaNGMXv2bNauXYuZmRnXr18nICBA6c8vMDCQBw8ekJiYSHJyMq6u\nrhw+fJh79+6xaNEixo0bR3BwMPv370cmk+Hk5ISlpaVaUTyamppK9y8iYl4fZTEw/46AKfDoUTTT\np0/G1rYhvXo5v61pCoIgCEq8sUXgzp07MTExwc/Pj8TERAYOHMjChQsVlyMXL16MoaEhmZmZdOvW\njaZNm7J48WJCQkIYMmSI0nFtbW359ddfycjI4K+//qJ58+aKp1ZDQkIwMTFhyZIlpKWl0atXL3bs\n2IGNjQ3nzp2jQYMGnDlzBm9vb7Zs2aIYU0dHh/Xr1xMZGcnGjRtp0aIF8+fPJyQkBFNTUyZPnlzi\nsd6+fRtPT09sbGzYt28f4eHhzJ07lyVLlpCRkcHt27epXr06mpqaBAYGEhYWhr6+PlOnTiUyMhLI\nf1Bi5syZpKWlUb58eX744QfkcjldunTh6dOnbNiwgXbt2jFgwAAiIyOJjIxEkiR8fHzYtm0blSpV\nYvny5ezevRtn5+L/g9XU1OTRo0cMGTIEQ0NDatSowcOHDwvlBhoYGJCWlkZaWpqi3cDAgNTU1EJt\nL+tbEj09PTZs2MDatWs5fvw4q1evJiwsjAMHDmBoaMjBgwfZtm0bMpkMd3d3WrZsWWyN7ezsePjw\nIZs3b8bCwoL+/ftz+fJlGjZsWOL+hdejuMsLMTExjBs3Gmtra7Zv/xE9PT0gP8B80qRJDB8+vNiz\n3O8LZZdchKJErVQnaqU6USv1vLFF4M2bNzl//jyXLl0CIDc3F0tLS4yMjNDW1qZOnTr5E9DSomnT\npkD+054nTpx46dht27YlIiKCU6dOMWbMGJYtWwbkx5q0aNECyI8Psba25uHDhy+NhimYy0cffUR2\ndjYJCQkYGhoqXm/VpEkT4uPjlc6nuJgYTU1NOnXqxOHDh/nrr7/o27cvUVFRJCQkMHLkSADS09N5\n+PAh8L9oFl1dXRISEvDw8KBcuXJkZGSQk5PDnTt3+OqrrxTzgfzXpsXGxjJx4kQAMjMzsbe3L7F2\nVatW5fDhw+zcuZOFCxfSsWNH0tPTFd+np6djZGRUbPxKQZsqfUtSt25dAIyMjKhZsyaQHwadlZXF\nzZs3iYmJUYQ+JycnExUVpVYUT0n2+fcQ94yo6GX31/z7u+IiYFJTc7hx4zrffDOWOXP8+PzzFu9t\n/cX9SKoTtVKdqJXqRK2Ue+sRMVZWVop7yNatW0fnzp35448/MDAwQEtLi59//hnIXxxev34dyH8D\nRMGioCTdunVjz549xMXFUb16dUW7tbU1586dA/LfGHLz5k2qVav20miYf8e5VKpUifT0dBISEgD4\n+++/S5yPspiYPn36sHfvXv7++2/s7e2pVq0aFhYWbNy4keDgYAYOHEiDBg0A0NDI/1GcOHGCx48f\nExAQgIeHhyL25cV4mb/++gvIXwB99NFHBAUFERwczOjRo0t8OGb06NHcv38fyD9jp6Ghga2tLefP\nnycrK4vU1FTu3LlDrVq1io1fUadvSYqLzylgZWVFzZo12bJlC8HBwfTq1YtatWqpFcUjlI6wsFCe\nPn3MiRO/KeJf3N1dWbXqOyRJYvXqlYq26dOnvHxAQRAE4Y16Y2cC+/fvz8yZMxk4cCBpaWm0b9+e\nwMBAfvzxRyRJwtXVVXEv37p164iJiaFKlSpMmjTppWNbWVmRmJhI796Fby53dnbGx8cHFxcXsrKy\nGDduHJUqVQJQKRqmgIaGBj4+PowYMQIjIyPkcnmJ2ymLiSl4mKFdu3ZoaGhQsWJF3N3dcXNzIy8v\nj6pVq+Lo6FhoLFtbW4KCgnB2dkZHRwdLS0tiY2MZMWIE06ZN49ChQ1SuXBktLS00NDTw9vZm5MiR\nSJKEgYEBixcvVjrPkSNH4uXlhba2Nvr6+syfPx8zMzPc3NxwdXVFkiQmTZqErq5usfEr5cqVU7nv\nq6pduzZffPEFLi4uZGdnY2tri7m5uVpRPELpcHMbQuXK5mzbFoxMJkNPT4+JE6dQq1ZtVq8O5NSp\nSDQ0ZFSrVp2pU2eU9nQFQRA+eKUeEdO2bVsOHTqkyMwrK9asWcOQIUPQ0dFhypQptGzZkp49e/7n\nca9evcrs2bPR0dGhTp06eHt7o6Ghwfz587lw4QIGBgZMmTJFcYawwPHjxzExMcHW1pZTp06xevXq\nQvc1vig6OhoPDw9CQ0PLbH0LPHjwgLFjxyqeJn4b0TTicoFq1L20EhV1n/HjRxXKCVyyZAFDhozg\nyJFfWLJkxUtzAt9l4lKU6kStVCdqpTpRK+XemYiYmJgYPD09i7Q3bdqUCRMmvLV5GBgY4OzsjJ6e\nHlWrVsXJyQk3N7ci/dSNbfHx8VE82bps2TL27dtH+fLluXfvHrt27SIpKYnhw4cTHh5eaLtq1aox\nY8YMNDU1kcvleHt7Fzt+TEwMHh4e3L59Gzc3N+Li4hg6dCjNmzd/q/UbN24cycnJhdoMDQ1ZtWqV\n4vOePXvYsmVLobN6qkbTKIuxKbiXUHj7tLV18PT0UdxLW7t2XRISnlGtmiVff/2NItvTxqYuu3fv\nLM2pCoIgCJSBReDRo0cLfa5SpQrBwcGlNJv/GThwIAMHDizUpu680tPTmTx5MikpKdSsWZOLFy+S\nkJCAnZ0dkP8gTEREBJaWlrRq1UpxyVhTU5O4uDjMzMwUY1lbWxMSEkJ0dDRjxoxh8eLFtG7dGnt7\ne+bNm4empia6urrMmzdPcT9hcHAwbdu2ZePGjUrPBHbo0IFGjRrx4MEDPv/8c1JTU7l06RI1atRg\nyZIlPH78GB8fH7KyshTjW1hY4O/vz5UrVwrF2AQGBhaJ22nVqpXS+hgbG7N161Y6dOigaFM1mkZZ\njM3LFoEiIub1+XdEjMgJFARBeLeU+iLwfbZt2zZsbGyYNGkSFy5c4OTJk1haWnLmzBmaNWvGsWPH\neP78OXXq1OGHH35gwIABPHnyhNu3b/P8+XOl48bFxREWFoaOjg69evXC19eXOnXqcOTIERYuXMi0\nadNUnuOjR4/YvHkzZmZmNGvWjJ07d+Lj40O7du1ISUlh0aJFuLm50aZNG/744w+WLl3K3Llzi42x\ngaJxOyUtAh0cHIq0vcloGuH1UnZ5ISMjAy8vL54+fcL69espXz6/X1RUFBMnjqVZs6aMGjXsvXyo\nR8RTqE7USnWiVqoTtVKPWAS+QdHR0YpFkJ2dHTo6Ovj5+eHr68v69ev57LPP0NHRoWXLlly+fJnB\ngwdTu3Zt6tWrR4UKFZSOW61aNcWltdjYWEXETdOmTdV+KKNChQqKV+eVK1dO8XS2kZGRIrJlzZo1\nrF+/HkmS0NbWVhpjA0XjdtT1JqNpQETEqEPdiBiAJ0+e4Ok5iU8++YSAgO/JypIRF5fKhQvnmDVr\nOq6ug3B1dSM+Pu1NTr1UiPuRVCdqpTpRK9WJWin31iNiBLCxseHChQsA3Lhxg+zsbI4fP46fnx9r\n164lKSkJe3t7xdsutm3bxogRI5DJZCUuaAriZCA/o7AgYufs2bN88sknas3xZWdjrKysmDJlCsHB\nwcydO5dOnTopjbFRZbyXeZPRNMKblZGRzvjxo2jTxoG5cxegq5sfFH3jxnVmzJjCzJlzcXUtel+t\nIAiCUDrEmcA3qG/fvnh7ezNgwADF2baPP/6YkSNHoq+vT/PmzWnTpg1ZWVn8/vvv7Nq1C11dXaXv\nFC7O/PnzmTdvHpIkoampiZ+f32s9Bk9PT+bMmUNWVhaZmZl4e3tTrVq1YmNsXofSiqYRVPPLLweL\nRMDUrl2X4OAf2LHjR5KTk9izZxcnThwD8n8hqFChgiIncPXqlUD+/YMLFiwtxSMRBEEQSj0i5kOR\nlZWFo6NjkQdhypKUlBRGjBiBgYEBn3/+OZ9//jm2trYv3e7s2bMYGRlRu3btEvv9/fffLF26VPGA\nzYMHD/Dy8kImk/Hpp58ye/ZsNDQ0WLlyJb/99htaWlrMmDEDW1tbtfq+jLhcoJp/X1pRFgEzdep0\n1q4NIihoAxoaGkyePJ6vvupLu3YdShj9/SMuRalO1Ep1olaqE7VS7p2JiBHyhYSEKLLzXuTh4UGj\nRo3UGisiIoJNmzYVaR80aFChJ3Nv3rxJ5cqVCQwMVGv8sLAwnJycil0Erly5ktOnTxMTE8OzZ8/Q\n0NDAzc0NPz8/FixYwMSJE2nevDmzZs0iIiKCKlWqcObMGXbu3Mnjx48ZP348YWFhavUV3gxlETDH\njkXQoUNnRUajk1M3Dh8++MEtAgVBEN41YhH4lujq6qp1FrBfv37069dP6fdpaWl4e3uTmppKYmIi\n7du3Z//+/Rw8eBCZTMbcuXNp0aIF5ubmBAUFYWBgQKVKldDV1WXhwoVFxsvOzmbevHnExsby3Xff\nERMTg5OTE/Hx8YSFhSGXy5kwYQI//fQTUVFRZGVlMWzYMKpXr87vv//O1atXqVmzpuKyd4Fx48Yx\nbtw4fvnlF2xsbJg2bZriTODVq1dp1qwZkB/xEhkZSY0aNWjZsiUymYwqVaqQl5dHQkKCWn0rVqyo\ncp0F1SmLgImPj6dZs88V/czMKhMX93puDxAEQRDeHLEIfEc9ePCALl260LFjR54+fYqbmxt169bl\n3LlzNGjQgDNnzuDt7U3fvn1ZvHgxn376KcuWLVNEufybjo4OM2bMYMeOHUyYMAEvLy/Fd+XLl2fV\nqlWkpaUxY8YMxdm2yMhI6tevT6tWrXByciqyAHxRp06diI6OLtQmSZLiQZIX42BefDK6oF2dviUt\nAkVOoOr2+fco9hLCvyNgJk6ciLFxOUVfY2N9dHS0P8iohg/xmF+VqJXqRK1UJ2qlHrEIfEeZmpqy\nefNmDh8+jKGhIbm5uTg7O7N7927i4uJo27YtWlpaxMbG8umnnwLQuHFjDh48qPa+atSoAeTHt/j4\n+ODj40NaWhrdu3f/T8fw4lPOL4uDUadvSUREjHr+XaviImBMTEy5ezdK0ffOnShMTEw/uDqL+5FU\nJ2qlOlEr1YlaKSciYt4zGzdupGHDhixdupTOnTsjSRJffPEF165dIywsjD59+gD5eX23b98G8h/M\neBUFC7DY2FiuXr3K999/z9q1a1myZAm5ubnIZDJe5fmiunXrcvr0aSA/4qVJkybY2dlx8uRJ5HI5\nMTExyOVyKlasqFZf4c1QFgHTsmUbDh/+mefPn5Odnc3Bg/to3frL0p2sIAiC8FLiTOA7ysHBgTlz\n5rBv3z4qVKiApqYmOTk5dOrUiVOnTvHxxx8DMHv2bGbMmEG5cuXQ1tbG3Nz8lfdpZmZGXFwcPXv2\npFy5cgwdOhQtLS0aNGjA0qVLqVatGtbW1iqP5+npiY+PDwEBAVhZWdGpUyc0NTVp0qQJ/fr1Qy6X\nK+Jy1Okr/HeSJOHp6UmVKh/j6upGXl4eEyd+zePHjwgO3kR4+M7/f0BExooVQbRp48CIEYPJzc2h\nZcs2dO7cpbQPQRAEQXgJERHznvvxxx9xdHSkYsWKLFu2DG1tbcaNG1fa0ypV4nJBye7fv0dAwCKu\nXbvKkCEjcXV14+DBffz88wGWLfseSZIYPXoorq6DaNu2fWlPt8wQl6JUJ2qlOlEr1YlaKSciYsoI\nZXl3Bd+NHTu22GgYddnb2xMZGcm2bdvYunUrJiYmGBkZsXDhQsaNG0dycnKh/oaGhqxateo/7XPO\nnDncuXOnSPu6devQ09MrdpuEhAT69+/Pvn370NXVJTMzk6lTp/Ls2TMMDAxYtGgRFStW5OjRo3z/\n/fdoaWnRu3dvnJ2dlfYV/pvw8FC6du3Jxx9bKtrk8jyeP39OTk4OcrmcnJwcxasLBUEQhHeTWAS+\nZcXl3XXo0IE9e/awZcsWEhMTX+v+KlasyJw5cwpdpl25cuVr3UeBOXPmqNX/999/x9/fn/j4eEXb\n9u3bqVWrFuPHj+fAgQMEBQXh6enJggUL2LVrF/r6+ri4uODg4MD+/fuL9J05c+ZrPqoPj4eHJwCX\nL59XtDk6duPo0Qh69nQkLy+PZs2a07Jl69KaoiAIgvAaiEXgG5Sens7kyZNJSUmhZs2aXLx4kaSk\npCJ5dx06dMDY2JitW7cWCm9WxsHBASsrK6ysrBg8eDDe3t6KBzRmzpz50jd3vKhbt240adKEmzdv\nUqNGDSpVqsS5c+fQ0dFh7dq1ilfFFSxOZ86ciY2NDVu3buXw4cPk5uZiZGREYGAg+/fv5/jx42Rm\nZhIVFcWIESPo1auX0n1raGjwww8/0Lt3b0Xb+fPnGT58uKI+QUFB3Llzh+rVq2NsbAzkP+V87ty5\nYvu+9HhFREwhG73aqtTvhx/WYWJSgX37DpOVlcX06ZPZvn0rLi4D3/AMBUEQhDdFLALfoG3btmFj\nY8OkSZO4cOECJ0+eLDbvDvIXdqp6/Pgx4eHhmJiYMGHCBNzc3Gjfvj3Xrl1jxowZhIeHqzxWeno6\nXbt2pXHjxnTu3Jnp06czadIkBg4cyO3bt9m/fz+ff/45rq6u3L9/n+nTp/Pjjz+SlJTEpk2b0NDQ\nYNiwYVy+fBnID7HesGED9+/fZ/To0SUuAu3t7Yu0paWlKWJeXswDfDH6xcDAgLS0tGL7vsw+/x4q\n10YAQ0NdzMyMiIw8zsyZM6lSJf9yu7NzH3755RfMzMaU8gzLFpFRpjpRK9WJWqlO1Eo9YhH4BkVH\nR9OqVSsA7Ozs0NHRITc3V/F9Qd6dukxMTDAxMQHgzp07NG3aFIA6derw5MkTtcerV68ekB8KXXDZ\nuHz58mRlZXHz5k3+/PNPDh06BOS/X1hDQwNtbW08PDwoV64cT548URxXwVlICwsLsrOz1Z7Li9l/\nL8sDLK6vKsSNw6pLS8siLi4VK6tP2b17L9bW9cjNzeXQocPUqlVb1PIF4qZ01YlaqU7USnWiVsqJ\nnMBSYGNjw4ULFwC4ceMG2dnZxebdqevF4GRra2vOnTsHwLVr1xTvdVVHwZnJ4lhZWeHu7k5wcDDL\nly+nW7duXL9+nSNHjrB8+XJ8fHyQy+WKnMCSxlKFnZ0dx48fB/Lr07hxY6ytrXnw4AFJSUlkZ2dz\n7tw5GjVqVGxfQTlJkpg/fzbbtgUr2rp0aYe7u6viz+HDh4rddsIED1JTU3F17Y27uyuVK1dmwIDB\nb2vqgiAIwhsgzgS+QX379sXb25sBAwYoXqlWXN7dfzFt2jR8fHzYuHEjubm5+Pr6vo6pK4wePRpv\nb29CQ0NJS0tj3LhxfPzxx+jr69OrVy90dHQwMzMjNvb1vCvWxcUFT09PXFxc0NbWxt/fH21tbby8\nvBg2bBiSJNG7d2/Mzc2L7SsUryD25Z9/rmBlVROAqKj7GBkZs2nTtmK3WbhwoeK3amPjCsyd6/fW\n5isIgiC8eSIn8C3JysrC0dGRo0ePlvZUPngf4uWCgIBF1K/fgLNn/6RGDWtcXd04cGAv27Ztwdi4\nAunpaXz5ZTsGDRqKpqYmIC6tqEvUS3WiVqoTtVKdqJVyIifwHRMREcGmTZuKtA8aNEilJ4hfdOnS\nJZYsWVKk3dHREVdX11ed4kvduHGDPXv2cOXKlSLfeXh40KhRo1ca99q1a0RERHzwodfqKIh9OXv2\nT0VbXl4eTZo0Y/To8eTm5jJt2jcYGBjg7Pzm/k0IgiAIZYc4Eyi8MYGBgZiamuLi4lLaU1H4ECJi\nSop98fWdozgT+G+//RbBrl0hrFy5FhC/VatL1Et1olaqE7VSnaiVcuJMoFBEeHg4x44dIzMzk7i4\nOAYNGkRERAS3bt1i2rRpPHnypEgW4M6dO7lw4QL+/v54enpia2vLgAEDioz99OlTdu/ejba2NvXq\n1SM1NZXly5ejq6tLhQoV8PPzU/o0r5eXF1paWsTExJCdnY2TkxPHjh3j8ePHBAUF8fjxY3bs2MGy\nZcvo2LEjdnZ23Lt3j0qVKhEYGKi4nPmhKikiQU9PWxH7smfPHmrXrq14otvISA99fd1C24u4BfWI\neqlO1Ep1olaqE7VSj1gEfuDS09PZuHEjBw4cYNOmTYSGhnL69Gk2bdpE/fr1i2QBDhgwgMjISLy8\nvMjJySl2AQhgbm7OV199hampKZ999hnt2rVj+/btmJubs3nzZlatWoWnp6fSeVWtWpX58+cza9Ys\noqOjWbduHd999x1Hjx6lTp06in4PHz5k8+bNWFhY0L9/fy5fvkzDhg2VjrvPv8d7/5tiSceXmZmj\niH35+++r7N9/kPnzF5Obm8MPP2ymY0dHxfbit2r1iHqpTtRKdaJWqhO1Uk6cCRSKVbCgMjIywtra\nGplMhrGxMTk5OUqzAEeOHEm/fv1UDqVOTEzE0NAQc3NzAJo2bUpAQECJ29StWxfIzyu0srJS/P3f\n2YMmJiZYWFgA+dmEWbizanUAACAASURBVFlZKh65MHToSAICFjF4cH9yc3NxcGhPt249S3tagiAI\nwlsicgI/cMpy/XJycorNAszOzsbPz49vv/2WOXPmlBgILZPJkMvlmJiYkJaWpoiROXPmDJ988skr\nzetV+wn5OYEv3gKsp6dHZOQJtLS00dPT548/Ivn1159LcYaCIAjC2yTOBArF0tLSKjYLcOnSpXz5\n5Zf069eP2NhY/P39mT59erFj1K9fn8WLF2Ntbc38+fMZP3684kzjggUL1J5TYmIiO3fuZNmyZQAk\nJCSQnJyMq2t+eLFcLv9Px/w+e5WcQEEQBOH9Jp4OFt4Je/bsYcuWLTx9+pTIyEgA5s+fT926denV\nqxdr165FR0cHd3f3l471Id4zInIC3zxRL9WJWqlO1Ep1olbKiXsChTciJiam2Ac8mjZtyoQJExSf\n09LS8Pb2JjU1lcTERBwcHNiwYQOfffYZMpmM+/fvK95dnJiYCKB4itjIyAhjY2O2bt1aKCPx/Pnz\njBo1CoDWrVsTEBCg0iLwQyRyAgVBEIR/E4tA4T+pUqUKwcHBL+334MEDunTpQseOHXn69Clubm44\nODgwYMAAGjRowFdffcVPP/2Eq6srfn5+1KxZk507d7J+/XomTZqEg4NDkTHT0tIwMsr/7cbAwIDU\n1Jf/Bvgh5ATu8++h9LsXI2KGDRtU6LsRI4YTHBzM2LGjFG0ibkE9ol6qE7VSnaiV6kSt1CMWgcJb\nYWpqyubNm/+PvXuP6/n+/z9+e+tE0kEnUm2V8yGT45bDpExizCzKsjlsY8NWUlmyTEmo+Yw1c4gU\nX7LYZw7bx8SHYR9Kxpxbo0lJhBS939X7/fujX+9peuf1Jis8r3/x6vV+vR7vh+3i6fV6Pe4vdu/e\njZGREeXl5Xh7e7Nt2zYKCgpwc3NDV1eXrKws5s2bB1QOpzg4OGg8ppGRESUlJTRu3JiSkhKNuYP3\nExExf0XE/PjjTlq3bkvr1m0AuH37LkqlTETEPCLRL+lEr6QTvZJO9EozTYtjMR0s/CPi4+N56aWX\nWLJkCUOGDEGlUvHyyy9z9uxZUlJSGD16NAAODg5ER0eTmJjIrFmzGDBggMZjuri4sH//fgAOHDhA\n9+7d/5Hv8qz4448s1qxZQUVFBXJ5KSkpyQwapN0rCQVBEISnl7gSKPwjBg4cSHh4ONu3b8fU1BQd\nHR3Kysp47bXXOHz4MC+88AIA4eHhBAcHU1FRAUBkZKTGY06dOpXg4GCSk5MxMzMjJibmH/kuTwuV\nSkVkZDiOjq0feE3cp5/OwtTUlGbNjEVOoCAIwnNKTAcLz53n4XbB/ZEwEyd+UG0RuGFDAv/3f4m4\nuXmoB0ZqIm6taEf0SzrRK+lEr6QTvdJM3A5+hmRnZ+Pj44Ovry+fffZZtXy87Oxshg0bVifncXV1\nBcDPz4+srCytPnv+/HnS0tIe2L5kyRJJbxpZt24dS5YsUf9+7969vPnmm4wZM4bk5GQASktLmT59\nOr6+vrz33nsUFhZqVeOzbOvWZIYNG8nAge7VtmdkpHPkyC+MGPFmPVUmCIIgNBRiEfgUioqK4pNP\nPmHjxo2oVCpSU1OByiw9f39/dcRKfdq9eze///671p8rLS0lMDCQjRv/CjAuKysjKiqK+Ph4EhMT\n2bx5MwUFBfzf//0fbdu2ZePGjYwcOZK4uLi6/ApPtYCAYAYPHlJt2/XrBfzrXzHMnRtBo0bif31B\nEITnnXgmsIErKSlh5syZFBUV0bp1a44fP86tW7fo1asXUJmPd+jQITw8PGrM0tNk4MCBODo64ujo\nyDvvvENoaCjl5eXIZDLmzJlD+/btJdf4xRdf8L///Q+lUomXlxeenp5s27YNPT09OnXqRF5eHl9/\n/TXNmzenrKxM/S7gmsjlckaOHMkrr7zCH3/8AUBWVhb29vaYmJgA0L17d9LT0zl27BiTJ09W90HK\nIvBZjIiJD3F76D7l5eWEh4cyY0YAFhYW/0BVgiAIQkMnFoEN3MaNG2nXrh3+/v5kZGRw8OBBVCqV\n+p259+fj1ZSlp0leXh5bt27FzMyMGTNm4Ofnh7u7O2fPnuXTTz+VdMu2ynfffUdSUhLW1tZs3boV\na2tr3njjDSwsLHB2diYgIIAtW7ZgamrK+++/X+uxTExM6Nu3b7Xz358HWPWdi4uLHyknsLYMvWdR\nVS7g1auXuHo1l6+//hcA169fp6KigkaNVLUO34jMLe2IfkkneiWd6JV0olfaEYvABi4nJ4d+/foB\nlZEo+vr6lJeXq38uNR/v78zMzDAzMwMqr7T17NkTgA4dOnD16lWtjhUbG0tsbCzXr19X11rl+vXr\nGBkZqc/VrVs3rWutygOsUlJSQrNmzapt16YPz9ODw1W5gLa2rfn22x3q7WvWfMPt27f45JNgjf0Q\nD1lrR/RLOtEr6USvpBO90kwMhjyl2rVrR0ZGBlA5bKFQKOjYsSNHjhwBKvPxevToofVx738mzMnJ\nifT0dADOnj2r1e1ChULBjz/+SGxsLAkJCWzbto0rV64gk8lQKpWYmppy584d9dDGb7/9pnWtTk5O\nZGdnc+vWLRQKBenp6XTr1k3kBD6ESqXixIlfOX782AM/S039iV9/zaiHqgRBEISGQlwJbODeeust\nQkNDGTduHDY2NgAEBwcTFhZGbGwsjo6OvPbaa491jqCgIMLCwoiPj6e8vLzW24N/p6+vj4mJCSNG\njMDExARXV1dsbGzo3LkzixYtwsnJiaioKCZNmoSJiQm6utr/J6enp0dISAiTJk1CpVLx5ptvYm1t\njY+PD8HBwfj4+KCnpydyAu9TFRFTWHidkSOrTwJv2JBAUdEt3NxEMLQgCMLzTOQEPkXkcjmenp7s\n3bu3vkt5qj0PtwtiY6Pp3LkraWn/w8HBSZ0TmJGRzrp1q+nSpSt37hSJnMA6JPolneiVdKJX0ole\naabpdrC4EvgMS01NZd26dQ9sHz9+fLUJYj8/P8LDw9m1axcWFhb4+Pg88JmTJ0+yePHiB7Z7enri\n6+urVV0KhYJJkyY9sN3BwYHPP/9c4+cqKirw9/dn9OjR9O/fH4Dly5fz3//+F11dXT799FOcnZ21\nquVZVbW4S0v7n3pbVURMTMwy/v3vlPoqTRAEQWggxCLwKWJgYKDVVcBBgwYxaNCgOjm3s7MziYmJ\ndXIsfX19rY/1559/EhwczNWrV9XvGT59+jRHjx5ly5Yt5OXlMX36dFJSal/ciIgYEREjCIIgVBKL\nwHq2detW9u3bR2lpKQUFBYwfP57U1FQyMzMJCgri6tWr7N69m/Lycpo1a8ayZcvYsmULGRkZxMTE\nEBwcjLOzM+PGjavx+H5+fpiZmVFUVMTKlSsJDQ3l8uXLVFRUMGHCBIYOHSq51pCQEHR1dcnNzUWh\nUDB06FD27dtHXl4ecXFx2NvbExMTQ1paGiqVinfffRdPT0+OHj3K8uXLgcow6OjoaPT09Jg5cyYt\nWrTg8uXLdOnShXnz5mk89927d4mIiGDVqlXqbceOHaNv377IZDJsbGyoqKigsLCQ5s2bS/5Oz4La\nIhFERMw/S/RLOtEr6USvpBO90o5YBDYAJSUlxMfHs3PnTtatW0dycjJHjhxh3bp1dO7cmXXr1tGo\nUSMmTZrEb7/9xrhx4zh06BAhISGUlZVpXABWGT58OB4eHiQlJWFmZsbixYspLi5m1KhR9OnTR6ta\nW7VqRUREBHPnziUnJ4dVq1bx5ZdfsnfvXhwcHMjJyWHTpk3I5XK8vb1xdXUlMzOTxYsXY21tzYoV\nK/jxxx8ZPnw4ly5dYs2aNTRp0gR3d3cKCgqwtLSs8bw1hVcXFxdjamqq/n1VVmBti8DtMSOeuWdG\navs+IiLmnyP6JZ3olXSiV9KJXmkmnglswDp06ABAs2bNcHJyQiaTYWJiQllZGXp6egQEBGBoaMjV\nq1fVGYHvv/8+Y8aMkRTq7ODgAFTmAb7yyitAZfaek5MTly9f1qrWjh07AmBsbKx+84exsTEKhYIL\nFy5w+vRp/PwqhxDKy8vJzc3F2tqayMhIDA0Nyc/Px8XFBQB7e3uMjIwAsLS0RC6Xa1WLpvxAQRAE\nQRAeTuQENgBVb//4u7KyMvbs2cPSpUsJCwtDqVSiUqlQKBQsWLCAzz//nPDwcBQKhaTj358HWFxc\nzIULF7C1ta2TWgEcHR3p3bs3iYmJJCQk4Onpia2tLXPmzGHBggUsXLgQKysrqgbSazuWFC4uLhw8\neBClUklubi5KpfK5uxV8P5VKRUTEZ2zcWPm8ZXFxMffu3eWHH7bz9ttvkZS0Tr3vpEkf1DoZLAiC\nIDz7xJXABkxXV5cmTZowatQo9PX1sbS05Nq1ayxZsoRXX32VMWPGcO3aNWJiYpg9e/ZDj+ft7U1Y\nWBg+Pj7I5XKmTZuGubl5ndXr5ubG0aNH8fX15e7du7i7u2NkZMSIESPw9vbG2NgYCwsLrl27Vifn\n69y5Mz169GDMmDEolUrmzp1bJ8d9GlXlAp45cwpHx9YArF79NZaW1kRELOLevXv4+Xnz0ksudO4s\nJqgFQRAEkRPYICUlJfH2229X21YXGYE5OTkEBASQnJyMm5sbP/zwAwYGBo9b7hORnZ3NRx99xI4d\nlc+xFRYWEhgYSGlpKVZWVkRFRdGkSROSk5PZtGkTurq6TJ06VdL7k5/FZ0ZqygVUqVRUVFSgq6tL\nTs5lPvpoMkuXfo2Dg6OkY4rna7Qj+iWd6JV0olfSiV5pJp4JfIp8/fXXDywCa5Obm0tw8IO39nr2\n7MmMGTO0OvejZvjVhaoswuvXr3P16lUUCgV+fn54enryxx9/MGzYMEaNGsXKlSvZvHkzXl5eJCYm\nkpKSglwux9fXF1dXV/T19TWe41mIiKkpEqamXECZTIauri6ffx7Gf/+bSr9+r2Jv/8I/VqcgCILQ\nsIlFYD27ePEis2fPRldXFx0dHfr06cPt27cJDw9n1qxZBAYGUlRUhL29vcZj2NjYEBUVxdSpUzE1\nNaV///64uroyf/58jh49ioGBAfPnz5dUT1WGn4eHB926dSM7O5s+ffpw584dRo8ejYODA4sXLyYv\nL4+wsDDkcrn6+C1btiQmJoZTp05RUlKifmXcsmXLyMnJ4caNG+Tm5jJ79mz69ev3wLmrsgj37dtH\n79698fDwUOcJvvHGG3zwwQcA9O/fn9jYWOzs7OjWrRv6+vro6+tjb2/PuXPnnvnAaCmRMPfvs2zZ\nUkpKSpgxYwbJyeu1+oeBiFvQjuiXdKJX0oleSSd6pR2xCKxnhw8fplOnToSEhJCeno65uTlJSUmE\nh4eTlJRE27Zt8ff358SJExw5cqTWYxUUFJCSkoK+vj6jRo0iMjKSDh06sGfPHhYuXEhQUJDkuq5c\nuUJCQgKWlpb06tWLLVu2EBYWxqBBgygqKiI6Oho/Pz8GDBjAL7/8wpIlS5g3bx7GxsasXbsWpVKJ\nl5cX+fn5QOXicvXq1Rw6dIj4+PgaF4FVarqlW1xcrJ78rYqCuX9b1fbi4uJav9ezEBEjJRKmoOAO\nR478gpNTaywsKmN3+vcfxH//u1fy9xe3VrQj+iWd6JV0olfSiV5ppmlxLKaD69no0aMxMzNj8uTJ\nbNiwAR0dHfXPMjMz6dKlCwBdu3ZFV7f2Nbutra36Vui1a9fU0TM9e/YkMzNTq7pMTU2xsbFBT08P\nQ0NDWrdujUwmo1mzZsjlci5cuMA333yDn58fX331FYWFhRgYGFBYWEhAQABz587l7t27lJWVAX/F\n4LRo0eKh08w1uT8OpqSkBGNjYxER8xB79/5EfPxK9UT53r0/0b17j/ouSxAEQWggxJXAepaamkr3\n7t2ZNm0aO3bsYPXq1eoIFUdHR3799Vfc3d05c+aMOiNQk0aN/lrTW1lZce7cOdq3b09aWhovvvii\nVnU9LL7F0dGRiRMn4uLiQlZWFmlpaRw4cIC8vDyWLl1KYWEhP/30U53Gwezfv59Ro0Zx4MABunfv\njrOzM0uXLkUul6NQKMjKyqJt27aPdZ6niUqlIjIyHEfH1vj6+iGXl3Ly5K8cOXKYXbu206ZNW8rK\nyhg/fgwA/fsP5K23HnwvtCAIgvB8em4WgfdPxjYknTt3ZtasWUyePJlu3boxe/ZscnJyCAwMZOHC\nhcyePRsfHx8cHR3R09OTfNyIiAjmz5+PSqVCR0eHBQsWEBAQQGlpKcuWLXvobdOHCQ4OJjw8HLlc\nTmlpKaGhodja2hIXF4e3tzf6+vrY2dk9UhxMRUUF/v7+1a4YGhgYEBkZSUREBB06dGDVqlUUFBRQ\nWlpKnz59MDAwIDw8vMFOO9e1miJhEhLi6dzZmdDQcFQqFZ9/HsaLLzoQERFdz9UKgiAIDdFzswhs\nqOzt7dm8eTOurq4kJSUBqIchABYvXizpOLa2ttUWuB07dmTDhg3V9jEwMCAqKopdu3bh7+9f64Lp\n0KFDNf763//+a7p2zZo1D3wuJSXlgW3du3dX/9rJyana9/u7P//8k+DgYK5evUpMTAwAp0+f5tSp\nU6Snp5OXl8f06dMxNDQkKiqKiIgIevfuzdy5c6vdSn/Wbd2azLBhI7G2bqHe9tJLLrRo0VJ9Rbht\n23ZcvPhHfZUoCIIgNHANdhFYXFxMaGgod+7c4ebNm7i7u7Njxw527dqFTCZj3rx5vPLKK3h4eNT4\n+bi4OPbs2UNFRQU+Pj707duXwsJCPvzwQwoKCmjXrh0RERFcuHCBhQsXolQqKSoqYs6cObi4uDB4\n8GBcXFy4ePEi5ubmLFu2jLKyMoKCgrh27RotW7YkLS2NgwcPcv78eSIiIoDKZ+kWLFig8dk0TedT\nKBT4+/uTl5dHu3btCA8PJyMjg+joaHR1dTE2NmbJkiXs3LlTnZ13vzt37mBvb09RURErV64kNDSU\ny5cvU1FRwYQJExg6dOgDn0lNTWXdunUPbJfJZNjb25Obm4tCoWDo0KHs27ePvLw84uLisLe3JyYm\nhrS0NFQqFe+++y6enp4cPXqU5cuXA1BaWkp0dDR6enrMnDmTFi1acPnyZbp06cK8efNYvnx5jYMu\nkydPJiIiglWrVqm3HTt2jL59+yKTybCxsaGiooLCwkJOnz5Nr169gMqJ4UOHDmn87+FZU1MkTK9e\nf70H+urVPJKT/4+goNB/vDZBEATh6dBgF4HZ2dl4eXkxePBg8vPz8fPzo2PHjqSnp9O1a1eOHj1K\naGjNf8GdOXOGAwcOsGXLFhQKBTExMbi6ulJcXExUVBTNmjXDw8ODGzdu8PvvvxMcHEy7du3Yvn07\nW7duxcXFhcuXL5OQkEDLli0ZO3Ysv/32GydOnMDW1pYvv/ySrKwshg0bBkBYWBgLFiygdevWbNmy\nhdWrV+Pv719jbZrOV1paSmBgIK1ateLjjz9m7969pKen4+HhwaRJk9i7dy9FRUWMGTOGMWPGPHBc\nPz8/hg8fjoeHB0lJSZiZmbF48WKKi4sZNWoUffr0eeAzgwYNYtCgQQ9sDwkJoVWrVkRERDB37lxy\ncnJYtWoVX375JXv37sXBwYGcnBw2bdqEXC7H29sbV1dXMjMzWbx4MdbW1qxYsYIff/yR4cOHc+nS\nJdasWUOTJk1wd3enoKCAadOmMW3aNEn/LRQXF2Nqaqr+fdV0sEqlUj9rWLXtYZ7GnMDtMSM0/qym\nSJhTp04xffo0xo/3Y+TIBxf/2hBxC9oR/ZJO9Eo60SvpRK+002AXgRYWFiQkJLB7926MjIwoLy/H\n29ubbdu2UVBQgJubm8Zp2YsXL+Ls7IyOjg5NmjRhzpw55OTkYGdnh4mJCQDm5ubcu3cPKysr4uLi\naNy4MSUlJRgZGQFgZmZGy5YtAWjZsiVyuZysrCz69+8PVN7WrHpPbVZWFvPmzQMq3/fr4OCg8Xtp\nOp+NjQ2tWrUCoFu3bly8eJEpU6awYsUK3nnnHaytrR+af1d13qysLF555RWgcqrWycmJy5cvP6Tj\n1XXs2BEAY2NjHB0d1b9WKBRcuHCB06dP4+fnB0B5eTm5ublYW1sTGRmJoaEh+fn5uLi4AJW3vKu+\np6WlJXK5XKtaNE0B3z8IUzUx/DBPY0SM1EgYgD17/kNMTDT+/kEMHjzksb6riFvQjuiXdKJX0ole\nSSd6pdlTFxETHx/PSy+9xJIlSxgyZAgqlYqXX36Zs2fPkpKSwujRozV+1tHRkTNnzqBUKikrK2PC\nhAkoFIoaJ1QjIyOZMWMG0dHRtG3bttZp1rZt23L8+HGg8tm1mzdvApWLr+joaBITE5k1axYDBgzQ\nWJum8129elU9RJGRkUGbNm3Yvn07b7zxBomJibRp0+ahQy1VNTs5OZGeng5UXkW7cOECtra2tX5W\n07Fq4ujoSO/evUlMTCQhIQFPT09sbW2ZM2cOCxYsYOHChVhZWdXpZPDBgwdRKpXk5uaiVCpp3rw5\nHTt2VN9SPnDgAD16PN/xJwcPHmDp0iV88cVyBg8eUt/lCIIgCA1cg70SOHDgQMLDw9m+fTumpqbo\n6OhQVlbGa6+9xuHDh3nhBc2vv+rQoQP9+vXDx8cHpVKJj4+PxleJvf7663z44YeYm5vTokUL9cKu\nJqNHjyYkJIRx48ZhY2OjHqwIDw8nODiYiooKoHKhp4mm85mamhIREUF+fj7dunVjwIABnDhxgpCQ\nEAwNDdHT05P82jZvb2/CwsLw8fFBLpczbdo0zM3NJX1WCjc3N44ePYqvry93797F3d0dIyMjRowY\ngbe3N8bGxlhYWDzSZHBNOnfuTI8ePRgzZgxKpZK5c+cClRPKYWFhxMbG4ujoyGuvvVYn52vI/h4L\nUyU//yqhobNo2tSIhQsj1Nu7dOnKzJkPvlJQEARBEGSqqss1wkNlZGRw9+5d+vbty6VLl5g8eTJ7\n9uyp77IELT2ttwvuj4WZOPED9SLwhx92EB+/kry8XHbs2FPt+cnHIW6taEf0SzrRK+lEr6QTvdJM\n0+3gBnslUIrNmzfXOCkbEBBAt27d6vx8dnZ2BAQEsHz5csrLy9VXpP5OoVAwadKkB7Y7ODhIvppX\nk9zcXIKDH7yq07NnT2bMmEF2djYhISHIZDLatGnDZ599pn5uLjs7m48++kjdr8ep0dXVlUOHDuHn\n50d4eDhOTk6Sv8P58+c5fvw4O3furLb98uXL9OrVi0WLFtX6+XXr1nH9+nUCAwMB2Lt3L1999RW6\nurq8+eabeHt7S67laVNTLMz16wX8/PN+YmKW4ev7Zj1WJwiCIDxtnupFoKZJ2SfF0tKy1oy7Kvr6\n+pL205aNjU2tx42KiuKTTz5R5+alpqbi4eHBd999x/r166vd6n5SNT7M7t27sbCweODcS5YsUQ+g\n1KS0tJQ5c+Zw8uRJBg8eDFQO4URFRfHtt9/SpEkTfHx8GDhwIJaWlk/0O9SXmmJhLCwsWbBAWpak\nIAiCINzvqV4EPs9KSkqYOXMmRUVFtG7dmuPHj3Pr1q0ac/NMTExISkqSlKE3cOBAHB0dcXR05J13\n3iE0NJTy8nJkMhlz5syhffv2kmv84osv+N///odSqcTLywtPT0+2bduGnp4enTp1Ii8vj6+//prm\nzZtTVlZW6yJQLpczcuRIXnnlFf74ozIAOSsrC3t7e/XEd/fu3UlPT8fT01PjcZ6WiJj4ELf6LkEQ\nBEF4xolF4FNq48aNtGvXDn9/fzIyMjh48KDG3LyBAwdKPm5eXh5bt27FzMyMGTNm4Ofnh7u7O2fP\nnuXTTz9l69atko/13XffkZSUhLW1NVu3bsXa2po33ngDCwsLnJ2dCQgIYMuWLZiamvL+++/XeiwT\nExP69u1b7fzFxcXVQrmbNm362K/Dayhqy7qqKRuwirl5U5o3r7ucLJG5pR3RL+lEr6QTvZJO9Eo7\nYhH4lMrJyaFfv35AZYSKvr4+5eXl6p9Lzc37OzMzM8zMzIDKK209e/YEKieur169qtWxYmNjiY2N\n5fr16+paq1y/fh0jIyP1uR7lGU5N+YG1eVpyArXJBrzfjRslVFRIf8d0bcRD1toR/ZJO9Eo60Svp\nRK80e+pyAoXatWvXjoyMDKBy2EKhUNRJbt79Acz35w2ePXsWCwsLycdRKBT8+OOPxMbGkpCQwLZt\n27hy5QoymQylUompqSl37tyhsLAQgN9++03rWp2cnMjOzubWrVsoFArS09OfyECQIAiCIDyLxJXA\np9Rbb71FaGioOrMQ6j43LygoiLCwMOLj4ykvL681//Dv9PX1MTExYcSIEZiYmODq6oqNjQ2dO3dm\n0aJFODk5ERUVxaRJkzAxMdH49pfa6OnpERISwqRJk1CpVLz55ptYW1trfZyG7O+5gBUVFRgZNWPH\nju/4979T8PF5m5EjK4PTDx5Mr+dqBUEQhKeJyAl8Bsjlcjw9Pdm7d299l/JUeFpuF9SUC7h16xYO\nH/6ZhQtjuXv3LlOmTGDOnHl07Ni5zs8vbq1oR/RLOtEr6USvpBO90uyZzAkUKn3yySda7Z+amsq6\ndese2D5+/PhqE8TLli3DwsICR0dHNm3axBdffPHAZ06ePMnixQ9GlHh6eqJUKnn77berba9twXp/\ndmFZWRlnzpyhS5cuODk58emnnzJr1ixu3LhB06ZNiY6Opnnz5s90TmBNuYAHDuzj9ddHoauri7Gx\nMYMGDWb37h+eyCJQEARBeLZJWgSePHmSY8eOMW7cOKZMmcKZM2dYtGgR/fv3f9L1CRJ8/fXXWu0/\naNAgBg0aVCfndnZ21pg36Orq+sAisDZV2YU///wzMTEx6OrqEh8fj4GBAWvXrqVt27ZMnz6dnTt3\nEhcXR3BwsNY5gQ01IqamSJiacgGvXcvHyuqvW95WVtZkZf3+5AsUBEEQnjmSFoERERHMmDGD//zn\nPzRu3Jht27Yxbdo0sQj8h2zdupV9+/ZRWlpKQUEB48ePJzU1lczMTIKCgvjss8/Ub/Bo3749mZmZ\nFBcX869//YtW0GagQQAAIABJREFUrVrVeMxly5Zx/Phx7t69S2RkJPv372fnzp3o6urSo0cPZs2a\nJbm+ixcvMnv2bHR1ddHR0WHRokVs3bqV27dvEx4ezqxZswgMDKSoqAh7e/uHHq9Ro0asXbuWN9/8\n6w0Yx44dY/LkyUBlBmJcXNwj5QQ2VFIjYRo1ktG8eVP1/kZGBjRpov/EYhFE3IJ2RL+kE72STvRK\nOtEr7UhaBCqVSvr27cvMmTMZPHgwLVu2pKKi4knXJtynpKSE+Ph4du7cybp160hOTubIkSOsX7++\n2n7Ozs6EhobyxRdfsHPnzlrz9xwdHZkzZw7nz5/nhx9+YNOmTejq6jJ9+nT27dsnubbDhw/TqVMn\nQkJCSE9P5/bt20ydOpWkpCTCw8NJSkqibdu2+Pv7c+LECfUEsyaurq4PbLs/E7AqA/FRcgIbakSM\n1EgYc3MrMjOzsbGpDNa+ePEyJibNn8h3Es/XaEf0SzrRK+lEr6QTvdLssSJimjRpQnx8PP/73/8Y\nOHAg69evp2nTpnVaoFC7Dh06ANCsWTOcnJyQyWSYmJggl8ur7dexY0cAWrRo8cDP/s7BwQGAP/74\ng65du6Knp4dMJqNHjx5kZmZKrm306NGYmZkxefJkNmzYgI6OTrWfZ2Zm0qVLFwC6du36SJPA92cC\nVmUgPkpO4NOuX7/+7Nz5PeXl5dy5c4fU1N306/dqfZclCIIgPIUkLQKXLFnC3bt3Wb58OSYmJuTn\n5xMTE/OkaxPuU/UmkLpUlQno6OjIyZMnKS8vR6VSkZaWpl4gSpGamkr37t1JSEhgyJAhrF69GqiM\nN6k6/q+//grAmTNnqoVaS+Xi4sL+/fuBygzE7t27Pxc5gSqVihMnfuX48WMADB/+BgUF+Xh49GPY\nMA9efNGBbt2613OVgiAIwtNI0iUZa2tr+vTpw7lz5+jUqROvvvoqLVq0ePgHhcd2+vRpvvrqK8rK\nyrh37576OcyIiAgOHTrEjRs3HvscTZs2paCgAB8fH86ePcuYMWNwd3fn3Llzkj6vVCqJiorCyMiI\nRo0aMXv2bKByATNhwgRWrVrF7Nmz8fHxwdHRET296m+0WLBgAQ4ODvj4+ACQnJzMpk2buHbtGvv3\n72fw4MEMGTKE0aNHs3btWgwMDEhOTkZPT4++ffsyYMAAZDIZI0aMeKZyAqsiYgoLrzNyZOXzkTt3\nfo+5uQWrVq1XR8ScOXNKTAcLgiAIWpO0CExISGDPnj1cu3aNIUOGMHfuXEaPHq2O8xCenLCwMBYv\nXoyLiwtffPEFt27d4rXXXiMpKYmdO3dy69Yt9cDE/VO6VQsqTaZPn17t95aWliQnJ+Pm5kZQUBAy\nmazaPr1799Z4rIMHDxIYGPjAoFCfPn0YO3Ysurq6NcbIFBYWEhQUxKVLl9T/LRUUFJCYmEhKSgpy\nuRxfX19effVV4uPjCQwMZNSoUaxcuZLU1FS8vLxIS0sjLS1Nva9CoUBfX7/W7/60EBExgiAIwpMk\naRG4bds2kpOT8fb2xszMjG+//Za33npLLALrWElJCTNnzqSoqIjWrVtz/PhxCgsLcXFxASpviaam\npmJnZ0e/fv1o1KgRzZs3R0dHh4KCghqjUSZNmsSxY8fQ1dXFxMQEExMTcnJyaNu2LQYGBsyfP19y\nfeHh4fz000+UlpaiVCpp2bIljRs35urVq5w+fZrWrVuzb98+tmzZgqWlZa1XKTdv3sy3335LeXk5\nFRUVrF27ll27dtG/f3+6deuGvr4++vr62Nvbc+7cOY4dO8YHH3wAVE4Hx8bGYmdnV+O+zs7OWna+\nYRIRMYIgCMKTJGkR2KhRo2pXVwwMDB54+F94fBs3bqRdu3b4+/uTkZHBwYMHsbOz4+jRo/Tq1Yt9\n+/Zx7949OnTowNq1axk3bhxXr17l999/5969ezUec968eYwePZoDBw6gr6/PqFGjSEhIoEOHDuzZ\ns4eFCxcSFBQkqb7AwED279/P9u3bATh06BDDhw8nJCSEoUOH0qxZM9avX8/27duRyWSMGjVK47HG\njBnDmDFjgL9CqX18fPj3v//NrVu31PtVTfzW1XRwQ80J3B4zQuPPRETM00P0SzrRK+lEr6QTvdKO\npEVgr169iI6O5t69e+zZs4fNmzfTp0+fJ13bcycnJ4d+/foBlVf99PX1WbBgAZGRkaxevZouXbqg\nr69P3759+e2333jnnXdo3749nTp1wtTUVONxbW1t1Yv4a9euqSeNe/bsqdWAj5GREWFhYYSFhVFc\nXMzrr79e7ed//PEHrVu3Vp/rUa7IaZr4rdreuHHjx5oOFhEx0om4Be2IfkkneiWd6JV0oleaPVZE\nTFBQEC+88ALt2rXju+++Y8CAAQQHB9dpgQK0a9eOjIwMAM6fP49CoWD//v0sWLCAlStXcuvWLVxd\nXbl48SLm5uZs3LiR9957D5lMhrGxscbjVk0BA1hZWakHPtLS0njxxRcl13ft2jX1oMrKlStZvHgx\n5eXlyGQyVCoVdnZ2/P7775SWllJRUcHZs2e17oGzszPHjh1DLpdz584dsrKyaNu2bY3TwZr2fZaJ\niBhBEAShrki6Evjee++xZs0axo4d+6Trea699dZbhIaGMm7cOGxsbAB44YUXeP/992nSpAm9e/dm\nwIAByOVyfv75Z7799lsMDAyYO3eu5HNEREQwf/58VCoVOjo6LFiwQPJnLS0tKSgoYOTIkRgaGjJx\n4kR0dXXp2rUrS5YsYenSpXz88ceMHTuW5s2b06RJE617YGlpiZ+fH76+vqhUKvz9/TEwMGDq1KkE\nBweTnJyMmZkZMTExGBoa1rjvs6QqIubWrVv4+voxfPgb7Nz5PR4e/VAqVfTt209ExAiCIAiPRKaq\nCnOrha+vLzExMbRs2fKfqEkA5HI5np6e7N27t75LeeY8LbcLqiJizpw5xcSJH+Dr68fWrVs4fPhn\nFi6MVUfEzJkz74lMB4tbK9oR/ZJO9Eo60SvpRK8003Q7WNKVwMLCQtzc3DA3N8fAwACVSoVMJiM1\nNbVOi3yenT59ms8++wx9fX06dOhAYGAgAFOmTOHWrVvo6elhYGCgDmL+u82bN7Njx44HtgcEBKgD\nlHNycggICFBHwfzwww81XjlLTU1l3bp1D2wfP348Hh4eWn2v3NzcGh8d6NmzJzNmzND4uezsbD76\n6CP1dyosLCQwMJDS0lKsrKyIioqiSZMm6kxBXV1dpk6dysCBA7WqryETETGCIAjCkyRpEbhmzZon\nXcdzLywsjDlz5qjzAHfv3s3evXsZOnQoO3fufOgbQ+6ftn1cgwYNYtCgQXVyLBsbm2r5hVJ89913\nrF+/nps3b6q3xcXFMWzYMHVO4ObNm/Hy8nogU9DV1fWZyQkUETGCIAjCkyRpEZiWllbj9latWtVp\nMc8LqXmArq6uFBUVMWXKFIqKinj//fc1XunKyclh6tSpmJqa0r9/f1xdXZk/fz46Ojpa5wECeHh4\n0K1bN7Kzs+nTpw937tzh5MmTODg4sHjxYvLy8ggLC0Mul6uP37JlS2JiYjh16hQlJSU4OTkRFRXF\nsmXLyMnJ4caNG+Tm5jJ79mz1FHRNTExMSEpKqnbVsa5yAhtqREx8iJuk/ZRKVbV/EKhUqmqDP4Ig\nCIIglaRF4JEjR9S/Lisr49ixY/To0YORI0c+scKeZVLzAMvKypg4cSLjx4/n9u3b+Pj44OzsjLm5\neY3HLSgoICUlRZ0HGBkZ+Uh5gABXrlwhISEBS0tLevXqxZYtWwgLC2PQoEEUFRURHR2Nn58fAwYM\n4JdffmHJkiXMmzcPY2Nj1q5di1KpxMvLi/z8fAD09fVZvXo1hw4dIj4+vtZFYE0L3brKCawtj6+h\nuj8n0M6uFWVlxernO+7dK+KFF2xFTmADIfolneiVdKJX0oleaUfSIjAqKqra72/duoW/v/8TKeh5\nIDUP0MLCQv3aNXNzczp06KCOh6lJXeUBApiamqonlA0NDWndujUAzZo1Qy6Xc+HCBb755htWr16N\nSqVSP7NYWFhIQEAAhoaG3L17l7KyMgB1LS1atEChUGjZMeosJxCensGQKvfnBPbu7crGjZvp3LkH\n9+7d4/vvtxMYOFvkBDYAol/SiV5JJ3olneiVZo81GPJ3hoaGXLly5bEKep5V5QG6u7s/kAdobW3N\n/Pnz6d+/P4cPH2bDhg2sXLmSkpISMjMzcXR01HjcmvIA27dvr3UeIPDQZxAdHR2ZOHEiLi4uZGVl\nkZaWxoEDB8jLy2Pp0qUUFhby008/UTV8/rDjPUxVTuCoUaOq5QQuXboUuVyOQqF46nMCv/12Eykp\nyRgYNOaFF15k5sxgjI1Nqu0zcuRorly5wrvv+lJeXsbrr48SETGCIAjCI5G0CPTz81P/Ja5SqcjJ\nyaF///5PtLBnmdQ8QICDBw/i7e1No0aNCAgIoHnz5pLO8Th5gFIEBwcTHh6OXC6ntLSU0NBQbG1t\niYuLw9vbG319fezs7Lh27VqdnO9ZzwnMyEhnw4b1fPPNWqysrPnxx50sWhRJRMQiQkPD1fvp6ury\n8ccz669QQRAE4ZkhKSfw6NGjf31AJsPMzEx9e/Bpd39sSn14WB6gq6srhw4deqI1+Pn5ER4ezq5d\nu9Tv8G2IKioq8Pf3Z/To0ep/hCxfvpz//ve/6Orq8umnn0p6VV1DvF2wcWMiFy9mqRd89+7dY+hQ\nN3bvPoCenl691CRurWhH9Es60SvpRK+kE73S7LFuB//nP/8hLCys2rbg4GCio6MfvzJBK1LyAKWq\nygM8e/YswcHB3LhxAz09PSwsLLTOA9TW8uXLqw0cVVmwYAF2dnYPbP/zzz8JDg7m6tWrjB49GqjM\nVjx69ChbtmwhLy+P6dOnk5KS8kTrflI6derMt99u4urVPFq0aMmuXd9TVlbG7du3sbCwqO/yBEEQ\nhGdQrYvA0NBQLl++zKlTp8jMzFRvr3pvaX0oLi4mNDSUO3fucPPmTdzd3dmxYwe7du1CJpMxb948\nXnnlFY2LmLi4OPbs2UNFRQU+Pj707duXwsJCPvzwQwoKCmjXrh0RERFcuHCBhQsXolQqKSoqUmf4\nDR48GBcXF/WAxrJlyygrKyMoKIhr167RsmVL0tLSOHjwIOfPnyciIgKoHLRYsGDBA4MLBgYG7N27\nV+P5FAoF/v7+5OXl0a5dO8LDw2ndujXR0dHqwOAlS5ZgZGRU4/f18/PDzMyMoqIiVq5cqf4zraio\nYMKECSQmJj5wJVBT70JCQtDV1SU3NxeFQsHQoUPZt28feXl5xMXFYW9vT0xMDGlpaahUKt599108\nPT05evQoy5cvB6C0tJTo6GhGjhzJzz//TIsWLbh8+TJdunRh3rx5Gv/c7969S0REBKtWrVJvO3bs\nGH379kUmk2FjY0NFRQWFhYW13jJvKBExf4+E6dq1GxMnvsennwYikzXCy+t1jI1N0NN7pMd2BUEQ\nBOGhav0bZurUqVy5coXIyEimTZum3q6jo4OTk9MTL64m2dnZeHl5MXjwYPLz8/Hz86Njx46kp6fT\ntWtXjh49SmhoaI2fPXPmDAcOHGDLli0oFApiYmJwdXWluLiYqKgomjVrhoeHBzdu3OD3338nODiY\ndu3asX37drZu3YqLiwuXL18mISGBli1bMnbsWH777TdOnDiBra0tX375JVlZWQwbNgyoDIBesGAB\nrVu3ZsuWLaxevVrjVLWm85WWlhIYGEirVq34+OOP2bt3L+np6Xh4eDBp0iT27t1LUVGRxkUgwPDh\nw/Hw8CApKQkzMzMWL15McXExo0aNok+fPlr1v1WrVkRERDB37lxycnJYtWoVX375JXv37sXBwYGc\nnBw2bdqEXC7H29sbV1dXMjMzWbx4MdbW1qxYsYIff/yR4cOHc+nSJdasWUOTJk1wd3enoKAAS0vL\nGs/bvn37B7YVFxdjamqq/n1VdIzU5ybr098vzRcXF+Pm1p8JE/wAyM/PJz7+G5ycbB97qOZxiLgF\n7Yh+SSd6JZ3olXSiV9qpdRFoa2uLra0t33//Pbdu3eLevXuoVCoqKio4e/YsL7/88j9Vp5qFhQUJ\nCQns3r0bIyMjysvL8fb2Ztu2bRQUFODm5oaubs1f6+LFizg7O6Ojo0OTJk2YM2cOOTk52NnZYWJS\nOYVpbm7OvXv3sLKyIi4uTh1JUrXIMjMzU79DuWXLlsjlcrKystTPqDk5OakXIVlZWeqrW2VlZTg4\nOGj8XprOZ2Njow7l7tatGxcvXmTKlCmsWLGCd955B2tr64c+B1d13qysLF555RWgMnLFycmJy5cv\nP6Tj1XXs2BEAY2Nj9aSysbExCoWCCxcucPr0afz8Khcy5eXl5ObmYm1tTWRkJIaGhuTn56tDse3t\n7dXf09LSErlcrlUtjxIRsz1mRIN4ZuTvNfz55yU+/vhDkpKSadrUiNjYL3Fz8+D69dpzD58k8XyN\ndkS/pBO9kk70SjrRK800LY4lvWpg2bJlDBo0iCFDhuDj48PgwYOJjY2t0wKlio+P56WXXmLJkiUM\nGTIElUrFyy+/zNmzZ0lJSVE/L1YTR0dHzpw5g1KppKysjAkTJqBQKGq80hIZGcmMGTOIjo6mbdu2\ntUadtG3bluPHjwOVz65Vve7MwcGB6OhoEhMTmTVrlnrityaaznf16lX1hG1GRgZt2rRh+/btvPHG\nGyQmJtKmTZuHDrVU1ezk5ER6ejpQeeXpwoUL2Nra1vpZTceqiaOjI7179yYxMZGEhAQ8PT2xtbVl\nzpw5LFiwgIULF2JlZVWnsTEHDx5EqVSSm5uLUql8Kq4C1sTe/kXefvsd3n//XXx8RlFWpuCjjz6u\n77IEQRCEZ5ikB462bdvG/v37iYyMZOrUqfzxxx9s3LjxSddWo4EDBxIeHs727dsxNTVFR0eHsrIy\nXnvtNQ4fPswLL7yg8bMdOnSgX79++Pj4oFQq8fHx0fie2ddff50PP/wQc3NzWrRoUe09tn83evRo\nQkJC1JEvVTEl4eHhBAcHU1FRAVQu9DTRdD5TU1MiIiLIz8+nW7duDBgwgBMnThASEoKhoSF6enp8\n/vnnD+0bgLe3N2FhYfj4+CCXy5k2bZrG4OlH4ebmxtGjR/H19eXu3bu4u7tjZGTEiBEj8Pb2xtjY\nGAsLizqLjencuTM9evRgzJgxKJVK5s6dWyfHrS8qlQqlUomBQWNKS0v//yv5Gtd3WYIgCMIzSlJE\nzNixY9m0aRPx8fHY2toyePBghg8fzvbt2/+JGhu8jIwM7t69S9++fbl06RKTJ09mz5499V2WoEFD\nvF2QkZHO/Plzq+UEHjy4n4iIRfVWk7i1oh3RL+lEr6QTvZJO9Eqzx4qIMTIy4rvvvqNTp04kJSVh\nZWVFaWlpnRZYl+oyRkUKOzs79a1PXV1djVekFAoFkyZNemC7g4PDQ6/mFRQU8NVXXxEeHv7Az3Jz\ncwkODn5ge8+ePZkxY8ZD6w8JCWHo0KFcv36dzMxMTp069Ug1Pq6TJ0+yePHiB7Z7enri6+sLVEbI\nODg4qLMMk5OT2bRpE7q6ukydOrXG9w4/Dc6dO0uPHr2wsrIGYMAAN6KjIygrK6u3nEBBEATh2SZp\nERgZGcnOnTsZOXIk+/btY+7cuXzyySdPurZHNmbMGMaMGfOPnc/S0pIOHToQHh5e69S0vr4+iYmJ\nj3yOmhaAUDk88qjH/TsdHZ06O5a2nJ2dNZ67sLCQoKAgLl26pF5IFxQUkJiYSEpKCnK5HF9fX1xd\nXTXe4m/IRE6gIAiC8E+TtAi0trZm7NixnDt3jqCgIEpLSzE0NHzStTVYf88qfOuttwD48ssvuXnz\nJvr6+ixaVHkb75NPPkGlUlFWVsa8efNo165djcdctmwZ2dnZ3Lx5k9u3b+Pr68vu3bu5ePEi0dHR\nWFhYqN9sMnz4cHr16sX58+eRyWTExcVpnIoNCQnh1q1b3Lp1i2+++Yavv/6aY8eOATBs2DDeeecd\nyd/7YTW+9NJLJCYmsmPHDmQyGUOHDmX8+PFaZS7q6OjUeO6SkhKmT5/OgQMH1NtOnjxJt27d0NfX\nR19fH3t7e86dO1frtHRDyQncHjOi2u/d3ftz8+Z05s4NRiaT8eabb2JqakqLFqaYmdVf5IGIW9CO\n6Jd0olfSiV5JJ3qlHUmLwF9++YW5c+dSUVHB5s2bGT58OEuWLKFv375Pur4GqaasQmtrawYPHoyX\nlxcbNmzgm2++4eWXX6ZZs2bExMTw+++/U1xce9xH48aNWbNmDStXrmT//v2sWLGClJQUdu7cWW2x\nVlJSgpeXF2FhYcycOZMDBw7g5eWl8bh9+vTh3XffZd++feTk5JCcnEx5eTm+vr5a5wTWVqORkRG7\ndu1i48aNyGQy3n33Xfr27atV5uJLL71U43nt7Oyws7OrtggsLi6utvht2rTpQ3vcUCNi7t4toXXr\nTqxcuf7///waSuW/KCvTqbd6xfM12hH9kk70SjrRK+lErzR7rGcCY2Nj2bhxI++99x6WlpYkJSUR\nEBDw3C4Ca8oqBOjRowdQGV2yf/9+goODuXTpEh9++KH6mbXaVGXwNWvWTP1uZhMTkxrz86r2rcoq\nrM39OYE9evRAJpOhp6dH165dycrK0uKb117jhQsXyM3N5d133wXg9u3b/Pnnn1plLmrjUXICG6rr\n1wuq5QQmJMTj7j64XoOiBUEQhGebpJxApVJZ7U0OVX/5P69qyioE+O233wBIT0+nTZs2HDlyBCsr\nK+Lj45k6depDsxW1+Qv/UfZ1cnJS3wouKyvj+PHjtUbqaHteR0dHWrduzfr160lMTGTUqFG0bdtW\nq8xFbTg7O3Ps2DHkcjl37twhKyuLtm3bPtYx64u9/Yt06tQZLy93Bg58mSNHfsHPb0J9lyUIgiA8\nwyRdCWzRogX79u1DJpNRVFTEhg0bsLGxedK1NVg1ZRUqFAr27NlDQkICTZs2JTo6GqVSib+/PwkJ\nCTRq1IiPPvqI7OxsQkJCkMlktGnThs8++4xGjRqxY8cOKioq2LVrF4WFhQwZMuSR68vJyVE/P/if\n//xH/S7ggQMHcvToUcaMGUNZWRlDhgyhU6dOddUW2rdvz8svv4yPjw8KhQJnZ2esra1rzVzMzs7m\no48+ok2bNkDlAEhgYCClpaVYWVkRFRVFkyZN1FPA169fZ9CgQUDlEEtFRQV9+vRRT2VXZTQ+bTIy\n0jl9+hTJyf9WR8T8619L6jUiRhAEQXi21ZoTmJ+fj7W1NTdu3CAyMpLDhw+jUqno3bs3c+bMwcrK\n6p+s9ZkwZcoUJkyYQO/evZk7dy79+vXDw8ODoUOHsnPnzjq5/Xf/ItDNzY0ffvihQS6OvvvuO9av\nX09+fj6HDh0CICIigo4dOzJq1ChWrlyJvr4+Xl5eTJw4sdoUcEpKCosWLXpg36pb0bVpiM+MbNyY\nyMWLWYSGhgNw7949hg51Y/fuA/UWESOer9GO6Jd0olfSiV5JJ3ql2SM9EzhlyhS2bduGubk5nTt3\nrrdXxT2tSkpKmDlzJkVFRbRu3Zrjx4+TnZ1NcXExy5cv5+bNm+zbt49NmzZRVFTElClTKCoq4v33\n39eYd5eTk8PUqVMxNTWlf//+uLq6Mm/ePLKyspDJZOrn/37//XdJb9Dw8PCgW7duZGdn06dPH+7c\nucP27dvR09PDyckJuVzOpUuXkMlkdOnShfnz59OyZUtiYmI4deoUJSUlODk5ERUVxbJly8jJyeHG\njRvk5uYye/Zs+vXrp/HcJiYmJCUlqa9U5ubmkpKSwm+//ca2bdu4e/culy9f5sSJEzVOAR87dowP\nPvgAgP79+xMbGytpEdgQiYgYQRAE4Z9W6yLw/ouE27dvZ+LEiU+8oGfJxo0badeuHf7+/mRkZHDw\n4EGMjY1JSkoCKqeuU1JSmDlzJj/88APjx4/n9u3b+Pj44OzsrPGVbgUFBaSkpKCvr8+oUaOIjIyk\nQ4cO7Nmzh++//56goCACAgL4/PPPcXNzq7XGK1eukJCQgKWlJb169WLLli2EhYUxaNAgvvrqK+bO\nnctHH33EgAED+OWXX1iyZAnz5s3D2NiYtWvXolQq8fLyIj8/H6jMQly9ejWHDh0iPj6+1kXg3xe6\nNjY26qGbxo0bc/nyZYKCgnj11Ve5cOGCer+qKeD7p4ObNm3KnTsP/xdgQ4mIiQ+p/ufStWs3Jk58\nj08/DUQma4SX1+sYG5ugpyfpiQ1BEARB0Fqtf8Pcf2tSwtvlhL/JyclRL4JcXFzQ19dXTxJD5ZXC\nqvfpjh07Fl1dXczNzenQoYM6O68mtra26kDka9eu0aFDB6DyDSExMTFa1Whqaqp+vtPQ0FA99NOs\nWTP1xO8333zD6tWrUalU6OnpYWBgQGFhIQEBARgaGnL37l3KysoA1LW0aNEChUKhVS3w18Rv1SSx\nsbGxxingmvZ9mL/n8zUUxcXFuLn1Z8IEP6DyUYz4+G9wcrKt1wlhkbmlHdEv6USvpBO9kk70SjuS\nLzOIqArttWvXjoyMDNzd3Tl//jwKhYKOHTty5MgRevfuzYEDB+jTpw+HDx9mw4YNrFy5kpKSEjIz\nM3F0dNR43EaN/hrqtrKy4ty5c7Rv3560tDRefPFFrWp82J+ro6MjEydOxMXFhaysLNLS0jhw4AB5\neXksXbqUwsJCfvrppzqb+K2K1xk1ahQHDhyge/fuODs7s3TpUuRyOQqFQj0FXNO+UjTEZ0b+/PNS\ntYiY2NgvcXPz4Pr12nMPnyTxfI12RL+kE72STvRKOtErzR7pmcDMzEz1JGZ+fr761yqVCplMRmpq\nah2X+Wx56623CA0NZdy4ceqrbcHBwYSFhREbG4ujoyOvvfYaOjo6HDx4EG9vbxo1akRAQADNmzeX\ndI6IiAjmz5+PSqVCR0eHBQsW1Ol3CA4OJjw8HLlcTmlpKaGhodja2hIXF4e3tzf6+vrY2dlx7dq1\nOjnf1KksVsbdAAAgAElEQVRTCQ4OJjk5GTMzM2JiYjA0NMTPzw9fX19UKhX+/v4YGBjUuO/TZv/+\nfcTHf4NM1ggDg8ZMmPA2OjqNcHZ+iYCAoPouTxAEQXiG1TodfOXKlVo/3KpVqzov6Fkll8vx9PRk\n79699V3Kc6+h/EtRLi/Fy8uddev+D1tbOzZv3kB6+lEWL/5XfZcGiH9Va0v0SzrRK+lEr6QTvdLs\nka4EikVe/dm8eTM7dux4YHtAQADdunXT6lipqamsW7fuge3jx49XT+Y+KcuXL+fIkSMPbF+wYAF2\ndnZP9NwNXUWFEpVKpX7V3b1799TPegqCIAjCk1brlUDhyUpKSuLtt9+utq0urhg+LTmB8FdYdNWC\n92Fh0VWv3xs4cKDGfR+mIf1L8YcfdrBoUSTGxiYolUq+/noNtrYNY3Es/lWtHdEv6USvpBO9kk70\nSrPHenew8GR8/fXXDywCnydVYdH3v0EkLi6OYcOGqQOgN2/ejJeXF4mJidXCol1dXWvc92E5gfUV\nEfP3SBiArKzfWbduNUlJW2jVypYtWzYRGhrEunUbxSCWIAiC8MSJReA/5OLFi8yePRtdXV10dHTo\n06cPt2/fJjw8nFmzZhEYGEhRURH29va1HqemsOj58+ejo6ODgYEB8+fP16qumsKiT548iYODA4sX\nLyYvL4+wsDDkcrn6+E8qLBqoMQDazs7uqQ+LrulfYd9/n0HPnj146aXKWJ0PPpjIsmWx6OqWSx4M\netJE3IJ2RL+kE72STvRKOtEr7YhF4D/k8OHDdOrUiZCQENLT0zE3NycpKYnw8HCSkpJo27Yt/v7+\nnDhxosZn6O5XW1j0woULCQqSPlVaW1h0UVER0dHR+Pn5/SNh0UCNAdD3b6va/qhh0dtjRtTL7YKa\nztmqlQPr1ydy/vwlmjc357//TaVlSxsqKvQaxC0NcWtFO6Jf0oleSSd6JZ3olWbidnA9Gz16NKtW\nrWLy5Mk0a9YMf39/9c8yMzPVi6WuXbuiq1v7H4sIi370sOiGpHv3nvj4+DF9+gfo6uphbGxMVNTT\nF3MjCIIgPJ3EIvAfkpqaSvfu3Zk2bRo7duxQL6qgMpD5119/xd3dnTNnzlR7q0hNRFj044VF15cf\nftjB5s0b1b8vKSnm2rV8tm3bRfPmNb8dRhAEQRCeFLEI/Id07tyZWbNmsWzZMho1asTs2bPJyckh\nMDCQhQsXMnv2bHx8fHB0dERPT0/yces6LHrZsmVYWFiofy8lLNrGxoZt27bVugBcu3Yt3377rfpZ\nt3nz5mFjY8OsWbO4desW7733HtHR0UydOpX33nuP+fPno6ury8cff4ylpSVjx46lb9++lJeX88IL\nL1BSUlLjvg2Zp+cwPD2HAVBeXs5HH73HuHHviAWgIAiCUC9ERIxQTdUi0MfHR/Jnjhw5wqZNm/ji\niy807hMYGMi7775L586d1dvWrl1LcXEx06dPZ+fOnRw/fpzg4GCGDh3Kt99+S5MmTfDx8WHFihXs\n2LFD8r6Wlpa11tsQnhlZt241586dYeHC2PouRSPxfI12RL+kE72STvRKOtErzcQzgU8ZKWHRxcXF\nhIaGcufOHW7evIm7uzs7duxg165dyGQy5s2bxyuvvMKff/7J8uXL0dHRQU9Pj0aNGuHo6FhrWHRF\nRQVz587l6tWr3Lx5k/79+/PJJ5+we/duVq1aha6uLq1atWLRokWsWLGCc+fOsXnzZsaMGVPtOFVh\n0SdPnuTgwYOUlZVhamrKunXrOHbsGJMnTwYqp3vj4uLIysrC3t4eExMTALp37056erpW+3p6emrs\n6z8ZEVNTLAzArVu32LRpA2vWJP5jtQiCIAjC34lFYAM1ZsyYBxZUf5ednY2XlxeDBw8mPz8fPz8/\nOnbsSHp6Ol27duXo0aOEhv4/9u4+Luf7////7ehUp6JIElM530LM+RjLSMzWptSWOZvT8laiTJFR\nNGLDO8QSMcqKy9vJ9nU6JpuT2BhLhGiFSFJ2HB3p+P3Rr+MjOvI6CGXP618dr+N1PJ/P4+G9y/t5\nPF+v5/01i2HDhpGYmEiLFi1YunQpN2/eZOHChVW2nZOTQ4cOHRg2bBgKhUI9Cdy5cycjR47Ezc2N\n7du3U1hYyIQJE9iyZUul4/X19cXX15cVK1bg7e2Nqakpvr6+XLp06bl3Ams6t6bQ9MsrKWkT/fu7\nqKNhajIRt6AdUS/pRK2kE7WSTtRKO2ISWItZWVmxfv169uzZg6mpKSUlJXh4eLBt2zZyc3Pp168f\nenp63Lp1ixYtWgBlq2W7d+9+atsWFhacPXuW3377DVNTU/VO35kzZ7J69Wo2b96Mvb09Li4uT21L\npVLx+eefqydsffr04fz58xV2/UrdCSzl3Kq8zIgYTf387387mTo1sMZfthCXVrQj6iWdqJV0olbS\niVpppmlyrFPpUaFWiI2NpUOHDixevJiBAweiUqno3r07f/31F0lJSXzyySdAWVzLpUuXAPjjjz8k\ntZ2cnIyZmRlRUVGMHj0auVyOSqUiISEBPz8/Nm7cCMDevXvR0dGhtLRUY1uFhYUMHjyYoqIiVCoV\nx44d480331Tv7gXUu3sdHBzIzMwkPz+f4uJiTp48SceOHbU6tyYrKCjg77+v89Zb7V/1UARBEIR/\nObESWIv17duXsLAwduzYgYWFBbq6uiiVSgYMGMDRo0dp1qwZAHPmzOHLL7/E2NgYfX19rK2tn9p2\n9+7dCQgIIDU1FSMjI5o1a8atW7dwcnJi1KhRWFhYYGJiwrvvvktxcTHp6enExcVV+sSO8lzEESNG\nYGBgQPfu3enTpw9dunQhKCgILy8v9PX1iYqKQl9fn+DgYMaMGYNKpeLjjz/G2toaLy8vyefWVD/+\nuJP162N5+PAhY8eOEBExgiAIwisldgf/C2zatAlXV1fq16/P0qVL0dfXx9fX91UP65WpCZcLyiNi\nXF0H8+GHH7/q4VRKXFrRjqiXdKJW0olaSSdqpZnYHVzLbdy4kc8++6zCMYVCgaurKwcOHKjys5aW\nlowePRpjY2PMzMxYuHAhvr6+3Lt3D4VCwaVLl2jXrh1nzpzh+PHjGBoaPtMY9+/fT1xc3BPHq9qF\nrI3MzEwmT56s3jWdl5dHYGAgcrmchg0bsmDBAoyMjJ67n5dh48Y46tWrV2MngIIgCMLrT6wE1hI9\ne/YkJSWlwjGpk8CqZGVlERAQQGJiIv369ePHH3985kngi7R9+3Y2bNjAzZs31XWYP38+bdu2xd3d\nnZiYGAwMDCq9HP24V/1LMT8/n+HDP+K77+KxtW3ySsdSFfGrWjuiXtKJWkknaiWdqJVmYiWwFrly\n5QozZ85ET08PXV1dunXrxr179wgLC2P69OkEBgZSUFBA06ZNq2wnKyuLiRMnYmFhQe/evenZsyfz\n5s1DV1cXQ0ND5s2bp9W4+vfvT8eOHcnMzKRbt27cv3+fM2fO0Lx5cxYtWkROTg6hoaEoFAp1+zY2\nNkRFRfHnn39SVFSEg4MDCxYsYPny5WRlZXHnzh2ys7OZOXOm+vnJlalbty4bN26ssKKYmprK+PHj\ngbLswCVLljx1EvgycwJ3RA2t9LiIiHl9iXpJJ2olnaiVdKJW2hGTwBro6NGjtGvXjuDgYE6ePIml\npSUbN24kLCyMjRs30rJlS/z9/fnjjz84duxYlW3l5uaSlJSEgYEB7u7uhIeH06ZNG/bt28fChQuZ\nMWOG5HH9/fffrF+/ngYNGtClSxe2bt1KaGgo7733HgUFBURGRuLj40OfPn349ddfWbx4MXPnzsXc\n3Jx169ZRWlqKm5sbN2/eBMDAwIC1a9eSkpJCbGxslZPAvn37PnGssuzApxERMdKJX9XaEfWSTtRK\nOlEr6UStNBMrgbXIJ598wpo1axg7dqx6Z225ixcvqidL7du3R0+v6n/CJk2aYGBgAMCtW7do06Zs\n9entt98mKipKq3FZWFjQuHFjAIyNjXF0dATKdv8qFArS09NZvXo1a9euRaVSoa+vj6GhIXl5eQQE\nBGBsbMyDBw9QKpUA6rE0atRInUOojfKcwDp16qizA2s6EREjCIIg1BRiElgD7d+/n06dOuHr68vO\nnTvVkyoAe3t7fv/9d1xcXDh//jwlJSVVtqWj839RkA0bNiQtLY3WrVtz4sQJ3njjDa3GJZPJqnzf\n3t6e0aNH4+zsTEZGBidOnODw4cPk5OTwzTffkJeXx969e9Xf5WntPU15dqC7u7s6O7Cmyci4xNKl\nX1NUVIiOji6ffDIcS0urp07eBUEQBOFFE/9PVAO9+eabTJ8+neXLl6Ojo8PMmTPJysoiMDCQhQsX\nMnPmTLy8vLC3t0dfX19yu/Pnz2fevHmoVCp0dXWJiIio1nEHBQURFhaGQqFALpcza9YsmjRpQnR0\nNB4eHhgYGGBnZ8etW7eqpb+JEycSFBREYmIi9erV03pl80WTy+UEBEwmODiU7t178csvP7Ny5XIS\nEra/6qEJgiAIgtgdXJNlZmYSHByMTCajRYsWzJkzR72y93hcyvMo33ns4+NDWFgYDg4Oz93mi5CX\nl8fw4cPZsWMHhoaGyOVypk+fzp07dzAxMSEyMpL69es/tZ2Xdc/I4cM/s3lzPCtXfgeUPT4vI+MS\njo4tXkr/z0vcX6MdUS/pRK2kE7WSTtRKM3FPYC20YMECpk6dSteuXZk9ezb79++nf//+6riUu3fv\nApCQkFDpZDAgIEDrx6j99ttvhIWFPXG8urL+qrJixYpKN7pERERw9epVoqKiuH37tvr45s2badmy\nJX5+fuzatYvo6GhCQkJe6Bi1cf16JpaWlixY8BWXLl3E1NSMSZOmvOphCYIgCAIgJoE1RlFREdOm\nTaOgoABHR0dOnz5Nfn4+Xbp0AcoiUFJSUujfv/8TcSmenp54enpW2m7fvn2xt7fH3t6ezz//nFmz\nZlFSUoJMJiMkJITWrVtXOL9bt258+umnlbY1ZMgQOnfuTHp6Os2bN8fS0pKTJ09iYGBATEyM+hJw\n+eQ0JCSEVq1asXHjRvbs2UNJSQlmZmYsX76cnTt3cujQIeRyOdeuXeOLL77A19dX45NMrl27xrp1\n6/j44/8LV05NTWXs2LHq+kRHRz+1zi8yIiY2uF+F1yUlJfz6awrLlq2mXbs3+eWXn5k+/T/88MMO\n9WYdQRAEQXhVxCSwhvj+++9p1aoV/v7+nDp1iiNHjqBSqdSbJx6NQKksLkWTnJwckpOTqVevHlOm\nTMHHxwcXFxf++usvvvzyS5KTkyW3VVRUxODBg+nUqRMDBw5k5syZ+Pv789lnn3Hp0iV27txJt27d\n8Pb25urVq8ycOZNNmzaRn59PXFwcOjo6jBkzhrNnzwJlES/fffcdV69eZcKECbi7u2vsu2fPnk8c\ne9aImJeleXM7HB0deffd7gC4uw/h66/DkcvzsbWtmZfcHycyt7Qj6iWdqJV0olbSiVppR0wCa4is\nrCx19IuzszMGBgYVdv4+awRKvXr1qFevHgAZGRm8/fbbQFk8y40bN7Rur127dgCYm5ur7x00NzdX\nR8T89ttv/Pjjj0BZHIqOjg76+vrqiJgbN26ov1f5KqSNjc1zRcSAdvV5WfeMtGvnzLVrC/nll+O0\nbt2G338/hUoFhoZ1a8V9K+L+Gu2IekknaiWdqJV0olaaiXsCa7hWrVpx6tQpXFxcuHDhAsXFxbRt\n25Zjx47RtWtXDh8+TLdu3bRu99GIGAcHB06ePMl7773HX3/9hZWVldbtVRXrYm9vzwcffMCQIUO4\nc+cOW7duJS0tjX379rF161b++ecf3N3dqz0ixsnJqUZGxFhaWuHr68+UKeP//0vwOkydGlgjH8sn\nCIIg/PuISWANMWzYMGbNmsWnn36qDmQOCgoiNDSUJUuWYG9vz4ABA56rjxkzZhAaGkpsbCwlJSWE\nh4dXx9DVJkyYwKxZs0hMTKSwsBBfX1+aNWuGkZER7u7uGBgY0KBBg2qLiPHy8iIoKAgvLy/09fVr\nZERMTMwK5s6NqBARM2TIh696aIIgCIIgImK0FRwczKBBg+jdu/cL60OhUODq6sqBAweeq52EhATc\n3d21yhLMysoiICCAxMTEau0rPz+fAQMG0LJlSwBcXFz4/PPPSUxMZMuWLejp6TFx4kT69u1LXl4e\ngYGByOVyGjZsyIIFCzAyMtLq3KqIiBhpxKUV7Yh6SSdqJZ2olXSiVpqJy8Gvsf379xMXF/fE8UuX\nLvHhh9qtOqWlpXHp0iV8fHwqHHd1dcXb21vj51avXl1lX+fPn2fw4MGEhoaqj+Xm5hIfH09SUhIK\nhYJBgwbRtGlTrl+/jrGxMQ0aNODs2bNERUUxfvz4Cud6e3vTs2dPoqOjGTx4MO7u7sTExJCQkMDI\nkSO1+s4vioiIEQRBEGqyWjEJTE5O5uDBg8jlcnJzcxkxYgT79+/n4sWLzJgxA6VSqd592qlTJwID\nA7lx44b66RX5+flMnjwZFxcXhgwZQpcuXbhw4QIymYzo6Gj1DtPHXb16lZCQEJRKJXXq1GHp0qVA\n2arX2rVrKSwsJCwsDCcnJ6Kiovjzzz8pKirCwcGBBQsWsHz5crKysrhz5w7Z2dnMnDmTd955h4MH\nD7Js2TJMTU2pW7curVq1ws/Pj6ioKE6cOIFKpWL69Oka65GVlcXEiROxsLCgd+/etG/fXn1/nVwu\nJzIykpMnT/LVV1/h7+9PdHR0hbZHjhyJq6trpW23bt0aR0dH4uPj+emnn9i0aRMAP/74IwMHDgRg\n6tSpqFQqlEolc+fO5cyZM+Tm5qr7qsyff/7JuXPn+Oyzz6hfvz4hISGcPXuWjh07YmBggIGBAU5O\nTowfP545c+YQExNDgwYNSEtLY8mSJZw5c6bCuU2bNiUtLY3U1FTGjx8PlMXELFmypMpJoIiIEQRB\nEIQytWISCGW7P2NjY9m1axdxcXEkJiZy7Ngx4uLiuHbtGklJSRgZGTF9+nRSUlKQyWSMGjWKrl27\ncurUKZYvX46LiwtFRUW4ubkRGhrKtGnTOHz4MG5ubpX2GRkZybhx4+jduze7d+/m/PnzQNkO2UmT\nJpGcnExycjL29vaYm5uzbt06SktLcXNz4+bNmwAYGBiwdu1aUlJSiI2NpUePHsyfP5+EhASsrKyY\nNm0aAIcOHSIrK4stW7agUCjw8PCgZ8+eGne85ubmkpSUhIGBAZs2bWLRokVYW1uzatUqfvrpJyZO\nnMjKlStZunSp1m2Xu3r1KjExMRgZGTF79myOHDmCubk5ZmZmREVFcenSJQoLCxk2bJi6L03s7e15\n88036dGjB//73/+YP38+7733XoUJuImJCYWFhZVGvzx67GnnviqPL7eLiJh/H1Ev6UStpBO1kk7U\nSju1ZhLYpk0bAMzMzHBwcEAmk1G3bl0ePHhAXl4e48aNA8omi9evX6dTp06sXLmSH374AZlMViFu\npW3btkBZNIlCodDY55UrV9RP3Bg0aBAAO3fuVMekWFlZIZfLMTQ0JC8vTx2D8uDBA5RKZYVxN2rU\niOLiYvLy8jA1NVXvzO3cuTO3b98mPT2dc+fOqS/DlpSUkJ2drXGi1qRJE/VqkrW1NeHh4RgbG3Pz\n5k2cnZ0rnKtt2+UsLS0JCgrCxMSEy5cv06FDB3r37s3Vq1eZNGmS+t48Kbp166a+V69///4sW7aM\noUOHqiNeoOzfzszMTB39UqdOHXX0y6NxME87tyo7ooa+sHtGHm9XRMT8u4h6SSdqJZ2olXSiVprV\n+nsCNcWJyGQybGxsiI2NRV9fn+TkZNq0acO3337LsGHD6NOnD0lJSWzbtu2pbT3OwcGBs2fPqlev\n7t27V+nnDx8+TE5ODt988w15eXns3btXYwyKpaUlRUVF5OXlUb9+ff744w9sbW2xt7ena9euzJs3\nj9LSUqKjo2nSpInGsT0a/RISEsK+ffswNTUlKCioQt+lpaVatw1w//59li1bxs8//wzAqFGjUKlU\nHDt2jIYNGxIbG8vp06dZsmQJ8fHx6r40CQkJ4f3332fQoEH8+uuvtGvXDicnJ7755hsUCgXFxcVk\nZGTQsmVLdfSLu7u7OvpFm3NrCktLKxYsWExU1ELk8n/Q1zcgPHyRiIgRBEEQaoRaMwnURE9Pj5Ej\nR+Lj48PDhw+xtbXF1dWVgQMHEh4ezurVq7GxsVE/ykwbM2bMYPbs2axcuZI6deqwaNEizp0798R5\nTk5OREdH4+HhgYGBAXZ2dhpjUHR0dAgNDeWLL77AzMyM0tJSmjVrRr9+/Th+/Dje3t48ePAAFxcX\nTE1NJY1z6NCheHh4YG5ujpWVlbrvzp07M27cODZs2KB126ampjg7O/PRRx9hbGyMubk5t27dol+/\nfvj7+7N+/Xp0dHSYPHnyE31VNsmeNm0aX375JZs3b8bIyIj58+fToEEDfHx88Pb2RqVS4e/vj6Gh\nIRMnTiQoKIjExETq1atHVFQUxsbGks+tSczMzDE0NKSkRIlMJhMTQEEQBKHGEBExL9m5c+eYOHEi\njRs3pl27dty7d49evXrx008/kZ+fj76+PoaGhqxdu/aZ+3g05qVfv378+OOPNXbykZmZyeTJk9m5\ncyfAC4+HgZcXESOXy/H0HEpwcGiFnMDvv096Kf0/L3FpRTuiXtKJWkknaiWdqJVmtf5y8ItSXFzM\nmDFjnjjevHlzvvrqq2rvLzQ0FFdXV44dO8bevXuxsbFh0KBBxMTEsGvXrgqraAkJCerJ0aMCAgLU\n9yo+qxfRtra13L59Oxs2bKiwSltZ5Iubm1uti4cBOH78Nxo3bkL37r0A6NWrDzY2tq94VIIgCIJQ\n5l8/CTQwMCA+Pv6FtF1UVMS0adMoKCjA0dGR06dPk5eXx8yZM4GyHcH79++noKCAgoICJkyYQEFB\nAePGjaNv3754enri6elZoc3H42F69uzJvHnz0NXVxdDQkHnz5kkaW3nb/fv3p2PHjmRmZtKtWzd2\n7NhBeHg4zZs3Z9GiReTk5BAaGopCoVC3b2NjU2kkzurVq2ncuPETkTia1K1bl40bN9K/f3/1scoi\nX+zs7KolHuZlEzmBgiAIQk32r58Evkjff/89rVq1wt/fn1OnTnHkyBHs7Ow4fvw4Xbp04eDBg/zz\nzz8olUpGjx7NiBEjuHfvHl5eXjg5OWFpaVlpu4/Gw7i7uxMeHk6bNm3Yt28fCxcuZMaMGZLH+Pff\nf7N+/XoaNGhAly5d2Lp1K6Ghobz33nsUFBQQGRmJj48Pffr04ddff2Xx4sXMnTtXciROVZPAvn37\nPnHsRcfDvMicwB1RQyu8NjTU5bffjrJhwwbat2/Pvn37CAqaysGDB2tNTqCIW9COqJd0olbSiVpJ\nJ2qlHTEJfIGysrLUkyBnZ2cMDAyIiIggPDyctWvX8tZbb2FgYICVlRXDhw9HT08PS0tL2rRpw5Ur\nVzROAh+Nh7l165Y6hubtt9/WemOEhYWF+lnFxsbGODo6AmVRPAqFgvT0dFavXs3atWtRqVTqexal\nRuJo60XGw8DLjYgxMjKnadM3aNzYntzc+7Rv35WSkhL++CONN95o/kLGUJ3E/TXaEfWSTtRKOlEr\n6UStNNM0Odap9KhQLVq1asWpU6cAuHDhAsXFxRw6dIiIiAhiYmLIz8+nZ8+eHD16lKlTpwJlE5yL\nFy9ib2+vsd1H42EaNmxIWloaACdOnOCNN97QaoxPi8uxt7cnMDCQ+Ph45s6dy4ABA9SROEuWLCEg\nIAC5XK4xEkdb5ZEvQIV4mNTUVBQKBffv338iHubRc2uSbt16kJOTTVraXwD8/vspQIaNTeNXOzBB\nEARBQKwEvlDDhg1j1qxZfPrpp+rVtmbNmjFu3DiMjIzo2rUrffr0AeDIkSN4eHigo6NDQEAA9evX\nl9TH/PnzmTdvHiqVCl1dXSIiIqr1OwQFBakfvyeXy5k1axZNmjSRHImjrdcpHsbS0gpfX3+mTBlP\nSUkJMpkOU6cG1tid2oIgCMK/i4iIeUkUCgWurq4cOHDgVQ/lX09ExEgjLq1oR9RLOlEr6UStpBO1\n0kxExFSD4OBgBg0aRO/evV94X88b4ZKQkIC7uzv6+vrs37+fuLi4J84ZMWJEhZ25j+YLajvW8r4e\nt2LFCo4dOwbAjRs3UCqV2NnZERERwcWLF/nvf/+Lnp4eH3/8MR4eHsjlcqZPn86dO3cwMTEhMjKS\n+vXrc+DAAcnn1hQiIkYQBEGoycRKoBZe5iTweT1LSPSzTgKf1pdcLickJIQzZ87w/vvvExgYiFKp\nZNCgQfzwww8YGRnh5eXFqlWr2LlzJ4WFhfj5+bFr1y5Onz5NUFCQ5HNDQkKeOt6X9Utx06b1/PXX\nOUxMTCtExLRq1fql9P+8xK9q7Yh6SSdqJZ2olXSiVprV2pXA5ORkDh48iFwuJzc3lxEjRrB//34u\nXrzIjBkzUCqVxMXFoaOjQ6dOnQgMDOTGjRvq+9jy8/OZPHkyLi4uDBkyhC5dunDhwgVkMhnR0dEV\nokcedfXqVUJCQlAqldSpU4elS5cCZatea9eupbCwkLCwMJycnCrNzFu+fDlZWVlPZOYdPHiQZcuW\nYWpqSt26dWnVqhV+fn5ERUVx4sQJVCoVI0eOxNXVtdJxPZ4T2L59e1asWAGUTbYiIyM5efIkubm5\n+Pv7Ex0dLbntR/30009s2rRJ/frbb78FYOrUqahUKpRKJXPnzuXMmTMV+qqMQqHgww8/pEePHly+\nfBmAjIwMmjZtSt26dQHo1KkTJ0+eJDU1lbFjxwJl2X/R0dFanfs0LzIiJja4X4XXJSUl/PprCsuW\nraZduzf55ZefmT79P/zww45aExEjCIIgvL5q/CQQynbMxsbGsmvXLuLi4khMTOTYsWPExcVx7do1\nkpKSMDIyYvr06aSkpCCTyRg1ahRdu3bl1KlTLF++HBcXF4qKinBzcyM0NJRp06Zx+PBh3NzcKu0z\nMkCe/fQAACAASURBVDKScePG0bt3b3bv3s358+cBaNeuHZMmTSI5OZnk5GTs7e0lZ+b16NGD+fPn\nk5CQgJWVFdOmTQPKQqOzsrLYsmULCoUCDw8PevbsqTHy5NGcwE2bNrFo0SKsra1ZtWoVP/30ExMn\nTmTlypUsXbpU67bLXb16lZiYGIyMjJg9ezZHjhzB3NwcMzMzoqKiuHTpEoWFhQwbNkzdlyZ169al\nV69eJCcnq49pk/1XnTmBj2f5vUjNm9vh6OjIu+92B8DdfQhffx2OXJ6Pra3DSxvH8xCZW9oR9ZJO\n1Eo6USvpRK20UysmgeXZc2ZmZjg4OCCTyahbty4PHjwgLy+PcePGAWWTxevXr9OpUydWrlzJDz/8\ngEwmo6SkRN1W27ZtAbCxsUGhUGjs88qVK+p77wYNGgTAzp07adeuHQBWVlbI5XKtMvPy8vIwNTXF\nysoKgM6dO3P79m3S09M5d+4cPj4+QNkKUnZ2tsaJ2qM5gdbW1oSHh2NsbMzNmzdxdnaucK62bZez\ntLQkKCgIExMTLl++TIcOHejduzdXr15l0qRJ6mf4PqunZf+VH5OSE/jouVK8rMsF7do5c+3aQn75\n5TitW7fh999PoVKBoWHdWnHJQlxa0Y6ol3SiVtKJWkknaqVZrb0cDJqz52QyGTY2NsTGxqKvr09y\ncjJt2rTh22+/ZdiwYfTp04ekpCS2bdv21LYe5+DgwNmzZ+nRowf/+9//uHfvXqWfL8/M++abb8jL\ny2Pv3r0aM/MsLS0pKioiLy+P+vXr88cff2Bra4u9vT1du3Zl3rx5lJaWEh0dTZMmTTSO7dGcwJCQ\nEPbt24epqSlBQUEV+i4tLdW6bYD79++zbNkyfv75ZwBGjRqFSqXi2LFjNGzYkNjYWE6fPs2SJUuI\nj49X96UNBwcHMjMzyc/Px9jYmJMnTzJmzBiys7M5dOgQTk5O6uw/bc591ZYvX8rBg/swNy+7dO3o\n2JKoqIXI5f+gr29AePgiEREjCIIg1Ai1YhKoiZ6eHiNHjsTHx4eHDx9ia2uLq6srAwcOJDw8nNWr\nV2NjY8Pdu3e1bnvGjBnMnj2blStXUqdOHRYtWsS5c+eeOM/JyUlyZp6Ojg6hoaF88cUXmJmZUVpa\nSrNmzejXrx/Hjx/H29ubBw8e4OLigqmpqaRxDh06FA8PD8zNzbGyslL33blzZ8aNG8eGDRu0btvU\n1BRnZ2c++ugjjI2NMTc359atW/Tr1w9/f3/Wr1+Pjo4OkydPfqIvqZNsfX19goODGTNmDCqVio8/\n/hhra2u8vLwICgrCy8sLfX19oqKitDr3VfvzzzPMnRvBW2+1f9VDEQRBEIQqid3BL9nq1asZNWoU\nBgYGBAYG0qtXLz788MNXPax/lRd1uaC4uJiBA9+la9fu/P13FnZ2TfHzm0ajRo1eSH8vmri0oh1R\nL+lEraQTtZJO1EqzWn05+EUpLi5mzJgxTxxv3rw5X3311Qvp08TEBA8PD+rUqYOtra36fsPHPW9O\nYFWep+3ymJzbt29z+fJlAgMDAWm1VCgU/O9//2PYsGEVzsnIyCAsLIz4+Pgq+87MzGTy5Mnqsefl\n5REYGIhcLqdhw4YsWLAAIyOjKtt4kW7fzsXZuTNffDGR5s0d2Lw5npkzA4iN3fTcj9MTBEEQhOom\nVgIFrWiaBEqhKYdQyiRw+/btbNiwgZs3b5KSkgKUPTKvbdu2uLu7ExMTg4GBASNHjqxyDNUZEfN4\nJMzjVCoVAwa8S1zc9zRuXPtCosWvau2IekknaiWdqJV0olaaiZXA10xhYSGzZs3i/v373L17FxcX\nF3bu3Mnu3buRyWTMnTuXHj160LRpU+bPnw+AhYUFERERGrMRg4ODyc/PJz8/n9WrV7Ny5UpSU1MB\nGDx4MJ9//rnk8aWmphIZGYmenh7m5uYsXryYVatWcenSJVasWIGHhweBgYGoVCoaNGjw1Pbq1q3L\nxo0bKzzhJDU1lfHjxwNlOYFLlix56iSwOj3+H1VaWhppaWnqy/tlv69UWFtb1NrYgto67ldF1Es6\nUSvpRK2kE7XSjpgE1lKZmZm4ubnx/vvvc/PmTXx8fGjbti0nT56kffv2HD9+nFmzZuHt7U1ERASO\njo5s3bqVtWvX4u/vr7Hdbt26MXLkSA4ePEhWVhaJiYmUlJTg7e1Nt27dJI9v37599O/fnzFjxnDg\nwAEKCgqYMGEC6enp+Pr6EhkZyeDBg/Hw8GD37t1s3ry5yvb69u37xLFnzQmsrl+Kj7dz794/zJs3\nn+bNW9O4sS3JyVtxcHBEV9ekVv46Fb+qtSPqJZ2olXSiVtKJWmkmVgJfM1ZWVqxfv549e/ZgampK\nSUkJHh4ebNu2jdzcXPr164eenh4ZGRnMnTsXAKVSSfPmzatst/z9jIwMOnfujEwmQ19fn/bt25OR\nkSF5fBMmTGDVqlV8/vnnWFtb4+TkRHFxsfr9ixcvMnRoWXCzs7PzUyeBlSnPCaxTp45WOYEvir29\nI/7+0wkK8qe0tJQGDRoyZ07EKx2TIAiCIGii8/RThJooNjaWDh06sHjxYgYOHIhKpaJ79+789ddf\nJCUl8cknnwBlk7rIyEji4+OZPn06ffr0qbLd8g0MDg4O6kvBSqWS06dP06xZM8nj27FjBx999BHx\n8fG0aNGCxMREdHR01HmC9vb2nD59GoCzZ89q/f2hbPJ46NAhgBqTE5iefoGioiL09Q3Iz88nOvrb\nVz0kQRAEQaiUWAmspfr27UtYWBg7duzAwsICXV1dlEolAwYM4OjRo+oJW1hYGEFBQTx8+BCA8PBw\nye0fP34cT09PlEolAwcOVD8tRYq33nqL4OBgjI2N0dfX56uvvsLS0hKlUsmiRYv4z3/+g7+/P7t3\n735qeLUmEydOJCgoiMTEROrVqydyAgVBEARBC2J38L+Mj48PYWFh7N69GysrK7y8vF71kCr18OFD\n/P39+eSTT+jduzcAK1as4Oeff0ZPT48vv/wSJycnMjMzCQ4ORiaT0aJFC+bMmVPhiSqVETmB0oj7\na7Qj6iWdqJV0olbSiVppJu4JFAAoLS0lKCiIO3fuoK+vz+7du4Fnz0b09fVVP1KvnKmpKStXrtS6\nrRUrVnDs2DHkcjmXL1+muLiYK1eu0Lx5cwoKCjh+/Dhbt24lJycHPz8/kpKSWLBgAVOnTqVr167M\nnj2b/fv3V9hB/LgXGREjcgIFQRCE2kRMAl+i5ORkDh48iFwuJzc3lxEjRrB//34uXrzIjBkzuHHj\nBnv27KGkpAQzMzOWL1/O1q1bOXXqFFFRUQQFBeHk5MSnn35aafs+Pj7Uq1ePgoICYmJimDVrFtev\nX+fhw4eMGjWKQYMGoaOjQ2Rk5FNXAoODg9HT0yM7O5vi4mIGDRrEwYMHycnJITo6mqZNmxIVFcXt\n27dRqVSMHDkSV1dXjh8/zooVKxgxYgRyuZzIyEj09fWZNq1sRez69eu89dZb6s0qj/L19cXX15e0\ntDT09fVZs2YNgwYNws7Ojg0bNtCrVy9kMhmNGzfm4cOH5OXlce7cObp06QKUxcSkpKRUOQmsTo//\nsmrQoDXr169Tv54yZRLr13+HQnEPOzu7lzKm6ibiFrQj6iWdqJV0olbSiVppR0wCX7KioiJiY2PZ\ntWsXcXFxJCYmcuzYMeLi4njzzTeJi4tDR0eHMWPGcPbsWT799FNSUlIIDg5GqVRqnACWGzJkCP37\n92fjxo3Uq1ePRYsWUVhYiLu7u1YRLwC2trbMnz+f2bNnk5WVxZo1a1i2bBkHDhygefPmZGVlsWXL\nFhQKBR4eHvTs2ZOLFy+yaNEirK2tWbVqFT/99BNDhgzh6tWrfPfddxgZGeHi4kJubq7GfMDWrVs/\ncaywsBALCwv16/JIGJVKpV5lkxIT8yIjYi5dusilS+kMHOgGlOUElpaqKChQ1MpLFOLSinZEvaQT\ntZJO1Eo6USvNxOXgGqJNmzYAmJmZ4eDggEwmo27duiiVSvT19QkICMDY2JgbN25QUlICwLhx4/D0\n9CQ5Ofmp7T8a8dKjRw+g7PKsg4MD169f12qsbdu2BcDc3Bx7e3v138XFxaSnp3Pu3Dl8fHwAKCkp\nITs7G2tra8LDwzE2NubmzZs4OzsD0LRpU0xNTQFo0KABCoVCq7GUx8GUKyoqwszMrML9f686JkZH\nR8Y33yzGyakDjRvbsm3bDzg6OtKwofUrG5MgCIIgaCIiYl4yTfeGKZVK9u3bxzfffENoaCilpaWo\nVCqKi4uJiIjgq6++IiwsrELWXlXtOzg4cPLkSaBsFS09PV3rXbhV3cdmb29P165diY+PZ/369bi6\nutKkSRNCQkKIiIhg4cKFNGzYkPJ9R897T5yzszNHjhyhtLSU7OxsSktLqV+/Pm3btuXYsWNAWUxM\n586dn6ufZ3X48M+MHz9anRP46aefcPjwQZETKAiCINRYYiWwhtDT08PIyAh3d3cMDAxo0KABt27d\nYvHixbz77rt4enpy69YtoqKimDlz5lPb8/DwIDQ0FC8vLxQKBb6+vlhaWlbbePv168fx48fx9vbm\nwYMHuLi4YGpqytChQ/Hw8MDc3BwrKytu3bqlddvJyclcvny5wrE333yTzp074+npSWlpKbNnzwYg\nKCiI0NBQFi9ejJ6eHnPmzKmW76eN69ev8d//fgOoGDBgEAMGDHrpYxAEQRAEbYmIGKHGKZ8EBgYG\nSv5MVlYWAQEBJCYmPvXc6rxnRC6X4+c3ns8/H83cuSHs3ftLtbX9qon7a7Qj6iWdqJV0olbSiVpp\nJu4JfE1kZ2cTFBT0xPG3336bKVOmaNVWcXExY8aMeeL4s8bFaOPMmTMsWrToieOurq7UqVMHgLy8\nPCZNmsTHH39MZmYmgYGBKBQKXF1dOXDgQIXd0E2aNOHSpUusWLECX1/fFzr2Ry1aFM7Qoe44OLR4\naX0KgiAIQnUQk8BapnHjxsTHx1dLWwYGBtXWlracnJw09p2cnMydO3eYOHEiX375ZZXPLC7fDZ2V\nlUV6evpTJ4DPkxO4I2pohdebNm3CxMSIUaM+IysrC5lM9trFE7xu3+dFE/WSTtRKOlEr6USttCMm\ngUKN9Msvv9CgQQP1s4bLPX73QvluaKmeJyLm8c9t3foDcrkcN7chlJQo1X8vXvwtVlaVx9/UJuLS\ninZEvaQTtZJO1Eo6USvNxOVgoVb58MMP+fDDD/nPf/7D+PHjyc3NBeDcuXMVzivfdayjo/PEhPFF\nW7Nmg/rvnJxsRozwJC7u+5c6BkEQBEF4ViIiRqixHB0d+eCDDzhx4gR///03Xl5e/Pjjj5iYmDxx\nrqWlJUqlstL7DF+G48d/459//nklfQuCIAjCsxC7g4V/neq+XHD9+jUCA6eQl3dH7A7+FxP1kk7U\nSjpRK+lErTTTdDlYrAQKwnOQy+V89VUofn7+r3oogiAIgqAVMQl8jT1LVMry5cvZvHmzVp/Jz89n\nx44dTz3vn3/+Yfjw4erdvuWhz56envj4+JCZmQnA77//zrBhwxg+fDgrVqzQ+tyXSUTECIIgCLWV\n2BjyGntZk6ILFy5w4MABhgwZovGcs2fPMmfOHG7evKk+tm/fPoqLi0lISOD3339n4cKFrFy5kjlz\n5rB8+XLs7OwYN24c586d4++//5Z8brt27TSO43kiYmKD+1V4nZy8FV1dPQYPHkpOTvYztysIgiAI\nr4KYBNZiycnJHDx4ELlcTm5uLiNGjGD//v1cvHiRGTNmMGfOHFJSUvDx8aF169ZcvHiRwsJCvv32\nW2xtbats++HDh8yePZsbN25w9+5devfuzdSpU9mzZw9r1qxBT08PW1tbvv76a1atWkVaWhoJCQl4\nenpW2l5xcTH//e9/mTFjhvpYamoq77zzDgAdOnTgzz//pLCwkOLiYpo2bQpAr169+PXXX8nNzZV8\nblWTwMez/p7H3r27kcvljB37GUqlEoVCwdixnxETE4O1tXW19fMqicwt7Yh6SSdqJZ2olXSiVtoR\nk8BarqioiNjYWHbt2kVcXByJiYkcO3aMDRs2VDjPycmJWbNmsXTpUnbt2sW4ceOqbDcnJ4cOHTow\nbNgwFAqFehK4c+dORo4ciZubG9u3b6ewsJAJEyawZcsWjRNAgE6dOj1xrLCwEFNTU/VrXV3dJ46Z\nmJhw/fp1rc59muq6cXjlynXqv8sjYtau3VitfbxK4iZr7Yh6SSdqJZ2olXSiVpqJnMDXVJs2bQAw\nMzPDwcEBmUxG3bp1USgUFc5r27YtAI0aNeL27dtPbdfCwoKzZ8/y22+/YWpqSnFxMQAzZ85k9erV\nbN68GXt7e1xcXJ557KamphQVFalfl5aWPnGsqKgIc3Nz5HK55HNfBRERIwiCINQ2YmNILVcellzd\nkpOTMTMzIyoqitGjRyOXy1GpVCQkJODn58fGjWUrXnv37n3moGZnZ2cOHz4MlG3waNmyJaampujr\n63Pt2jVUKhVHjhyhc+fOWp37sl2/fo3vv9+AkZHRS+9bEARBEJ7Vv3olMCsri4CAABITE1/1UJ4q\nOzubtLQ0+vXr9/STtRQcHMygQYO4ffs2R48e5YMPPqB79+4EBASQmpqKkZERzZo149atWzg5OTFq\n1CgsLCwwMTHh3Xffpbi4mPT0dOLi4hg5cqTkfvv3709KSgrDhw9HpVIREREBwNy5cwkMDOThw4f0\n6tWL9u3b89Zbb5GSkkKvXr0wNDQkJiYGgN69e/PBBx8A0KdPH9q3b1/t9anKoxExc+eGvNS+BUEQ\nBOF5/KsngbXJb7/9xuXLlytMAt3d3dV/9+7dm969ewNll4i/++479Xvx8fHqv728vKrsp1OnTupz\nKot9sba2rnQi+uOPP0r6Ho+ORUdHh6+++uqJczp06PDExDw/P5/s7Gzq1KnDmDFjcHBwIDc3l59/\n/pnjx4+jUCjw9vamuLgYAwMDSWOpDiIiRhAEQaitatUksLCwkFmzZnH//n3u3r2Li4sLO3fuZPfu\n3chkMubOnUuPHj3o379/pZ+Pjo5m3759PHz4EC8vL3r16kVeXh6TJk0iNzeXVq1aMX/+fNLT01m4\ncCGlpaUUFBQQEhKCs7Mz77//Ps7Ozly5cgVLS0uWL1+OUqlkxowZ3Lp1CxsbG06cOMGRI0e4cOEC\n8+fPB8rur4uIiMDMrPIbM4ODg8nPzyc/P5+VK1eyePHiCrty/fz8iImJQS6X07FjR5o0afJMba9e\nvZqVK1eSmprK1atXsbCwoFGjRly+fJlz586hq6tLr169NNZ/+fLlZGZmcvfuXe7du4e3tzd79uzh\nypUrREZGcufOHRYuXMidO3eQyWTUr1+fRo0a4eLiwqFDhyTVU1dXt9K+i4qK8PPzU18SBjhz5gwd\nO3bEwMAAAwMDmjZtSlpaGk5OThq/g4iIEQRBEIQytWoSmJmZiZubG++//z43b97Ex8eHtm3bcvLk\nSdq3b8/x48eZNWtWpZ89f/48hw8fZuvWrRQXFxMVFUXPnj0pLCxkwYIFmJmZ0b9/f+7cucOlS5cI\nCgqiVatW7Nixg+TkZJydnbl+/Trr16/HxsaG4cOHc/bsWf744w+aNGnCsmXLyMjIYPDgwQCEhoYS\nERGBo6MjW7duZe3atfj7a36qRLdu3Rg5ciRZWVmV7sodN24cly9f5r333sPDw+OZ2j548CBZWVkk\nJiZSUlKCt7c3ISEhrFu3Tn05+PLly1X+G9SpU4fvvvuOmJgYDh06xKpVq0hKSmLXrl14enpiZWXF\nTz/9hEwmY+TIkYSFhZGWlia5nh06dKi0Xzs7O+zs7CpMAgsLCytMfk1MTCgsLKxy/M/j8d1VIiJG\neJyol3SiVtKJWkknaqWdWjUJtLKyYv369ezZswdTU1NKSkrw8PBg27Zt5Obm0q9fP/T0Kv9KV65c\nwcnJCV1dXYyMjAgJCSErKws7Ozvq1q0LgKWlJf/88w8NGzYkOjqaOnXqUFRUpI4hqVevHjY2NgDY\n2NigUCjIyMhQX4Z1cHCgfv36AGRkZDB37lwAlEolzZs3r/K7lb+vaVfuo5617YyMDDp37oxMJkNf\nX5/27durn94hVfkuYzMzMxwdHQHUu5HT09PJzs5W3xd47949rl27plU9tVHZ7mBNK6LldkQNfeYI\ngcc/JyJihEeJekknaiWdqJV0olaavRbPDo6NjaVDhw4sXryYgQMHolKp6N69O3/99RdJSUl88skn\nGj9rb2/P+fPnKS0tRalUMmrUKIqLiyvdXRseHs6UKVOIjIykZcuWqFQqoPKduC1btuT06dMAXLt2\njbt37wJlE6/IyEji4+OZPn06ffr0qfK7lbetaVfuoztwn7VtBwcHUlNTgbLJ4+nTp2nWrFmVn9XU\nVmXs7e1xdHRkw4YNxMfH4+7uTsuWLbWqpzacnJxITU1FoVBw//59MjIyaNmy5XO1KQiCIAj/FrVq\nJbBv376EhYWxY8cOLCws0NXVRalUMmDAAI4ePVrlhKZNmza88847eHl5UVpaipeXl8YNBB988AGT\nJk3C0tKSRo0aqSd2lfnkk08IDg7m008/pXHjxhgaGgIQFhZGUFAQDx8+BMomllJo2pXbsmVLVq5c\nSbt27Z657b59+3L8+HE8PT1RKpUMHDiwyqdraKt169Z0794dLy8viouLcXJywtraWqt6aqNBgwb4\n+Pjg7e2NSqXC399fXf+X7eLFdODFxPUIgiAIwosgU5UvywgVSI2POXXqFA8ePKBXr15cvXqVsWPH\nsm/fvhc+vp49e5KSkvJC+/Dx8SEsLIzdu3djZWX11J3Fj8rPz+eXX3554nnChw8fZvfu3SxcuLDK\nz//xxx8sXrxYvZs4MzOT4OBgZDIZLVq0YM6cOejo6LBixQp+/vln9PT0+PLLL6vcFFKuui8XXL9+\njcDAKeTl3WHv3l+qte1XSVxa0Y6ol3SiVtKJWkknaqXZv+aJIQkJCezcufOJ4wEBAXTs2LHa+7Oz\nsyMgIIAVK1ZQUlLC7NmzKz2vuLiYMWPGPHG8efPmlcakaKO62/b19eXevXv89ddfBAUFcefOHczM\nzLSaBF64cIEDBw48MQmsSnZ2NkFBQWRnZ3Pnzh10dHTw8fHh7bff5vz580ydOpWuXbsye/Zs9u/f\nT+PGjTl+/Dhbt24lJycHPz8/kpKStP6+z0PkBAqCIAi11WszCXw0PqawsLDS+Jjbt2+/kPiY+Ph4\nddzJf//7X7Zs2VJpfMyVK1eeiI+5e/cu9+/f17ihQVNcTXFxMf7+/uTk5NCqVSs2bNjAqVOniIyM\nRE9PD3Nzc2bMmKGxXj4+PtSrV4+CggJiYmKYNWsW169f5+HDh4waNYpBgwY9sRKoyZ49e1izZg16\nenrY2try9ddfs2rVKtLS0khISKBz5858+eWXGBkZYWRkpN6I87jGjRsTHx/P//t//49WrVoxY8YM\n9UrgO++8Q5cuXYCyTMSUlBSaN29Or169kMlkNG7cmIcPH5KXl6fenPMyiJxAQRAEobZ6bSaBr2t8\njKb+5HI5gYGB2Nra8p///IcDBw5w8uRJ+vfvz5gxYzhw4AAFBQXqnbiVGTJkCP3792fjxo3Uq1eP\nRYsWUVhYiLu7O926dZNc+507dzJy5Ejc3NzYvn07hYWFTJgwgS1btuDp6cmUKVOYMmUKPXv2JCYm\n5qkxNAMGDCArK6vCMZVKpd5IYmJiop7sW1hYqM8pP17VJPB5cgJ3RA2t8HrTpk2YmBgxatRnZGVl\nIZPJXrt4gtft+7xool7SiVpJJ2olnaiVdl6bSeDrGh+jqb/GjRtja2sLQMeOHbly5QoTJkxg1apV\nfP7551hbWz/1/rhHo2N69OgBlMWuODg4cP369adU/P/MnDmT1atXs3nzZuzt7XFxcanw/sWLF9Vj\ncXZ2fuoksDI6Ov+3kb2oqAhzc/NXHhGzdesPyOVy3NyGUFKiVP+9ePG3WFk1eKY+ahJxf412RL2k\nE7WSTtRKOlErzV6LiJiqvK7xMZr6u3HjBrdu3QLKNqe0aNGCHTt28NFHHxEfH0+LFi2euqnl0eiY\nkydPAmWX1dPT02nSpEmVn31UQkICfn5+bNxYlpG3d+/eCpE29vb26jr8+eefktt9VNu2bTl27BhQ\ntrmkc+fOODs7c+TIEUpLS8nOzqa0tPSlXgpes2YD8fGJxMV9z6JF32JoaEhc3PevxQRQEARBeP29\nNiuBr2t8jKb+LCwsmD9/Pjdv3qRjx4706dOHP/74g+DgYIyNjdHX15e8KcTDw4PQ0FC8vLxQKBT4\n+vpiaWkp6bNQltc3atQoLCwsMDEx4d1336W4uJj09HTi4uKYM2cO/v7+fPfdd9SvX/+ZYlyCgoII\nDQ1lyZIl2NvbM2DAAHR1dencuTOenp6UlpZq3JTzMhw//hv//PPPK+tfEARBELQlImJeoFcVHyNU\nTUTESCMurWhH1Es6USvpRK2kE7XS7F8TEVOVmhwf8+mnn6rvlTMyMuKNN97A3t6eW7dukZ+fj76+\nPoaGhqxdu1Zy/+WRK+UUCgWXLl1i5MiRbN++nR9//FHyqlx1x9CEhYVV+si6NWvWUKdOHTIzM5k8\nebL63ysvL4/AwEDkcjkNGzZkwYIFGBkZkZiYyJYtW9DT02PixIn07dtX67E8DxERIwiCINRWYiWw\nhpgwYQKjRo1S5+C988479O/fn0GDBrFr167nfsQaVAzA7tevn1aTwJdp+/btbNiwgZs3b6oDsefP\nn0/btm1xd3cnJiYGAwMD3NzcGD16NElJSSgUCry9vUlKStJ4Kb9cdf5SnDcvlI4dO9Op09uMGOEp\nVgL/xUS9pBO1kk7USjpRK83ESmANUlRUxLRp0ygoKMDR0ZHTp0+Tn5//RA5ex44dKSgoYMKECRQU\nFDBu3DiNK11ZWVlMnDgRCwsLevfuTc+ePZk3bx66uroYGhoyb948rcbYv39/OnbsSGZmJt26y7xq\n5QAAHCVJREFUdeP+/fucOXOG5s2bs2jRInJycggNDUWhUKjbt7GxISoqij///JOioiIcHBxYsGAB\ny5cvJysrizt37pCdnc3MmTN55513NPZdt25dNm7cWCHTMTU1lfHjx6vrs2TJEuzs7OjYsSMGBgYY\nGBjQtGlT0tLSqtwV/TwRMbHB/Sq8Tk7eiq6uHoMHDyUnJ/uZ2xUEQRCEV0FMAl+B77//nlatWuHv\n78+pU6c4cuRIpTl4SqWS0aNHM2LECO7du4eXlxdOTk4aN23k5uaqV8Lc3d0JDw+nTZs27Nu3j4UL\nF1YZHv24v//+m/Xr19OgQQO6dOnC1q1bCQ0N5b333qOgoIDIyEh8fHzo06cPv/76K4sXL2bu3LmY\nm5uzbt06SktLcXNz4+bNmwAYGBiwdu1aUlJSiI2NrXISWNlEt7CwUB3/8mhO4KORMCYmJhQWFlb5\nvR7P+nsee/fuRi6XM3bsZyiVShQKBWPHfkZMTAzW1tbV1s+rJDK3tCPqJZ2olXSiVtKJWmlHTAJf\ngaysLPUkyNnZGQMDA0pKStTvl+fgWVlZMXz4cPT09LC0tKRNmzZcuXJF4ySwSZMm6kuht27dok2b\nNgC8/fbbREVFaTVGCwsLGjduDICxsTGOjo4AmJmZoVAoSE9PZ/Xq1axduxaVSqW+ZzEvL4+AgACM\njY158OABSqUSQD2WRo0aUVxcrNVYAHUmYHle4rPmBEL1XQ5euXKd+u+cnGxGjPBk7dqN1drHqyQu\nrWhH1Es6USvpRK2kE7XS7LXPCaxNWrVqxalTp4CyZ+wWFxdXmoN39OhRpk6dCpRNcC5evIi9vb3G\ndh8NVG7YsCFpaWkAnDhxgjfeeEOrMT7tHkR7e3sCAwOJj49n7ty5DBgwgMOHD5OTk8OSJUsICAhA\nLpdXmaOoDWdnZw4dOgSU1adTp044OTmRmpqKQqHg/v37ZGRk0LJly+fq51mJiBhBEAShthErga/A\nsGHDmDVrljo/EDTn4B05cgQPDw90dHQICAiQHIY8f/585s2bh0qlQldXl4iIiGr9DkFBQYSFhaFQ\nKJDL5cyaNYsmTZoQHR2Nh4cHBgYG2NnZqQOtn9fEiRMJCgoiMTGRevXqERUVhbGxMT4+Pnh7e6NS\nqfD3938lG12uX7/G999vwMjI6KX3LQiCIAjPSuwOfsUUCgWurq4cOHDgVQ+lxitfafT09Hyudqrz\ncoFcLsfPbzyffz6auXNDxO7gfzFRL+lEraQTtZJO1EozsTv4NVGdWYf79+8nLi7uieMjRoyosDP3\nRVixYoX68vejIiIisLOzq/Qz5c9hrkkWLQpn6FB3HBxavOqhCIIgCIJWxEqgUGskJydz+fJldHR0\nOHLkCNbW1uTl5REVFcW2bds4ffo0Dx48IDw8HAcHh0rbqO6ImLS083z55Rz1xhCxEvjvJeolnaiV\ndKJW0olaaSZWAoXXwpkzZygpKeGHH36gsLCQgQMHqt+zt7cnJOTFPbXj8f+IRESM8DhRL+lEraQT\ntZJO1Eo7YhIo1CpZWVm4uLigo6ODubm5OnoGyh5h9zQ7ooY+8y/Fxz8nImKER4l6SSdqJZ2olXSi\nVpqJlUDhtfDWW29x5swZHj58SHFxMZcuXVK/92hEjiAIgiAIVROTQKFWsbOzo0OHDnh6emJlZYWe\n3sv/n3BSUgLbtiUhk4GtbROCgkKwsWn8Wt0PKAiCILz+xNKJFoKDgzl8+PCrHoYkCQkJ6qd1aGPp\n0qW4u7sTFxfHihUrJH0mPz+fHTt2VHnOunXrcHNzw8fHBx8fHy5fvvz/x6v44e3tzRdffEFeXh4A\nBw4c4OOPP8bT05PExESgLIrl4MGDnDp1iqNHjxITE8OqVavQ09Nj4sSJHDlyBF1dXa2/r7bS0v5i\n8+aNrFoVS3x8Ik2aNGXNmpUvvF9BEARBqG5iJfA1tXr1aj788EOtP7d79262bduGqamp5M9cuHCB\nAwcOMGTIEI3nnDt3jsjISN588031sXXr1tGyZUv8/PzYtWsX0dHRBAUFsWDBAn744QeMjIzw8vKi\nb9++7Ny5s9Jzs7OzSUxMxMHBQX1ugwYNtP7eUrVu3YYtW7ahp6eHQqEgN/cWjRvbvrD+BEEQBOFF\nqfGTwOTkZA4ePIhcLic3N5cRI0awf/9+Ll68yIwZM1AqlcTFxaGjo0OnTp0IDAzkxo0b6qdZ5Ofn\nM3nyZFxcXBgyZAhdunThwoULyGQyoqOjNT5r9urVq4SEhKBUKqlTpw5Lly4FylbY1q5dS2FhIWFh\nYTg5OREVFcWff/5JUVERDg4OLFiwgOXLl5OVlcWdO3fIzs5m5syZvPPOOxw8eJBly5ZhampK3bp1\nadWqFX5+fkRFRXHixAlUKhUjR47E1dW10nFlZWUxceJELCws6N27N+3bt1ev2MnlciIjIzl58iS5\nubn4+/sTHR0tue0VK1Zw48YNxo8fz7hx49i+fTtLly6lb9++2NvbY29vz9tvv82aNWvQ09PD1taW\nr7/+mlWrVpGWlkZCQoLGIOdz584RExNDbm4u7777LuPHjyc1NZWxY8cCZRmA0dHRZGRk0LRpU+rW\nrQtAp06dOHnypMZzu3Tpot4cUn6upu8H2kXEPB4JU05PT4/Dh38mMnIe+voGjB07QXKbgiAIglBT\n1PhJIJQ9Nzc2NpZdu3YRFxdHYmIix44dIy4ujmvXrpGUlISRkRHTp08nJSUFmUzGqP+vvXuPiuI+\n/zj+XsEVdbkJxWAgHsCQqikgGo2RELUEL2g9UUsCKYiJJfEaxQtEwWCCKEGtBo+IsQRLooKFNGpN\na7UpxKZKxUu9xPgrN0UNgoh1V2EXdn9/eNwWZcniJaD7vP7b2ZnvfOfD5Tw7s/PM1KkMGTKEI0eO\nkJaWRlBQEBqNhpCQEBISEpg/fz6FhYWEhIS0uM+UlBSio6MJDAxkz549nD59GoD+/fszY8YM8vPz\nyc/Px9PTEzs7Oz755BP0ej0hISFUVVUBoFQq2bx5M3//+9/JzMzkhRdeICkpiZycHJydnZk/fz4A\nBQUFVFZWsn37dhoaGggNDWXYsGHY2dm1OLfq6mry8vJQKpV89tlnpKam0rNnTzZu3Mif/vQnpk+f\nTnp6Or/5zW/aNPasWbPIz88nMzOTY8eOGZdfunSJ/Px8HB0dmTNnDlFRUYSEhPCHP/wBtVrN22+/\nzfbt21t9kkdISAjh4eGoVCpmzZrFV199hVqtNhbh3bt35/r1682W3V6uVqvbtO6D0lqrgUmTxjNp\n0nhyc3NZuHAOf/nLXx7LG1Ok3ULbSF7mk6zMJ1mZT7Jqm0eiCLx9psfW1hYvLy8UCgX29vbcuHGD\n2tpaoqOjgVvF4vnz5xk4cCDp6en8/ve/R6FQ0NjYaByrX79+ALi6utLQ0GByn2VlZcYncIwdOxaA\n3bt3079/fwCcnZ2pr6+nS5cu1NbWEhMTQ7du3bhx44bxu3i35/3EE0+g1Wqpra1FpVLh7OwMwKBB\ng6ipqeHs2bOcOnWKiIgIABobG7l48aLJItDNzQ2lUglAz549Wb58Od26daOqqgp/f/9m67Z17JY4\nOjri6OgIwLvvvktGRgbbtm3D09OToKCgH9zeYDAwZcoUY8H20ksvcfr0aVQqFRqNBrj1s7Ozs2u2\n7PZyW1vbNq3bmra0iGlpvcrK81y5cgVfXz8AAgODee+99ygtvYC9vYNZ4z4qpN1C20he5pOszCdZ\nmU+yMs1UcfxInLpQKBQml7u6upKZmUl2dja/+tWv8PX1Zd26dUyYMIHU1FSGDBnC/z4UxdRYd/Ly\n8uLEiRMA7Ny5k+zs7Ba3v/082zVr1hATE0N9fb1xf3eu6+TkhEajMd4Acfz4ceBWk+MhQ4aQnZ3N\nli1bGDNmDG5ubibn9r9nnOLj40lOTmblypW4uLg027der2/z2D+0v5ycHGbPns2nn97qh3f7DJhe\nrze5vVqtZty4cWg0GgwGA4cOHeLZZ5/F39+fgoIC4FaOAwcOxMvLi4qKCurq6tBqtRw+fJgBAwa0\nad2H6cqVGhITF1NXVwfA3r1f4uHh9dgVgEIIIR5/j8SZQFOsra2JiooiIiKCpqYmnnzyScaMGcPo\n0aNZvnw5GRkZuLq6cvXq1TaPvWjRIpYuXUp6ejo2NjakpqZy6tSpu9bz8fFhw4YNhIaGolQqcXd3\n5/Llyy2O2alTJxISEvj1r3+Nra0ter2e3r17M3LkSIqKiggPD+fGjRsEBQWZfWPGhAkTCA0Nxc7O\nDmdnZ+O+Bw0aRHR0NL/73e/ueeyW+Pj4MHXqVBwcHOjevTvDhw9Hq9Vy9uxZsrKyiIqKumsbW1tb\n5s2bR2RkJEqlkqFDh/LSSy8xePBgYmNjCQsLo3PnzqxevZrOnTsTFxfHm2++icFgYNKkSfTs2ZOw\nsDCz132YfH0H8LOf+fDKK2NQKBR0796dDz9c+1D3KYQQQjwMFvHs4MrKSmJiYoztRtpTRkYGU6dO\nRalUsmDBAgICAprdxXvx4kXOnDnDyJEt35RwP+Li4hg7diw1NTWUlpayYMGCB76PByU5ORkPDw/C\nwsIAyM3NZfv27caWMCNGjKC2tpYFCxZQX1+Pi4sLK1asoGvXrj849v1cLjhz5lvi4xeRlbUNlUrF\n+vVruXFDw6JFS+55zI5KLq20jeRlPsnKfJKV+SQr0+SJIS3QarW8+eabdy338PDg/ffffyj77N69\nO6GhodjY2PDkk08av29428GDByktLaW6uprdu3fftX1MTMx9X/IsKiri4MGDxsvRD2rs/fv3k5WV\nddfyyMhIXn75ZbPGqK2tZdGiRZSXlxt/NtXV1WRnZ5OXl0dDQwPh4eEMGzaMDRs2MG7cOCZOnMim\nTZvIyclp8UzkgyQtYoQQQjwuOmQRqFarWbJkCdevX+fq1asEBQWxe/du9uzZg0KhYNmyZbzwwgsm\nC4sNGzawb98+mpqaCAsLIyAggNraWmbMmEF1dTXPPPMMSUlJlJeX07lzZ/R6Pf/5z3+Ij4/H39+f\n4OBg4uLiKCsrw8nJibS0NHQ6HYsWLeLy5cu4urryz3/+kwMHDvDdd9+RlJQEgIODA8nJySZvToiL\ni6Ourg4bGxvS09NZtWoV06dP5+rVqwQGBjJ79mw2bdpEfX09CQkJxMfHt3nsuro6MjIySE9Pp7i4\nGIBx48YxZcoU47qDBw/G2dnZ5JnAtLQ0KioquHr1KteuXSM8PJy9e/dSVlZGSkoKfn5+ZGdns3v3\nbhQKBWPHjiUyMhJ3d3eTee7fv79ZnqYaO2s0GmbPnt2sKfe//vUvBgwYgFKpRKlU8tRTT3HmzBmK\ni4t56623gFttY9asWfPQi0CQFjFCCCEeDx2yCKyoqCAkJITg4GCqqqqIiIigX79+HD58GF9fX4qK\niliypOXLb6dPn6awsJAdO3ag1WpZvXo1w4YNQ61Ws2LFCmxtbXn55Ze5cuUK//73v4mNjeWZZ55h\n165d5Ofn4+/vz/nz59myZQuurq689tprnDhxguPHj+Pm5sZHH31ESUkJ48aNAyAhIYHk5GT69OnD\njh072Lx5M/PmzTN5bM8//zxRUVFUVlbi5+fHL3/5SxoaGggMDGTu3LlER0dTWlrKz3/+c0JDQ+9p\n7K+++orKykpyc3NpbGwkPDyc559/vk0/AxsbG37729+yadMmCgoK2LhxI3l5efzxj39EpVKxZ88e\ntm7dikKhICoqioCAgDbl6efn1+J+3d3dcXd3b1YEtqVtzA95EH0CAQIDhxMYOJydOz8nJmY2OTmf\nP5YtYoQQQjy+OmQR6OzszJYtW9i7dy8qlYrGxkZCQ0P5/PPPqa6uZuTIkSafGVtWVoaPjw9WVlZ0\n7dqV+Ph4KisrcXd3NzYgdnJy4ubNm7i4uLBhwwZsbGzQaDTGGyYcHR1xdXUF/ttKpqSkhMDAQODW\nncM9evQAoKSkhGXLlgGg0+nw8PBo9dhuv+/g4MCJEyc4ePAgKpUKrVZ717r3OnZJSQmDBg1CoVDQ\nuXNnfH19KSkpaXXbO91upWNra0ufPn0AsLe3p6GhgbNnz3Lx4kXjWbdr165x7ty5NuXZFj/UNub2\n/sxpe7Nr9YQ27ftOFRUVVFdXM2jQIACiol5n1aoVKJV6HB3t72vsjkh6brWN5GU+ycp8kpX5JKu2\n6ZBFYGZmJn5+foSHh3Pw4EEKCgoYOnQoqampVFVVsXTpUpPbenp6sm3bNvR6PU1NTURHR5OQkNBi\na5jly5ezatUqvLy8+Oijj7hw4QLQchsZb29vjh49SlBQEOfOnTPecezh4UFKSgq9evWiuLiY6urq\nVo/t9tj5+fnY2try/vvvU1FRQW5uLgaDoVm7lXsd28vLi/z8fKKiotDpdBw9epRXXnml1W1NjdUS\nT09P+vTpw+bNm1EoFGRlZeHt7c3MmTPNzrMtfHx8WLt2LQ0NDWi1WkpKSvD29ja2jZk4caKxbYw5\n7ueLw//3fxUkJi7hk0+24uDgwJdf7sbDw4vGRuvH7gvJ8iXrtpG8zCdZmU+yMp9kZdojdWPIiBEj\nSExMZNeuXTg4OGBlZYVOp2PUqFF888039O7d2+S2ffv25cUXXyQsLAy9Xk9YWJixsfKdfvGLXzBj\nxgycnJx44oknWm0lM3nyZOLi4nj99dfp1asXXbp0ASAxMZHY2FiampqAW4WlOYYOHUpMTAzFxcV0\n7dqV3r17c/nyZby9vUlPT6d///73PPaIESMoKiri1VdfRafTMXr0aGOT6wfhpz/9KUOHDiUsLAyt\nVouPjw89e/ZsU55t8ZOf/ISIiAjCw8MxGAzMmzePLl26MH36dGJjY8nNzcXR0ZHVq1c/kP21xtd3\nAJGRbzB7djRWVtY4OzuzYsWqh75fIYQQ4kGziBYxD8KRI0e4ceMGAQEBlJeXM23aNPbt29fe0xL3\nQD4pmkc+VbeN5GU+ycp8kpX5JCvTHqkzgebIycl5aC1UWuLu7k5MTAzr16+nsbHR5CXph9l25kGP\nPWvWLK5du9ZsmUqlIj09/Z7naI6LFy8SGxt71/LnnnuOOXPmPNR9CyGEEOIWORMoLI58UjSPfKpu\nG8nLfJKV+SQr80lWpj3Szw4WQgghhBAPlpwJFEIIIYSwQHImUAghhBDCAkkRKIQQQghhgaQIFEII\nIYSwQFIECiGEEEJYICkChRBCCCEskBSBQgghhBAW6JF9YogQbaHX60lMTOS7775DqVSSlJTU6jOo\nLY1Op2Px4sVcuHABrVbL9OnT6dOnD3FxcSgUCp5++mnee+89OnWSz423XblyhYkTJ5KZmYm1tbVk\nZUJGRgZ//etf0el0hIWFMXjwYMnKBJ1OR1xcHBcuXKBTp0588MEH8rvVguPHj7Nq1Sqys7OpqKho\nMZ/169fzt7/9DWtraxYvXoyPj097T7tDsuzfJGEx9u3bh1arJScnh/nz57Ny5cr2nlKHsnPnThwc\nHNi6dSsff/wxH3zwAStWrGDu3Lls3boVg8HA/v3723uaHYZOp2Pp0qXY2NgASFYmHDp0iKNHj7Jt\n2zays7P5/vvvJatWFBQU0NjYyPbt25k5cyZr166VvO7w8ccfEx8fT0NDA9Dy396pU6coKipix44d\nrFmzhmXLlrXzrDsuKQKFRSguLubFF18EwM/Pj5MnT7bzjDqW0aNH88477xhfW1lZcerUKQYPHgxA\nYGAg33zzTXtNr8NJSUnhtddew8XFBUCyMuHAgQN4e3szc+ZM3n77bYYPHy5ZtcLDw4Ompib0ej1q\ntRpra2vJ6w5PPfUUaWlpxtct5VNcXExAQAAKhYJevXrR1NREbW1te025Q5MiUFgEtVqNSqUyvray\nsqKxsbEdZ9SxdO/eHZVKhVqtZs6cOcydOxeDwYBCoTC+f/26PJMTID8/nx49ehg/VACSlQlXr17l\n5MmTrFu3jmXLlrFgwQLJqhXdunXjwoULjBkzhoSEBCIiIiSvO4waNQpr6/9+k62lfO78fy+5mSbf\nCRQWQaVSodFojK/1en2zfyQCLl26xMyZMwkPD2f8+PGkpqYa39NoNNjZ2bXj7DqOvLw8FAoF//jH\nP/j222+JjY1tdpZBsvovBwcHPD09USqVeHp60qVLF77//nvj+5JVc1lZWQQEBDB//nwuXbrElClT\n0Ol0xvclr7v97/cjb+dz5/97jUaDra1te0yvw5MzgcIi+Pv7U1hYCMCxY8fw9vZu5xl1LDU1Nbzx\nxhssXLiQyZMnA9CvXz8OHToEQGFhIYMGDWrPKXYYn332GZ9++inZ2dn07duXlJQUAgMDJasWDBw4\nkK+//hqDwUBVVRU3b95k6NChkpUJdnZ2xmLF3t6exsZG+Tv8AS3l4+/vz4EDB9Dr9Vy8eBG9Xk+P\nHj3aeaYdk8JgMBjaexJCPGy37w4+e/YsBoOB5ORkvLy82ntaHUZSUhJffvklnp6exmVLliwhKSkJ\nnU6Hp6cnSUlJWFlZteMsO56IiAgSExPp1KkTCQkJklULPvzwQw4dOoTBYGDevHm4ublJViZoNBoW\nL15MdXU1Op2OyMhInn32WcnrDpWVlcTExJCbm0tZWVmL+aSlpVFYWIher+fdd9+V4tkEKQKFEEII\nISyQXA4WQgghhLBAUgQKIYQQQlggKQKFEEIIISyQFIFCCCGEEBZIikAhhBBCCAsk3XKFEEKYVFlZ\nyejRo+9qqbRx40ZcXV3baVZCiAdBikAhhBCtcnFx4YsvvmjvaQghHjApAoUQQty3Xbt2sXnzZqys\nrHBzcyM1NRWlUsmqVavYt28fVlZWvPrqq0yZMoWysjKWLl1KXV0d3bp1Y8mSJfj4+BAXF0ddXR0V\nFRUsXLgQZ2dnVqxYQX19PY6Ojixbtgx3d/f2PlQhHhtSBAohhGjV5cuXmTBhgvH1+PHjmTZtWrN1\n1q5dS25uLk5OTqSkpFBaWkp5eTlHjhxh165d6HQ6wsPDGTt2LAsXLiQ6Oprg4GCOHTvGO++8w5//\n/Gfg1vOGN27ciFarZfLkyWzcuJFevXrx9ddfk5CQQFZW1o956EI81qQIFEII0SpzLgePGDGCsLAw\ngoKCGDVqFH379mXHjh2MGTMGpVKJUqnkiy++QKPRcO7cOYKDgwHw8/PD3t6e0tJSAHx8fAAoLy/n\n/PnzTJ8+3bgPtVr9kI5QCMskRaAQQoj7Fh8fz5kzZygoKGDhwoXMmjULa2trFAqFcZ3Kykrs7e3v\n2tZgMNDU1ASAjY0NcOt5325ubsbis6mpiZqamh/hSISwHNIiRgghxH1pbGwkODgYR0dH3nrrLSZM\nmMC3337Lc889x969e9HpdNy8eZNp06ZRU1ODm5sbe/fuBeDYsWPU1NTw9NNPNxvT09OTa9eucfjw\nYQDy8vJYsGDBj35sQjzO5EygEEKI+2Jtbc2cOXN444036NKlC05OTqxcuRInJydOnjzJxIkT0ev1\nREZG4uHhQWpqKomJiaSlpdG5c2fS0tJQKpXNxlQqlaxbt47ly5fT0NCASqUiJSWlnY5QiMeTwmAw\nGNp7EkIIIYQQ4scll4OFEEIIISyQFIFCCCGEEBZIikAhhBBCCAskRaAQQgghhAWSIlAIIYQQwgJJ\nESiEEEIIYYGkCBRCCCGEsEBSBAohhBBCWKD/B/thFuKY/YNeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from xgboost import plot_importance\n",
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.figure(figsize = (10,40))\n",
    "plot_importance(grid_search.best_estimator_,max_num_features=40)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predictions = grid_search.best_estimator_.predict(scaled_test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn回归模型进行Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1决策树回归\n",
    "from sklearn import tree\n",
    "model_DecisionTreeRegressor = tree.DecisionTreeRegressor(max_depth=None,max_leaf_nodes=None)\n",
    "# 2线性回归\n",
    "from sklearn import linear_model\n",
    "model_LinearRegression = linear_model.LinearRegression(n_jobs = -1)# 并行\n",
    "# 3SVM回归 \n",
    "from sklearn import svm\n",
    "###  C：惩罚参数，数值越大，正则化越小\n",
    "### epsilon：不敏感损失函数的epsilon参数\n",
    "model_SVR_linear = svm.SVR(kernel='linear',epsilon = 0.1,C = 1.0)  ## 不可用于回归器，不明白为什么\n",
    "model_SVR_rbf = svm.SVR(kernel='rbf',gamma = 'auto',epsilon = 0.1,C = 1.0,tol = 1e-3)\n",
    "# 4KNN回归\n",
    "from sklearn import neighbors\n",
    "#### p：Power parameter for the Minkowski metric. \n",
    "model_KNeighborsRegressor = neighbors.KNeighborsRegressor(n_neighbors = 5,weights='uniform', algorithm = 'auto', leaf_size = 30, p = 2,n_jobs = -1)\n",
    "# 5随机森林回归\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model_RandomForestRegressor = RandomForestRegressor(criterion = 'mae',max_depth = None,n_estimators = 20)#默认20个决策树\n",
    "# 6Adaboost回归\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "model_AdaBoostRegressor = AdaBoostRegressor(n_estimators = 50,loss = 'linear')#这里使用50个决策树\n",
    "# GBRT回归\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "### lad = least absolute deviation\n",
    "model_GradientBoostingRegressor = GradientBoostingRegressor(loss = 'lad',criterion = 'mae',learning_rate = 0.1,n_estimators = 100)#这里使用100个决策树\n",
    "# 8Bagging回归\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "model_BaggingRegressor = BaggingRegressor(n_estimators = 10,n_jobs = -1)# 并行\n",
    "# 9ExtraTree极端随机树回归\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "model_ExtraTreeRegressor = ExtraTreesRegressor(criterion = 'mae',n_jobs = -1)\n",
    "# 10 Ridge\n",
    "from sklearn.linear_model import Ridge\n",
    "model_Ridge = Ridge(random_state=1,alpha = 1.0)\n",
    "# 11 LR-Lasso\n",
    "from sklearn.linear_model import Lasso\n",
    "model_Lasso = Lasso(random_state=1,alpha = 10.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mlxtend.regressor import StackingRegressor  # 回归模型stacking\n",
    "# regressors = [ model_DecisionTreeRegressor,model_KNeighborsRegressor,model_LinearRegression, model_Ridge, model_Lasso,model_RandomForestRegressor,model_AdaBoostRegressor,model_GradientBoostingRegressor,\n",
    "#              model_BaggingRegressor,model_ExtraTreeRegressor]\n",
    "regressors = [model_Ridge, model_Lasso,xgb_model,\n",
    "             model_BaggingRegressor]\n",
    "stregr = StackingRegressor(regressors=regressors, meta_regressor=model_SVR_rbf,verbose = 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.2s remaining:    6.7s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.2s remaining:    3.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.2s remaining:    3.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.0s remaining:    3.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.0s remaining:    3.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.2s remaining:    3.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.2s remaining:    3.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.0s remaining:    3.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.2s remaining:    3.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.0s remaining:    3.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.0s remaining:    3.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.2s remaining:    3.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.2s remaining:    3.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.0s remaining:    3.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.2s remaining:    3.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.2s remaining:    3.7s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.0s remaining:    3.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.0s remaining:    3.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.0s remaining:    3.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.0s remaining:    3.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.2s remaining:    3.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.0s remaining:    3.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.0s remaining:    3.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.0s remaining:    3.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.0s remaining:    3.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.1s remaining:    3.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.8s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.8s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.8s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.8s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.7s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.7s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.7s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.7s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.8s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.8s remaining:    5.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.8s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.7s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.1s remaining:    6.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.8s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.8s remaining:    5.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.8s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.8s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.8s remaining:    5.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.8s remaining:    5.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.8s remaining:    5.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.6s remaining:    4.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.9s remaining:    5.7s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.9s remaining:    5.7s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.1s remaining:    6.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.9s remaining:    5.7s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.9s remaining:    5.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.8s remaining:    5.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.8s remaining:    5.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.0s remaining:    6.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.8s remaining:    5.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.8s remaining:    5.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.9s remaining:    5.7s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.8s remaining:    5.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.0s remaining:    6.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.9s remaining:    5.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.8s remaining:    5.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.1s remaining:    6.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.8s remaining:    5.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.1s remaining:    6.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.8s remaining:    5.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.9s remaining:    5.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.8s remaining:    5.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.0s remaining:    6.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.1s remaining:    6.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.2s remaining:    6.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.9s remaining:    5.8s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.0s remaining:    6.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.8s remaining:    5.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.1s remaining:    6.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.7s remaining:    5.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=30, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.8s remaining:    5.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.7s remaining:    8.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.7s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.4s remaining:   10.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.0s remaining:    9.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.0s remaining:    9.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.7s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.7s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.7s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.8s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.0s remaining:    9.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.7s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.1s remaining:    9.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.0s remaining:    9.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.0s remaining:    8.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.7s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.7s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.0s remaining:    9.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.0s remaining:    9.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.8s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.0s remaining:    8.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.0s remaining:    9.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.8s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.0s remaining:    9.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.1s remaining:    9.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.0s remaining:    9.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.2s remaining:    9.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.8s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.0s remaining:    9.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.3s remaining:    9.8s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.7s remaining:    8.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.2s remaining:    9.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.8s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.2s remaining:    9.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.7s remaining:    8.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.7s remaining:    8.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.4s remaining:   10.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.0s remaining:    9.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.0s remaining:    9.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.0s remaining:    9.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.7s remaining:    8.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.0s remaining:    9.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.0s remaining:    8.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.0s remaining:    8.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.0s remaining:    9.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.7s remaining:    8.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.0s remaining:    9.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.7s remaining:    8.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.7s remaining:    8.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.7s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.7s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.7s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.7s remaining:    8.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.1s remaining:    9.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.7s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.7s remaining:    8.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.7s remaining:    8.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.1s remaining:    9.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.7s remaining:    8.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.0s remaining:    8.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=6.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.8s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 360 out of 360 | elapsed: 29.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 regressors...\n",
      "Fitting regressor1: ridge (1/4)\n",
      "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=1, solver='auto', tol=0.001)\n",
      "Fitting regressor2: lasso (2/4)\n",
      "Lasso(alpha=4.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=1,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Fitting regressor3: xgbregressor (3/4)\n",
      "XGBRegressor(Missing=None, base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fitting regressor4: baggingregressor (4/4)\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=40, n_jobs=-1, oob_score=False, random_state=None,\n",
      "         verbose=1, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.8s remaining:   11.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    4.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'grid_scores_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-78cfc23659f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_train_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         print(\"%0.3f +/- %0.2f %r\"\n\u001b[1;32m     34\u001b[0m               % (mean_score, scores.std() / 2.0, params))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'grid_scores_'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 网格：“模型（小写）__参数名”\n",
    "params = {\n",
    "#         'decisiontreeregressor__max_depth':[None],\n",
    "#         'kneighborsregressor__n_neighbors':[5,10,15,20],\n",
    "#         'kneighborsregressor__leaf_size':[20,30,40],\n",
    "#         'randomforestregressor__max_depth':[None],\n",
    "#         'randomforestregressor__n_estimators':[20],\n",
    "#         'adaboostregressor__n_estimators':[50],\n",
    "#         'gradientboostingregressor__learning_rate':[0.01,0.1,0.2],\n",
    "#         'gradientboostingregressor__n_estimators':[50,80,100,130],  # GBRT具有较好的防止过拟合特性\n",
    "        'baggingregressor__n_estimators':[20,30,40],\n",
    "#         'extratreeregressor_n_estimators':[10,20,40,60,100],  # 20.0开始\n",
    "        'lasso__alpha': [5.0,4.0,6.0],\n",
    "        'ridge__alpha': [1.0,1.5],\n",
    "        'meta-svr__C': [0.2,0.3],\n",
    "        'meta-svr__gamma': [0.2,0.3],\n",
    "        'meta-svr__tol':[1e-3]}\n",
    "\n",
    "grid = GridSearchCV(estimator=stregr, \n",
    "                    param_grid=params,\n",
    "                    scoring=\"neg_mean_absolute_error\",\n",
    "                    cv=5,\n",
    "                    verbose = 1,\n",
    "                    refit = True)\n",
    "# # 用分割的数据训练和评估\n",
    "# grid.fit(train_X_train, train_y_train)\n",
    "# 用所有数据训练和评估\n",
    "grid.fit(scaled_train_X, train_y)\n",
    "\n",
    "for params, mean_score, scores in grid.grid_scores_:\n",
    "        print(\"%0.3f +/- %0.2f %r\"\n",
    "              % (mean_score, scores.std() / 2.0, params))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -2.231349 using {'baggingregressor__n_estimators': 40, 'lasso__alpha': 4.0, 'meta-svr__C': 0.2, 'meta-svr__gamma': 0.2, 'meta-svr__tol': 0.001, 'ridge__alpha': 1.5}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best: %f using %s\" % (grid.best_score_, grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据GridSearchCV得到的最好的模型进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# print('GridSearchCV 得到的最好模型参数')\n",
    "# print(grid.best_estimator_.get_params())\n",
    "# stregr.fit(scaled_train_X,train_y)\n",
    "# 预测\n",
    "test_predictions = grid.best_estimator_.predict(scaled_test_X)\n",
    "# test_predictions = stregr.predict(scaled_test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型集合(From Andrew Lukyanenko)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型集合，包括lgb、xgb、sklearn、CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import NuSVR, SVR\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(X=scaled_train_X, X_test=scaled_test_X, y=train_y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):\n",
    "    \"\"\"\n",
    "    模型集合，来自于Andrew Lukyanenko(https://www.kaggle.com/artgor/earthquakes-fe-more-features-and-samples)\n",
    "    \"\"\"\n",
    "    oof = np.zeros(len(X))\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',\n",
    "                    verbose=10000, early_stopping_rounds=200)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'sklearn':\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = mean_absolute_error(y_valid, y_pred_valid)\n",
    "            print(f'Fold {fold_n}. MAE: {score:.4f}.')\n",
    "            print('')\n",
    "            \n",
    "            y_pred = model.predict(X_test).reshape(-1,)\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', **params)\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        scores.append(mean_absolute_error(y_valid, y_pred_valid))\n",
    "\n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = X.columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= n_fold\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        feature_importance[\"importance\"] /= n_fold\n",
    "        if plot_feature_importance:\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "        \n",
    "            return oof, prediction, feature_importance\n",
    "        return oof, prediction\n",
    "    \n",
    "    else:\n",
    "        return oof, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Mon Mar  4 19:02:42 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1050]\ttraining's l1: 1.62533\tvalid_1's l1: 2.08538\n",
      "Fold 1 started at Mon Mar  4 19:02:50 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1133]\ttraining's l1: 1.61156\tvalid_1's l1: 2.02249\n",
      "Fold 2 started at Mon Mar  4 19:03:01 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1157]\ttraining's l1: 1.57871\tvalid_1's l1: 2.12446\n",
      "Fold 3 started at Mon Mar  4 19:03:11 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1045]\ttraining's l1: 1.66208\tvalid_1's l1: 1.90824\n",
      "Fold 4 started at Mon Mar  4 19:03:21 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1165]\ttraining's l1: 1.5902\tvalid_1's l1: 2.06125\n",
      "CV mean score: 2.0404, std: 0.0739.\n"
     ]
    }
   ],
   "source": [
    "# LGB\n",
    "\n",
    "params = {'num_leaves': 54,\n",
    "          'min_data_in_leaf': 79,\n",
    "          'objective': 'huber',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.01,\n",
    "          \"boosting\": \"gbdt\",\n",
    "          \"bagging_freq\": 5,\n",
    "          \"bagging_fraction\": 0.8126672064208567,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'mae',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.1302650970728192,\n",
    "          'reg_lambda': 0.3603427518866501\n",
    "         }\n",
    "oof_lgb, prediction_lgb, feature_importance = train_model(params=params, model_type='lgb', plot_feature_importance=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_cols = list(feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predictions = prediction_lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Mon Mar  4 19:08:22 2019\n",
      "[0]\ttrain-mae:4.93515\tvalid_data-mae:5.08567\n",
      "Multiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-mae hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[61]\ttrain-mae:0.610753\tvalid_data-mae:2.19989\n",
      "\n",
      "Fold 1 started at Mon Mar  4 19:08:29 2019\n",
      "[0]\ttrain-mae:4.96377\tvalid_data-mae:4.98294\n",
      "Multiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-mae hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[64]\ttrain-mae:0.588951\tvalid_data-mae:2.04072\n",
      "\n",
      "Fold 2 started at Mon Mar  4 19:08:38 2019\n",
      "[0]\ttrain-mae:4.95612\tvalid_data-mae:5.00366\n",
      "Multiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-mae hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[68]\ttrain-mae:0.549976\tvalid_data-mae:2.1382\n",
      "\n",
      "Fold 3 started at Mon Mar  4 19:08:45 2019\n",
      "[0]\ttrain-mae:5.00228\tvalid_data-mae:4.81454\n",
      "Multiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-mae hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[65]\ttrain-mae:0.59676\tvalid_data-mae:2.01047\n",
      "\n",
      "Fold 4 started at Mon Mar  4 19:08:54 2019\n",
      "[0]\ttrain-mae:4.95942\tvalid_data-mae:4.99938\n",
      "Multiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-mae hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[52]\ttrain-mae:0.756073\tvalid_data-mae:2.11486\n",
      "\n",
      "CV mean score: 2.1008, std: 0.0681.\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {'eta': 0.05,\n",
    "              'max_depth': 10,\n",
    "              'subsample': 0.9,\n",
    "              'objective': 'reg:linear',\n",
    "              'eval_metric': 'mae',\n",
    "              'silent': True,\n",
    "              'nthread': 4}\n",
    "oof_xgb, prediction_xgb = train_model( params=xgb_params, model_type='xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predictions = prediction_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NuSVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Mon Mar  4 19:19:05 2019\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be real number, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-6320c2b49974>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNuSVR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'scale'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0moof_svr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_svr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sklearn'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-29-180f80512942>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(X, X_test, y, params, folds, model_type, plot_feature_importance, model)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'sklearn'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0my_pred_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    252\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\svm\\libsvm.pyx\u001b[0m in \u001b[0;36msklearn.svm.libsvm.fit\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: must be real number, not str"
     ]
    }
   ],
   "source": [
    "model = NuSVR(gamma='scale', nu=0.9, C=10.0, tol=0.01)\n",
    "oof_svr, prediction_svr = train_model(params=None, model_type='sklearn', model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be real number, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-419ef287f027>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNuSVR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'scale'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# oof_svr1, prediction_svr1 = train_model( params=None, model_type='sklearn', model=model)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    252\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\svm\\libsvm.pyx\u001b[0m in \u001b[0;36msklearn.svm.libsvm.fit\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: must be real number, not str"
     ]
    }
   ],
   "source": [
    "model = NuSVR(gamma='scale', nu=0.7, tol=0.01, C=1.0)\n",
    "model.fit([[0],[1],[2],[3]],[[0],[1],[2],[3]])\n",
    "# oof_svr1, prediction_svr1 = train_model( params=None, model_type='sklearn', model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Mon Mar  4 19:26:29 2019\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-60984342bb1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'loss_function'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'MAE'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0moof_cat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_cat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cat'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-29-180f80512942>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(X, X_test, y, params, folds, model_type, plot_feature_importance, model)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'cat'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCatBoostRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20000\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0meval_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'MAE'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_best_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0my_pred_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Python\\Anaconda\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, cat_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval)\u001b[0m\n\u001b[0;32m   2547\u001b[0m                          \u001b[0muse_best_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2548\u001b[0m                          \u001b[0mverbose_eval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2549\u001b[1;33m                          save_snapshot, snapshot_file, snapshot_interval)\n\u001b[0m\u001b[0;32m   2550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mntree_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mntree_end\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthread_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Python\\Anaconda\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, cat_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval)\u001b[0m\n\u001b[0;32m   1124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1125\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mlog_fixup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1126\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_sets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_leaf_weights_in_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Python\\Anaconda\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool)\u001b[0m\n\u001b[0;32m    826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = {'loss_function':'MAE'}\n",
    "oof_cat, prediction_cat = train_model( params=params, model_type='cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='6'>生成预测结果</a>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2624\n"
     ]
    }
   ],
   "source": [
    "print(len(test_predictions))\n",
    "\n",
    "submission.time_to_failure = test_predictions\n",
    "submission.to_csv('submission.csv',index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
